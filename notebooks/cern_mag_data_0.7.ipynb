{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "## CERN is celebrating its 70th anniversary \n",
    "During a crucial period for high-energy physics, coinciding with the initiation of the third update to the European strategy for particle physics. In this special edition of CERN Courier magazine, early-career researchers share their visions for the future of the field while reflecting on CERN's scientific and societal contributions. The magazine features expert insights into the achievements of the Large Hadron Collider (LHC) and explores the advancements of the hybrid pixel detector technology, emphasizing its applications beyond particle physics.\n",
    "\n",
    "## The CERN Courier website \n",
    "is a rich repository of articles covering a wide array of topics in particle physics, high-energy physics, and associated technological advancements. It provides in-depth reporting on the latest experimental results from CERN and other international laboratories, offering insights into ongoing research and discoveries in the field.\n",
    "\n",
    "## Last 11 years of the CERN Courier Magazine in PDF\n",
    "In this dataset I am downloading the Last 11 years of the CERN Courier Magazine.  I will then take this database and then encode it to be used as a Context Window to ask Questions to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "# 1: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555b0f1c-a3f7-47cc-9c3e-546e7d19754f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 existing PDF files\n",
      "Starting CERN PDF crawler (pages 0 to 7)\n",
      "\n",
      "Processing page 0...\n",
      "Found 15 new article links on page 0\n",
      "\n",
      "Found PDF: CERNCourier2024MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2896932/files/CERNCourier2024MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 14.7M/14.7M [00:01<00:00, 8.20MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2024MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2893513/files/CERNCourier2024MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.5M/13.5M [00:01<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2024JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2886335/files/CERNCourier2024JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.0M/13.0M [00:01<00:00, 8.04MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023NovDec-digitaledition NEW.pdf\n",
      "URL: https://cds.cern.ch/record/2879381/files/CERNCourier2023NovDec-digitaledition%20NEW.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023NovDec-digitaledition NEW.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 12.3M/12.3M [00:01<00:00, 9.70MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2869155/files/CERNCourier2023SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 19.6M/19.6M [00:05<00:00, 3.46MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2863407/files/CERNCourier2023JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 16.3M/16.3M [00:05<00:00, 3.35MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2857134/files/CERNCourier2023MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 18.3M/18.3M [00:04<00:00, 4.35MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2857133/files/CERNCourier2023MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 15.9M/15.9M [00:02<00:00, 5.93MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2845914/files/CERNCourier2023JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.7M/11.7M [00:01<00:00, 6.24MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2840144/files/CERNCourier2022NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 15.4M/15.4M [00:01<00:00, 8.74MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2826497/files/CERNCourier2022SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.6M/13.6M [00:06<00:00, 2.16MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2807618/files/CERNCourier2022MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 12.9M/12.9M [00:05<00:00, 2.42MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 1...\n",
      "Found 15 new article links on page 1\n",
      "\n",
      "Found PDF: CERNCourier2022MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2804425/files/CERNCourier2022MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:03<00:00, 3.49MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2799462/files/CERNCourier2022JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 9.46M/9.46M [00:02<00:00, 4.61MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2789409/files/CERNCourier2021NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.9M/11.9M [00:01<00:00, 6.29MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2782568/files/CERNCourier2021SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 15.1M/15.1M [00:07<00:00, 2.11MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2773907/files/CERNCourier2021JulAug-digitaledition.pdf\n",
      "Error downloading CERNCourier2021JulAug-digitaledition.pdf: 404 Client Error: Not Found for url: https://cds.cern.ch/record/2773907/files/CERNCourier2021JulAug-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier2021MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2765233/files/CERNCourier2021MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 9.97M/9.97M [00:01<00:00, 7.10MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2753402/files/CERNCourier2021MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.8M/13.8M [00:01<00:00, 11.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2750037/files/CERNCourier2021JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 21.6M/21.6M [00:01<00:00, 12.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2743359/files/CERNCourier2020NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 10.6M/10.6M [00:01<00:00, 9.85MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2743358/files/CERNCourier2020SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.1M/13.1M [00:01<00:00, 10.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2722711/files/CERNCourier2020JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.5M/13.5M [00:01<00:00, 12.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2717129/files/CERNCourier2020MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 12.2M/12.2M [00:01<00:00, 9.70MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2712176/files/CERNCourier2020MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.7M/13.7M [00:01<00:00, 11.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2706508/files/CERNCourier2020JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 8.74M/8.74M [00:00<00:00, 9.34MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2701615/files/CERNCourier2019NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.3M/11.3M [00:01<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 2...\n",
      "Found 15 new article links on page 2\n",
      "\n",
      "Found PDF: CERNCourier2019SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2689203/files/CERNCourier2019SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.6M/11.6M [00:01<00:00, 7.50MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CCJulAug19-digital.pdf\n",
      "URL: https://cds.cern.ch/record/2681906/files/CCJulAug19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CCJulAug19-digital.pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 8.52MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CCMayJun19-digital.pdf\n",
      "URL: https://cds.cern.ch/record/2673718/files/CCMayJun19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CCMayJun19-digital.pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15.4M/15.4M [00:01<00:00, 12.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2666160/files/CERNCourier2019MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 12.1M/12.1M [00:01<00:00, 11.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2654576/files/CERNCourier2019JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.2M/11.2M [00:01<00:00, 7.28MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Dec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2649360/files/CERNCourier2018Dec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Dec-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 10.2M/10.2M [00:01<00:00, 9.58MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Nov-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2645275/files/CERNCourier2018Nov-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Nov-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 8.90M/8.90M [00:00<00:00, 9.51MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Oct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2640475/files/CERNCourier2018Oct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Oct-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 8.13M/8.13M [00:00<00:00, 8.73MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Sep-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2636286/files/CERNCourier2018Sep-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Sep-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 15.6M/15.6M [00:01<00:00, 12.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2628313/files/CERNCourier2018JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 6.69M/6.69M [00:00<00:00, 8.40MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier June 2018 (Volume 53 Issue 5).pdf\n",
      "URL: https://home.cern/sites/default/files/2018-06/CERN%20Courier%20June%202018%20%28Volume%2053%20Issue%205%29.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier June 2018 (Volume 53 Issue 5).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 8.69M/8.69M [00:01<00:00, 6.94MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier May 2018 (Volume 53 Issue 4).pdf\n",
      "URL: https://cds.cern.ch/record/2318574/files/CERN%20Courier%20May%202018%20(Volume%2053%20Issue%204).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier May 2018 (Volume 53 Issue 4).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 9.32M/9.32M [00:01<00:00, 6.67MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 3 (April 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2309976/files/CERN%20Courier%20Volume%2058%20Issue%203%20(April%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 3 (April 2018).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 13.3M/13.3M [00:01<00:00, 10.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 2 (March 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2304934/files/CERN%20Courier%20Volume%2058%20Issue%202%20(March%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 2 (March 2018).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 9.73MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2300591/files/CERN%20Courier%20Volume%2058%20Issue%201%20(Jan-Feb%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 8.53MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 3...\n",
      "Found 15 new article links on page 3\n",
      "\n",
      "Found PDF: CERN Courier December 2017 (Volume 57 Issue 10).pdf\n",
      "URL: https://cds.cern.ch/record/2292627/files/CERN%20Courier%20December%202017%20(Volume%2057%20Issue%2010).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier December 2017 (Volume 57 Issue 10).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 8.04M/8.04M [00:01<00:00, 5.80MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 9 (November 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2289267/files/CERN%20Courier%20Volume%2057%20Issue%209%20(November%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 9 (November 2017).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 8.51M/8.51M [00:00<00:00, 9.13MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 8 October 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2285637/files/CERN%20Courier%20Volume%2057%20Issue%208%20October%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 8 October 2017.pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 9.17M/9.17M [00:00<00:00, 9.69MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 7 (September 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2281303/files/CERN%20Courier%20Volume%2057%20Issue%207%20(September%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 7 (September 2017).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 10.1M/10.1M [00:01<00:00, 9.15MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 6 (July-August 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2273705/files/CERN%20Courier%20Volume%2057%20Issue%206%20(July-August%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 6 (July-August 2017).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████| 10.6M/10.6M [00:01<00:00, 9.84MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 4 .pdf\n",
      "URL: https://cds.cern.ch/record/2259560/files/CERN%20Courier%20Volume%2057%20Issue%204%20.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 4 .pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 12.5M/12.5M [00:01<00:00, 8.01MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 3 April 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2256135/files/CERN%20Courier%20Volume%2057%20Issue%203%20April%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 3 April 2017.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 6.81M/6.81M [00:00<00:00, 8.59MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 2 March 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2252407/files/CERN%20Courier%20Volume%2057%20Issue%202%20March%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 2 March 2017.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 9.75M/9.75M [00:01<00:00, 10.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf\n",
      "URL: https://cds.cern.ch/record/2241972/files/CERN%20Courier%20Jan-Feb%202017%20(Volume%2057%20issue%201).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 9.89M/9.89M [00:00<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier November 2016 (Volume 56 Issue 9).pdf\n",
      "URL: https://cds.cern.ch/record/2224294/files/CERN%20Courier%20November%202016%20(Volume%2056%20Issue%209).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier November 2016 (Volume 56 Issue 9).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 9.23M/9.23M [00:01<00:00, 6.56MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier October 2016 (Volume 56 Issue 8).pdf\n",
      "URL: https://cds.cern.ch/record/2219443/files/CERN%20Courier%20October%202016%20(Volume%2056%20Issue%208).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier October 2016 (Volume 56 Issue 8).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16.3M/16.3M [00:01<00:00, 11.8MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier September 2016 (Volume 56 Issue 7).pdf\n",
      "URL: https://cds.cern.ch/record/2211464/files/CERN%20Courier%20September%202016%20(Volume%2056%20Issue%207).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier September 2016 (Volume 56 Issue 7).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 15.3M/15.3M [00:01<00:00, 10.8MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier July-August 2016 (Volume 56 Issue 6).pdf\n",
      "URL: http://cds.cern.ch/record/2198166/files/CERN%20Courier%20July-August%202016%20(Volume%2056%20Issue%206).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier July-August 2016 (Volume 56 Issue 6).pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 21.9M/21.9M [00:46<00:00, 497kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 4...\n",
      "Found 15 new article links on page 4\n",
      "\n",
      "Found PDF: CERN Courier June 2016 (Volume 56 Issue 5).pdf\n",
      "URL: https://cds.cern.ch/record/2155287/files/CERN%20Courier%20June%202016%20(Volume%2056%20Issue%205).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier June 2016 (Volume 56 Issue 5).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 9.50M/9.50M [00:01<00:00, 6.79MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier May 2016 (Volume 56 Issue 4).pdf\n",
      "URL: http://cds.cern.ch/record/2146835/files/CERN%20Courier%20May%202016%20(Volume%2056%20Issue%204).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier May 2016 (Volume 56 Issue 4).pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10.2M/10.2M [00:18<00:00, 584kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Mar 2016 (Volume 56 Issue 2).pdf\n",
      "URL: https://cds.cern.ch/record/2131754/files/CERN%20Courier%20Mar%202016%20(Volume%2056%20Issue%202).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Mar 2016 (Volume 56 Issue 2).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 9.46M/9.46M [00:01<00:00, 6.78MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 5...\n",
      "Found 15 new article links on page 5\n",
      "\n",
      "Found PDF: CERNCourier2013Oct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1603700/files/CERNCourier2013Oct-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Oct-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1735007/files/CERNCourier2013Oct-digitaledition.pdf\n",
      "\n",
      "Processing page 6...\n",
      "Found 5 new article links on page 6\n",
      "\n",
      "Found PDF: CERN Courier June 2013.pdf\n",
      "URL: http://cds.cern.ch/record/1550751/files/CERN%20Courier%20June%202013.pdf\n",
      "Error downloading CERN Courier June 2013.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734960/files/CERN%20Courier%20June%202013.pdf\n",
      "\n",
      "Found PDF: CERN Courier digital edition May 2013.pdf\n",
      "URL: http://cds.cern.ch/record/1544352/files/CERN%20Courier%20digital%20edition%20May%202013.pdf\n",
      "Error downloading CERN Courier digital edition May 2013.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734947/files/CERN%20Courier%20digital%20edition%20May%202013.pdf\n",
      "\n",
      "Found PDF: CERNCourier2013Apr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1537017/files/CERNCourier2013Apr-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Apr-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734933/files/CERNCourier2013Apr-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier2013Mar-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1517538/files/CERNCourier2013Mar-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Mar-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734919/files/CERNCourier2013Mar-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier-2013-53-1.pdf\n",
      "URL: http://cds.cern.ch/record/1514615/files/CERNCourier-2013-53-1.pdf\n",
      "Error downloading CERNCourier-2013-53-1.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734891/files/CERNCourier-2013-53-1.pdf\n",
      "\n",
      "Processing page 7...\n",
      "Found 0 new article links on page 7\n",
      "\n",
      "Download Summary:\n",
      "--------------------\n",
      "Total PDFs found: 64\n",
      "Successfully downloaded: 57\n",
      "Skipped (already downloaded): 0\n",
      "Failed downloads: 7\n",
      "\n",
      "Failed downloads:\n",
      "- CERNCourier2021JulAug-digitaledition.pdf\n",
      "- CERNCourier2013Oct-digitaledition.pdf\n",
      "- CERN Courier June 2013.pdf\n",
      "- CERN Courier digital edition May 2013.pdf\n",
      "- CERNCourier2013Apr-digitaledition.pdf\n",
      "- CERNCourier2013Mar-digitaledition.pdf\n",
      "- CERNCourier-2013-53-1.pdf\n",
      "CPU times: user 12.6 s, sys: 2.28 s, total: 14.9 s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin, unquote\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "class CERNPDFCrawler:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://home.cern/resources\"\n",
    "        self.session = requests.Session()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.download_folder = \"cern_pdfs\"\n",
    "        self.processed_article_urls = set()\n",
    "        self.downloaded_files = set()\n",
    "        \n",
    "        if not os.path.exists(self.download_folder):\n",
    "            os.makedirs(self.download_folder)\n",
    "        self.load_existing_files()\n",
    "\n",
    "    def load_existing_files(self):\n",
    "        for filename in os.listdir(self.download_folder):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                self.downloaded_files.add(filename)\n",
    "        print(f\"Found {len(self.downloaded_files)} existing PDF files\")\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "                    return None\n",
    "                time.sleep(2 ** attempt)\n",
    "        return None\n",
    "\n",
    "    def extract_pdf_urls_from_text(self, text):\n",
    "        \"\"\"Extract PDF URLs from text content including 'File path:' patterns\"\"\"\n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Look for \"File path:\" pattern\n",
    "        file_path_matches = re.finditer(r'File path:\\s*(https?://[^\\s<>\"]+\\.pdf)', text, re.IGNORECASE)\n",
    "        for match in file_path_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        # Look for direct PDF links\n",
    "        pdf_link_matches = re.finditer(r'href=\"(https?://[^\\s<>\"]+\\.pdf)\"', text, re.IGNORECASE)\n",
    "        for match in pdf_link_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        return pdf_urls\n",
    "\n",
    "    def find_courier_links(self, page_url):\n",
    "        content = self.get_page_content(page_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        courier_links = []\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/resources/courier/' in href or '/record/' in href:\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                if full_url not in self.processed_article_urls:\n",
    "                    courier_links.append(full_url)\n",
    "                    self.processed_article_urls.add(full_url)\n",
    "        \n",
    "        return courier_links\n",
    "\n",
    "    def find_pdf_links(self, article_url):\n",
    "        \"\"\"Find all PDF download links on an article page\"\"\"\n",
    "        content = self.get_page_content(article_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Extract URLs from text content\n",
    "        pdf_urls.update(self.extract_pdf_urls_from_text(content))\n",
    "        \n",
    "        # Parse with BeautifulSoup for structured extraction\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Look for links containing PDF\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.lower().endswith('.pdf'):\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                pdf_urls.add(full_url)\n",
    "        \n",
    "        return list(pdf_urls)\n",
    "\n",
    "    def sanitize_filename(self, url):\n",
    "        \"\"\"Create a safe filename from URL\"\"\"\n",
    "        filename = unquote(url.split('/')[-1])\n",
    "        # Remove or replace unsafe characters\n",
    "        filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "        return filename\n",
    "\n",
    "    def download_pdf(self, pdf_url, filename):\n",
    "        if filename in self.downloaded_files:\n",
    "            print(f\"Skipping {filename} - already downloaded\")\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            response = self.session.get(pdf_url, headers=self.headers, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            file_path = os.path.join(self.download_folder, filename)\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(file_path, 'wb') as file, tqdm(\n",
    "                desc=filename,\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = file.write(data)\n",
    "                    pbar.update(size)\n",
    "            \n",
    "            self.downloaded_files.add(filename)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def crawl_and_download(self, start_page=0, end_page=7):\n",
    "        print(f\"Starting CERN PDF crawler (pages {start_page} to {end_page})\")\n",
    "        \n",
    "        found_pdfs = 0\n",
    "        downloaded_pdfs = 0\n",
    "        skipped_pdfs = 0\n",
    "        failed_downloads = []\n",
    "        \n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = f\"{self.base_url}?type=52&page={page_num}\"\n",
    "            print(f\"\\nProcessing page {page_num}...\")\n",
    "            \n",
    "            courier_links = self.find_courier_links(page_url)\n",
    "            print(f\"Found {len(courier_links)} new article links on page {page_num}\")\n",
    "            \n",
    "            for article_url in courier_links:\n",
    "                pdf_urls = self.find_pdf_links(article_url)\n",
    "                \n",
    "                for pdf_url in pdf_urls:\n",
    "                    found_pdfs += 1\n",
    "                    filename = self.sanitize_filename(pdf_url)\n",
    "                    \n",
    "                    print(f\"\\nFound PDF: {filename}\")\n",
    "                    print(f\"URL: {pdf_url}\")\n",
    "                    \n",
    "                    if filename in self.downloaded_files:\n",
    "                        print(f\"Skipping - already downloaded\")\n",
    "                        skipped_pdfs += 1\n",
    "                        continue\n",
    "                        \n",
    "                    if self.download_pdf(pdf_url, filename):\n",
    "                        downloaded_pdfs += 1\n",
    "                    else:\n",
    "                        failed_downloads.append(filename)\n",
    "                \n",
    "                time.sleep(1)\n",
    "        \n",
    "        print(\"\\nDownload Summary:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Total PDFs found: {found_pdfs}\")\n",
    "        print(f\"Successfully downloaded: {downloaded_pdfs}\")\n",
    "        print(f\"Skipped (already downloaded): {skipped_pdfs}\")\n",
    "        print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "        if failed_downloads:\n",
    "            print(\"\\nFailed downloads:\")\n",
    "            for fail in failed_downloads:\n",
    "                print(f\"- {fail}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = CERNPDFCrawler()\n",
    "    crawler.crawl_and_download(0, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78865c",
   "metadata": {},
   "source": [
    "# 2: Encode the CERN PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93cde4-96fd-47f5-b862-93900a21ae9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.db_dir = Path(db_dir)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                \n",
    "                # Extract text from each page\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        \"\"\"Process all PDFs in the directory and return chunks with metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        # Process each PDF file\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Split text into chunks\n",
    "                chunks = self.text_splitter.create_documents(\n",
    "                    texts=[text],\n",
    "                    metadatas=[{\"source\": pdf_path.name}]\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def create_vector_db(self):\n",
    "        \"\"\"Create and populate the vector database\"\"\"\n",
    "        # Get text chunks\n",
    "        chunks = self.process_pdfs()\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"No text chunks were created. Check the PDF processing.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nCreating vector database with {len(chunks)} chunks...\")\n",
    "        \n",
    "        # Create and persist the vector store\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=str(self.db_dir)\n",
    "        )\n",
    "        \n",
    "        # Persist the database\n",
    "        vectordb.persist()\n",
    "        print(f\"Vector database created and saved to {self.db_dir}\")\n",
    "        \n",
    "        return vectordb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        processor = PDFProcessor()\n",
    "        vectordb = processor.create_vector_db()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd11e0-aa06-440b-af5d-d3835d32be28",
   "metadata": {},
   "source": [
    "# 3: Chat with Cern Magazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6facc867-bba9-44a6-9c78-2f199b187dfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:20: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN Research Assistant Ready!\n",
      "Ask questions about CERN research (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  What is driving the accelerated expansion of the Universe?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The accelerated expansion of the universe is one of the most profound discoveries in cosmology. This phenomenon is primarily attributed to what is broadly referred to as dark energy, although its exact nature and physics remain unknown. The concept of dark energy emerged from observations that contradicted the then-existing understanding of the universe's expansion. Specifically, in 1998, researchers found that, contrary to expectations that the universe's expansion was slowing down, it was actually speeding up. This conclusion was reached through comparisons of the expansion rate of the universe over time, significantly strengthened by combined measurements with those of the High-z Supernova Search Team (CERNCourier2019SepOct-digitaledition.pdf, start_index: 186634).\n",
      "\n",
      "One early attempt to explain the universe's dynamics involved the cosmological constant, initially introduced by Einstein to allow for a static universe within the framework of general relativity. This cosmological constant is linked to the concept of vacuum energy. However, calculations of the vacuum energy, summing the contributions from presumed quantum states in the universe, produce a value for the expansion rate that is about 120 orders of magnitude higher than what is observed. Such a rate would have prevented the formation of galaxies, stars, and planets, indicating that vacuum energy in its straightforward interpretation cannot be the final answer (CERNCourier2019SepOct-digitaledition.pdf, start_index: 188182).\n",
      "\n",
      "The discovery of the universe's accelerated expansion also offered a solution to the so-called \"age crisis,\" where some stars appeared older than the universe itself. This discrepancy suggested either an overestimation of stellar ages or an incorrect understanding of the universe's age and expansion rate. The concept of accelerated expansion helped reconcile these differences by implying that previous models of the universe's expansion were incomplete or inaccurate (CERNCourier2019SepOct-digitaledition.pdf, start_index: 187594).\n",
      "\n",
      "The nature of dark energy remains one of the most significant puzzles in modern fundamental physics. It is the primary driver of the dynamical evolution of the universe in the current epoch. Theories and experimental programs have been developed to understand dark energy, whether it is due to a cosmological constant, a new dynamical field, a deviation from general relativity on cosmological scales, or an entirely different mechanism. The current lambda-cold-dark-matter (ΛCDM) model incorporates dark energy but does not yet provide a complete explanation of its nature. Physicists and astronomers believe that uncovering the nature of dark energy could lead to a revolution in physics, given its fundamental role in the universe's expansion (CERN Courier Volume 57 Issue 4 .pdf, start_index: 58211).\n",
      "\n",
      "In summary, the accelerated expansion of the universe is driven by dark energy, a mysterious force whose exact nature is still under investigation. Despite significant progress in understanding the universe's expansion through observations and theoretical models, dark energy remains a central unanswered question in cosmology.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 317 ms, total: 1.96 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class CERNResearchAssistant:\n",
    "    def __init__(self, db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        # Initialize the vector store\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        # Initialize the language model\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Create the retriever\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Setup the prompt template\n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Setup the RAG chain\n",
    "        self.chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Ask a question about CERN research\"\"\"\n",
    "        try:\n",
    "            response = self.chain.invoke(question)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {e}\"\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize the assistant\n",
    "        assistant = CERNResearchAssistant()\n",
    "        \n",
    "        print(\"CERN Research Assistant Ready!\")\n",
    "        print(\"Ask questions about CERN research (type 'quit' to exit)\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            response = assistant.query(question)\n",
    "            print(\"\\nAssistant:\", response)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1f129-fdef-4725-a154-1bffd26c5b2b",
   "metadata": {},
   "source": [
    "# 4: Fine Tuning with OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6a4e76-cbb5-43da-92c4-e78531886a02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   2%|██▎                                                                                                                                 | 1/57 [00:05<04:56,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2024MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|████▋                                                                                                                               | 2/57 [00:10<04:35,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2022NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   5%|██████▉                                                                                                                             | 3/57 [00:13<03:49,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier Volume 57 Issue 8 October 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|█████████▎                                                                                                                          | 4/57 [00:17<03:45,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2023MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   9%|███████████▌                                                                                                                        | 5/57 [00:24<04:30,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2018Sep-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█████████████▉                                                                                                                      | 6/57 [00:28<04:06,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2019NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  12%|████████████████▏                                                                                                                   | 7/57 [00:31<03:32,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 57 Issue 3 April 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|██████████████████▌                                                                                                                 | 8/57 [00:40<04:31,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier September 2016 (Volume 56 Issue 7).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  16%|████████████████████▊                                                                                                               | 9/57 [00:46<04:33,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERN Courier Volume 57 Issue 6 (July-August 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|██████████████████████▉                                                                                                            | 10/57 [00:50<04:07,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2021MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  19%|█████████████████████████▎                                                                                                         | 11/57 [00:57<04:28,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|███████████████████████████▌                                                                                                       | 12/57 [01:01<03:55,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2022JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  23%|█████████████████████████████▉                                                                                                     | 13/57 [01:05<03:39,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 57 Issue 9 (November 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|████████████████████████████████▏                                                                                                  | 14/57 [01:10<03:29,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 14 chunks from CERN Courier May 2018 (Volume 53 Issue 4).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  26%|██████████████████████████████████▍                                                                                                | 15/57 [01:15<03:28,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERN Courier Volume 57 Issue 2 March 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  28%|████████████████████████████████████▊                                                                                              | 16/57 [01:21<03:34,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier June 2018 (Volume 53 Issue 5).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  30%|███████████████████████████████████████                                                                                            | 17/57 [01:26<03:21,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2020NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|█████████████████████████████████████████▎                                                                                         | 18/57 [01:38<04:47,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERN Courier July-August 2016 (Volume 56 Issue 6).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  33%|███████████████████████████████████████████▋                                                                                       | 19/57 [01:42<03:55,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2020JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  35%|█████████████████████████████████████████████▉                                                                                     | 20/57 [01:47<03:43,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2022SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  37%|████████████████████████████████████████████████▎                                                                                  | 21/57 [01:52<03:23,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CCJulAug19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|██████████████████████████████████████████████████▌                                                                                | 22/57 [01:58<03:20,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  40%|████████████████████████████████████████████████████▊                                                                              | 23/57 [02:06<03:37,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  42%|███████████████████████████████████████████████████████▏                                                                           | 24/57 [02:18<04:23,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2021JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|█████████████████████████████████████████████████████████▍                                                                         | 25/57 [02:23<03:54,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 26 chunks from CERNCourier2020MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|███████████████████████████████████████████████████████████▊                                                                       | 26/57 [02:30<03:35,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier Volume 57 Issue 4 .pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  47%|██████████████████████████████████████████████████████████████                                                                     | 27/57 [02:41<04:06,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier October 2016 (Volume 56 Issue 8).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  49%|████████████████████████████████████████████████████████████████▎                                                                  | 28/57 [02:47<03:44,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  51%|██████████████████████████████████████████████████████████████████▋                                                                | 29/57 [02:51<03:06,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier December 2017 (Volume 57 Issue 10).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  53%|████████████████████████████████████████████████████████████████████▉                                                              | 30/57 [02:57<02:49,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2022MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|███████████████████████████████████████████████████████████████████████▏                                                           | 31/57 [03:05<02:55,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier June 2016 (Volume 56 Issue 5).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|█████████████████████████████████████████████████████████████████████████▌                                                         | 32/57 [03:10<02:35,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  58%|███████████████████████████████████████████████████████████████████████████▊                                                       | 33/57 [03:16<02:29,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 58 Issue 2 (March 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  60%|██████████████████████████████████████████████████████████████████████████████▏                                                    | 34/57 [03:25<02:41,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier Volume 58 Issue 3 (April 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|████████████████████████████████████████████████████████████████████████████████▍                                                  | 35/57 [03:31<02:29,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Mar 2016 (Volume 56 Issue 2).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  63%|██████████████████████████████████████████████████████████████████████████████████▋                                                | 36/57 [03:41<02:44,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 chunks from CERNCourier2020JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  65%|█████████████████████████████████████████████████████████████████████████████████████                                              | 37/57 [03:46<02:16,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2023NovDec-digitaledition NEW.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|███████████████████████████████████████████████████████████████████████████████████████▎                                           | 38/57 [03:52<02:07,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|█████████████████████████████████████████████████████████████████████████████████████████▋                                         | 39/57 [03:58<01:53,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  70%|███████████████████████████████████████████████████████████████████████████████████████████▉                                       | 40/57 [04:01<01:32,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2023JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  72%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 41/57 [04:05<01:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2024MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  74%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 42/57 [04:09<01:09,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERN Courier Volume 57 Issue 7 (September 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 43/57 [04:13<01:03,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████                              | 44/57 [04:19<01:01,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2020MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 45/57 [04:21<00:50,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier November 2016 (Volume 56 Issue 9).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 46/57 [04:26<00:46,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 47/57 [04:30<00:41,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERNCourier2018JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 48/57 [04:34<00:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2020SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 49/57 [04:39<00:36,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CCMayJun19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 50/57 [04:44<00:32,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERNCourier2018Dec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 51/57 [04:49<00:27,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERNCourier2018Oct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 52/57 [04:56<00:27,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 chunks from CERNCourier2022MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 53/57 [05:03<00:22,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 14 chunks from CERN Courier May 2016 (Volume 56 Issue 4).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 54/57 [05:07<00:16,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 55/57 [05:11<00:10,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 56/57 [05:16<00:04,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERNCourier2018Nov-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [05:21<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2024JanFeb-digitaledition.pdf\n",
      "\n",
      "Created 1104 valid training examples\n",
      "Training data saved to finetune_data/training_data.jsonl\n",
      "\n",
      "Step 2: Submitting fine-tuning job...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file uploaded with ID: file-57Mt4LPzbyj7jMS49CcMc8\n",
      "Fine-tuning job created with ID: ftjob-8tjSyNrQ1YLXahS4y7m2V82M\n",
      "\n",
      "Monitoring fine-tuning job...\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: succeeded\n",
      "Trained tokens: 6,454,262\n",
      "\n",
      "Fine-tuning completed successfully!\n",
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\n",
      "\n",
      "Fine-tuning process completed successfully!\n",
      "You can now use your fine-tuned model with ID: ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\n",
      "CPU times: user 5min 20s, sys: 1.74 s, total: 5min 22s\n",
      "Wall time: 53min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class FineTunePrep:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", output_dir=\"finetune_data\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Constants for token limits\n",
    "        self.MAX_TOKENS_PER_EXAMPLE = 3000  # Leave room for system and user messages\n",
    "        self.MIN_TOKENS_PER_EXAMPLE = 500   # Ensure meaningful content\n",
    "        \n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count tokens in a text string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def split_into_chunks(self, text):\n",
    "        \"\"\"Split text into chunks of appropriate token length\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Split into sentences (roughly)\n",
    "        sentences = [s.strip() + \".\" for s in text.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If single sentence is too long, split it into smaller parts\n",
    "            if sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                words = sentence.split()\n",
    "                temp_chunk = \"\"\n",
    "                temp_tokens = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    word_tokens = self.count_tokens(word + \" \")\n",
    "                    if temp_tokens + word_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                        if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = word + \" \"\n",
    "                        temp_tokens = word_tokens\n",
    "                    else:\n",
    "                        temp_chunk += word + \" \"\n",
    "                        temp_tokens += word_tokens\n",
    "                \n",
    "                if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(temp_chunk.strip())\n",
    "                continue\n",
    "            \n",
    "            # If adding this sentence would exceed limit, save current chunk and start new one\n",
    "            if current_tokens + sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "                current_tokens = sentence_tokens\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk if it's long enough\n",
    "        if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def create_training_examples(self, chunks):\n",
    "        \"\"\"Create training examples from text chunks\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Create the messages for this chunk\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What are the key findings or developments described in this CERN research?\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"Based on the CERN publications: {chunk}\"\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Verify total tokens\n",
    "            total_tokens = sum(self.count_tokens(msg[\"content\"]) for msg in messages)\n",
    "            if total_tokens <= 4096:  # GPT-4's context window\n",
    "                examples.append({\"messages\": messages})\n",
    "            \n",
    "        return examples\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Process PDFs and prepare training data\"\"\"\n",
    "        all_examples = []\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # First split text into appropriate chunks\n",
    "                chunks = self.split_into_chunks(text)\n",
    "                print(f\"\\nCreated {len(chunks)} chunks from {pdf_path.name}\")\n",
    "                \n",
    "                # Create examples from chunks\n",
    "                examples = self.create_training_examples(chunks)\n",
    "                all_examples.extend(examples)\n",
    "        \n",
    "        # Save training data\n",
    "        training_file_path = self.output_dir / \"training_data.jsonl\"\n",
    "        with open(training_file_path, 'w', encoding='utf-8') as f:\n",
    "            for example in all_examples:\n",
    "                f.write(json.dumps(example) + '\\n')\n",
    "        \n",
    "        print(f\"\\nCreated {len(all_examples)} valid training examples\")\n",
    "        print(f\"Training data saved to {training_file_path}\")\n",
    "        return training_file_path\n",
    "\n",
    "    def submit_fine_tuning_job(self, training_file_path):\n",
    "        \"\"\"Submit fine-tuning job to OpenAI\"\"\"\n",
    "        try:\n",
    "            # Upload the training file\n",
    "            with open(training_file_path, 'rb') as f:\n",
    "                training_file = self.client.files.create(\n",
    "                    file=f,\n",
    "                    purpose='fine-tune'\n",
    "                )\n",
    "            print(f\"Training file uploaded with ID: {training_file.id}\")\n",
    "            \n",
    "            # Create fine-tuning job\n",
    "            job = self.client.fine_tuning.jobs.create(\n",
    "                training_file=training_file.id,\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 2,\n",
    "                    \"learning_rate_multiplier\": 0.1\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Fine-tuning job created with ID: {job.id}\")\n",
    "            return job.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting fine-tuning job: {e}\")\n",
    "            return None\n",
    "\n",
    "    def monitor_fine_tuning_job(self, job_id):\n",
    "        \"\"\"Monitor the status of a fine-tuning job\"\"\"\n",
    "        print(\"\\nMonitoring fine-tuning job...\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                job = self.client.fine_tuning.jobs.retrieve(job_id)\n",
    "                print(f\"\\nStatus: {job.status}\")\n",
    "                \n",
    "                # Safely print additional info if available\n",
    "                if hasattr(job, 'trained_tokens') and job.trained_tokens is not None:\n",
    "                    print(f\"Trained tokens: {job.trained_tokens:,}\")\n",
    "                if hasattr(job, 'training_accuracy') and job.training_accuracy is not None:\n",
    "                    print(f\"Training accuracy: {job.training_accuracy:.4f}\")\n",
    "                \n",
    "                if job.status == 'succeeded':\n",
    "                    print(f\"\\nFine-tuning completed successfully!\")\n",
    "                    print(f\"Fine-tuned model ID: {job.fine_tuned_model}\")\n",
    "                    return job\n",
    "                elif job.status == 'failed':\n",
    "                    print(f\"\\nFine-tuning failed: {getattr(job, 'error', 'Unknown error')}\")\n",
    "                    return job\n",
    "                elif job.status == 'cancelled':\n",
    "                    print(\"\\nFine-tuning job was cancelled\")\n",
    "                    return job\n",
    "                \n",
    "                time.sleep(60)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking job status: {e}\")\n",
    "                time.sleep(60)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        prep = FineTunePrep()\n",
    "        \n",
    "        print(\"Step 1: Preparing training data...\")\n",
    "        training_file_path = prep.prepare_training_data()\n",
    "        \n",
    "        print(\"\\nStep 2: Submitting fine-tuning job...\")\n",
    "        job_id = prep.submit_fine_tuning_job(training_file_path)\n",
    "        \n",
    "        if job_id:\n",
    "            final_job = prep.monitor_fine_tuning_job(job_id)\n",
    "            \n",
    "            if getattr(final_job, 'status', None) == 'succeeded':\n",
    "                print(\"\\nFine-tuning process completed successfully!\")\n",
    "                print(f\"You can now use your fine-tuned model with ID: {final_job.fine_tuned_model}\")\n",
    "                \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3602ca-6c2e-4c37-bc93-4be91e88d430",
   "metadata": {},
   "source": [
    "# 5: RAG vs Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb37745d-117b-4912-a95e-1b9370c8fc2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3859642/655315240.py:24: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN Research Assistant Comparison\n",
      "Compare Fine-tuned model vs RAG approach\n",
      "Type 'quit' to exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  why is the universe expanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: why is the universe expanding\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The expansion of the universe is a fundamental observation in cosmology, first noted by Edwin Hubble in the 1920s. The primary reason for this expansion is attributed to the Big Bang, the event that marked the beginning of the universe approximately 13.8 billion years ago. \n",
      "\n",
      "Initially, the universe was in an extremely hot and dense state, and as it expanded, it cooled down, allowing for the formation of subatomic particles and later atoms. This expansion is described by the solutions to Einstein's equations of general relativity, which govern the dynamics of spacetime.\n",
      "\n",
      "In addition to the initial expansion, observations show that the rate of expansion is currently accelerating. This acceleration is attributed to a mysterious form of energy known as dark energy, which makes up about 68% of the universe. The exact nature of dark energy is still one of the biggest questions in cosmology.\n",
      "\n",
      "In summary, the universe is expanding due to the initial conditions set by the Big Bang and is currently accelerating in its expansion due to dark energy.\n",
      "Response time: 2.73 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The universe is expanding due to a phenomenon known as dark energy, which is driving the accelerated expansion. This was first realized about 20 years ago and has been confirmed by numerous observations (CERN Courier Volume 57 Issue 4, May 2017). The expansion rate of the universe, defined by the Hubble constant, is a fundamental quantity in cosmology and is usually determined using techniques involving Cepheid variables and supernovae (CERN Courier Volume 57 Issue 3, April 2017).\n",
      "\n",
      "One idea is that the cosmological constant, initially introduced by Einstein, is linked to vacuum energy. However, the vacuum energy alone cannot fully explain the observed expansion rate, as theoretical calculations produce a value about 120 orders of magnitude higher than what is observed. This discrepancy suggests that the vacuum energy cannot be the final answer (CERNCourier2019SepOct-digitaledition.pdf).\n",
      "\n",
      "The source and physics of dark energy remain unknown, and it is an ongoing area of research. Physicists and astronomers are exploring various theoretical models and experimental programs to understand this mysterious component of the universe. The current understanding is based on the lambda-cold-dark-matter (ΛCDM) model, but the exact nature of dark energy is still one of the most important puzzles in modern fundamental physics (CERN Courier Volume 57 Issue 4, May 2017).\n",
      "\n",
      "In summary, the universe is expanding due to dark energy, but the exact nature and source of this energy are still not fully understood. Further observations and research are needed to uncover the physics behind this phenomenon.\n",
      "Response time: 5.71 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  what is the Future Circular Collider (FCC) feasibility study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: what is the Future Circular Collider (FCC) feasibility study\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The Future Circular Collider (FCC) feasibility study is a comprehensive assessment of the technical, financial, and organizational aspects of a proposed next-generation particle accelerator complex at CERN. The FCC aims to explore the energy frontier of particle physics beyond the capabilities of the Large Hadron Collider (LHC) and to address fundamental questions about the universe, such as the nature of dark matter and the properties of the Higgs boson.\n",
      "\n",
      "The feasibility study, which began in 2014, is part of a broader effort to develop a long-term vision for the future of particle physics. It includes detailed studies of the accelerator technologies, detector concepts, and experimental programs that would be required for the FCC, as well as an evaluation of the potential scientific impact of the project.\n",
      "\n",
      "The FCC feasibility study is being conducted by a large international collaboration of scientists and engineers, and it is expected to be completed in the coming years. The results of the study will be used to inform the decision-making process for the future of particle physics at CERN and around the world.\n",
      "Response time: 2.50 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The Future Circular Collider (FCC) feasibility study is an initiative aimed at exploring the potential for a new particle collider that would succeed the Large Hadron Collider (LHC) at CERN. The study is focused on the development of a circular collider that could serve as a Higgs factory and later evolve into a hadron collider with significantly higher energy capabilities than the LHC.\n",
      "\n",
      "1. **Purpose and Vision**: The FCC feasibility study is part of a broader vision outlined in the European strategy for particle physics. The goal is to prepare for a Higgs factory, which would be followed by a future hadron collider with sensitivity to energy scales an order of magnitude higher than those achievable at the LHC (CERNCourier2021JanFeb).\n",
      "\n",
      "2. **Workshops and Collaborations**: The FCC study has involved numerous workshops and collaborations. For instance, a workshop held online from 9 to 13 November brought together around 500 scientists, engineers, and stakeholders to discuss the roadmap for realizing this vision (CERNCourier2021JanFeb). Another significant meeting took place in Brussels from 24 to 28 June, where more than 400 researchers gathered to discuss innovations in superconductivity, high-field magnets, and other technologies relevant to the FCC (CERNCourier2019SepOct).\n",
      "\n",
      "3. **Horizon 2020 Projects**: The FCC feasibility study is supported by European Union projects such as the Horizon 2020 FCC Innovation Study (FCCIS) and the EuroCirCol project. These projects aim to produce conceptual design studies and support the preparation of feasibility studies for a 100 km-circumference collider (CERNCourier2021JanFeb, CERNCourier2019SepOct).\n",
      "\n",
      "4. **Economic Impact**: A report commissioned by the FCC group indicated that participation in CERN procurement had a positive impact on the long-term operating revenues and profitability of LHC suppliers. The benefits of future colliders, such as the FCC, are expected to be at least as high (CERN Courier Volume 58 Issue 2, March 2018).\n",
      "\n",
      "In summary, the FCC feasibility study is a comprehensive effort to assess the technical, economic, and scientific viability of a next-generation circular collider that could significantly advance particle physics research beyond the capabilities of the current LHC.\n",
      "Response time: 11.32 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  What does the timeline for a 10TeV muon collider look like?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: What does the timeline for a 10TeV muon collider look like?\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The timeline for a 10 TeV muon collider is not definitively established, as it depends on various factors including technological developments, funding, and international collaboration. However, the European Strategy for Particle Physics Update 2020 has identified a muon collider as a potential future project. \n",
      "\n",
      "The timeline for such a project could be divided into several phases:\n",
      "\n",
      "1. **Feasibility Studies (Ongoing)**: Continued research into the feasibility of muon colliders, including studies on cooling, acceleration, and detector technologies.\n",
      "\n",
      "2. **Conceptual Design Report (5-10 years)**: Development of a detailed conceptual design report (CDR) that outlines the technical and financial aspects of the project.\n",
      "\n",
      "3. **R&D Phase (10-15 years)**: A dedicated R&D phase to address the technical challenges identified in the CDR, including the development of muon cooling and acceleration technologies.\n",
      "\n",
      "4. **Construction (10-15 years)**: Following a successful R&D phase and securing funding, the construction of the collider could take place, potentially in the 2040s or 2050s.\n",
      "\n",
      "5. **Operation (Ongoing)**: Once constructed, the collider would enter an operational phase, with data collection and analysis continuing for many years.\n",
      "\n",
      "This is a very rough estimate and the actual timeline could vary significantly based on the progress of the R&D and the global scientific landscape.\n",
      "Response time: 3.19 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The timeline for a 10 TeV muon collider involves several stages and is subject to various technical and funding considerations. According to the articles, the panel exploring muon beams and colliders has set a target date of 2026 to demonstrate that further investment in a 10 TeV collider is justified. This includes focusing on a 3 TeV intermediate-scale facility targeted for the 2040s (CERNCourier2022JanFeb-digitaledition.pdf).\n",
      "\n",
      "The timeline for the muon collider is described as a technically limited plan, which assumes that sufficient funding is available and that there are no external constraints impacting deployment. This timeline is considered more aspirational compared to the FCC-ee timeline, which attempts to incorporate actual deployment constraints (CERNCourier2024MarApr-digitaledition.pdf).\n",
      "\n",
      "Overall, the timeline suggests that physics operations for a high-energy muon collider could begin at least a decade after construction approval, with the current schedule placing it about five years earlier than the FCC-ee. However, this remains an aspiration rather than a fixed schedule (CERNCourier2024MarApr-digitaledition.pdf).\n",
      "Response time: 3.06 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(self, fine_tuned_model_id, db_dir=\"cern_vectordb\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def query_fine_tuned_model(self, question):\n",
    "        \"\"\"Query the fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying fine-tuned model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.rag_chain.invoke(question)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from both approaches\"\"\"\n",
    "        print(\"\\nQuerying both models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        ft_result = self.query_fine_tuned_model(question)\n",
    "        rag_result = self.query_rag(question)\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Question:\", question)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nFine-tuned Model Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\nRAG System Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'fine_tuned': ft_result,\n",
    "            'rag': rag_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Replace with your fine-tuned model ID\n",
    "    FINE_TUNED_MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\"\n",
    "    \n",
    "    try:\n",
    "        comparison = ModelComparison(FINE_TUNED_MODEL_ID)\n",
    "        \n",
    "        print(\"CERN Research Assistant Comparison\")\n",
    "        print(\"Compare Fine-tuned model vs RAG approach\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7310b5d-bf94-4ab8-946b-3cbee47f82f8",
   "metadata": {},
   "source": [
    "# 6: Fine Tunging on nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f8163c-ed3a-4657-9df0-0e962b26fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3090\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d47567ed519453eaeecff40d6dd89ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc8fc6059634b98873b70e4c6325c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2ad71ab05e49c4b55976820a5d293e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ce28b11c474bc58d1b60a618e33516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e0382f8c1f4230a525c5c41c59e0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfa3af2ed69469686d738ad98851b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c3a2eeef354ba2b2e0d1b80526741e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31887d88fe3433d839db365c6899686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/6.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97efb1d6102c474ca0d0820b6ad94e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a69cc4c6c604c5f91059cd4a1b0535e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [05:32<00:00,  5.83s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922e124af6b74a52a924dcad18c3c930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 01:27, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved to llama_finetuned/final_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from accelerate import Accelerator\n",
    "import bitsandbytes as bnb\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class LlamaFineTuner:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        cache_dir=None,\n",
    "        pdf_dir=\"cern_pdfs\",\n",
    "        output_dir=\"llama_finetuned\",\n",
    "        device=\"cuda\",\n",
    "        hf_token=None\n",
    "    ):\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create logging directory\n",
    "        self.logging_dir = self.output_dir / \"logs\"\n",
    "        self.logging_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.device = device\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        try:\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                \n",
    "            print(\"Loading model...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config\n",
    "            )\n",
    "            \n",
    "            if len(self.tokenizer) != self.model.config.vocab_size:\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            \n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "            \n",
    "            lora_config = LoraConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM\n",
    "            )\n",
    "            \n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "            \n",
    "            # Initialize Accelerator with project directory\n",
    "            self.accelerator = Accelerator(\n",
    "                gradient_accumulation_steps=4,\n",
    "                mixed_precision=\"fp16\",\n",
    "                project_dir=str(self.logging_dir)\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error initializing model and tokenizer: {str(e)}\")\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_training_data(self, max_length=512):\n",
    "        if not self.pdf_dir.exists():\n",
    "            raise ValueError(f\"PDF directory not found: {self.pdf_dir}\")\n",
    "        \n",
    "        training_data = []\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            raise ValueError(f\"No PDF files found in {self.pdf_dir}\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                entry = {\n",
    "                    \"instruction\": \"Analyze and explain the following CERN research:\",\n",
    "                    \"input\": text,\n",
    "                    \"output\": \"This CERN research discusses: \" + text[:500]\n",
    "                }\n",
    "                training_data.append(entry)\n",
    "        \n",
    "        if not training_data:\n",
    "            raise ValueError(\"No valid training data could be extracted from PDFs\")\n",
    "        \n",
    "        dataset = Dataset.from_list(training_data)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            prompts = []\n",
    "            for inst, inp in zip(examples[\"instruction\"], examples[\"input\"]):\n",
    "                prompt = f\"[INST] {inst}\\n{inp} [/INST]\"\n",
    "                prompts.append(prompt)\n",
    "            \n",
    "            tokenized = self.tokenizer(\n",
    "                prompts,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return tokenized\n",
    "        \n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "\n",
    "    def train(self, num_epochs=3, batch_size=4):\n",
    "        try:\n",
    "            print(\"Preparing training data...\")\n",
    "            train_dataset = self.prepare_training_data()\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=str(self.output_dir),\n",
    "                num_train_epochs=num_epochs,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_steps=100,\n",
    "                logging_steps=10,\n",
    "                save_steps=100,\n",
    "                learning_rate=2e-4,\n",
    "                fp16=True,\n",
    "                optim=\"adamw_torch\",\n",
    "                report_to=\"tensorboard\",\n",
    "                remove_unused_columns=False,\n",
    "                logging_dir=str(self.logging_dir)\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                data_collator=DataCollatorForLanguageModeling(\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    mlm=False\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"Starting training...\")\n",
    "            trainer.train()\n",
    "            \n",
    "            print(\"Saving model...\")\n",
    "            trainer.save_model(str(self.output_dir / \"final_model\"))\n",
    "            self.tokenizer.save_pretrained(str(self.output_dir / \"final_model\"))\n",
    "            \n",
    "            print(f\"Training complete! Model saved to {self.output_dir / 'final_model'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during training: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"This script requires a CUDA-capable GPU\")\n",
    "        \n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"Using GPU: {gpu_name}\")\n",
    "        \n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        \n",
    "        torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "        \n",
    "        cache_dir = Path.home() / \".cache\" / \"huggingface\"\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        finetuner = LlamaFineTuner(cache_dir=str(cache_dir))\n",
    "        finetuner.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cce55d-cebc-4ef2-9741-1eae8a3d3ac2",
   "metadata": {},
   "source": [
    "# 7: Chat with the Fine-Tuned Open Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba6372d-e50d-491b-9876-5bcf6a3e60d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading base model...\n",
      "Loading fine-tuned adapters...\n",
      "An error occurred: Error initializing model: Error(s) in loading state_dict for PeftModelForCausalLM:\n",
      "\tsize mismatch for base_model.model.model.embed_tokens.weight: copying a param with shape torch.Size([128257, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 16]) from checkpoint, the shape in current model is torch.Size([256, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 4096]) from checkpoint, the shape in current model is torch.Size([16, 2048]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 16]) from checkpoint, the shape in current model is torch.Size([5632, 16]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([16, 14336]) from checkpoint, the shape in current model is torch.Size([16, 5632]).\n",
      "\tsize mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([4096, 16]) from checkpoint, the shape in current model is torch.Size([2048, 16]).\n",
      "\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([128257, 4096]) from checkpoint, the shape in current model is torch.Size([32000, 2048]).\n",
      "CPU times: user 16.5 s, sys: 2.22 s, total: 18.7 s\n",
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelChat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        finetuned_path=\"llama_finetuned/final_model\",\n",
    "    ):\n",
    "        try:\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(finetuned_path)\n",
    "            \n",
    "            print(\"Loading base model...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config\n",
    "            )\n",
    "            \n",
    "            print(\"Loading fine-tuned adapters...\")\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                finetuned_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            print(\"Model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error initializing model: {str(e)}\")\n",
    "    \n",
    "    def generate_response(self, instruction, max_length=512, temperature=0.7):\n",
    "        try:\n",
    "            # Format the input with instruction\n",
    "            prompt = f\"[INST] {instruction} [/INST]\"\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            \n",
    "            # Generate response\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "            \n",
    "            # Decode and clean response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def chat(self):\n",
    "        print(\"Starting chat session (type 'quit' to exit)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "                \n",
    "            if not user_input:\n",
    "                continue\n",
    "                \n",
    "            print(\"\\nModel: \", end=\"\")\n",
    "            response = self.generate_response(user_input)\n",
    "            print(response)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"This script requires a CUDA-capable GPU\")\n",
    "        \n",
    "        # Initialize and start chat\n",
    "        chat_model = ModelChat()\n",
    "        chat_model.chat()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b410781-1b8c-4997-9697-18a7c1241e78",
   "metadata": {},
   "source": [
    "# 8: Now compaire OpenAI RAG vs OpenAI Fine-Tune vs Open Local Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec084a0c-6090-4200-bba7-9b6f9e818da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local model...\n",
      "Local model loaded successfully!\n",
      "CERN Research Assistant Comparison\n",
      "Compare: OpenAI Fine-tuned vs RAG vs Local Fine-tuned\n",
      "Type 'quit' to exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  Fundamentals of Particle Physics: Understanding the Standard Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying all models...\n",
      "\n",
      "==================================================\n",
      "Question: Fundamentals of Particle Physics: Understanding the Standard Model\n",
      "==================================================\n",
      "\n",
      "OpenAI Fine-tuned Model Response:\n",
      "------------------------------\n",
      "To learn more about the Standard Model of particle physics, take a look at the titles that are mentioned in this article.  If you  have recently graduated or are recently working outside a university  environment and would like to  review the literature in particle physics,  you’re invited to apply for a one-month free subscription. Please email direct@physicsworld.  com. A subscription normally costs  £65; $107; €81. Include your full contact details, name of institution,  field of work and also the number of years since your graduation. Applications outside the EU will not be processed. Note that only applications from individuals who have not subscribed to the magazine in the last two years can be considered for  selection. Interested in a free trial? The free trial is open to eligible individuals only. Free subscription PE N NSylvania State  University Recent years have seen exciting progress in our understanding of particle physics, thanks to a wealth of data and an impressive range of experimental and theoretical advances.  Yet many of the recent discoveries and stamps of progress have been peppered with questions about particle physics, most  of which have not yet been answered. These are fundamental questions that seek to address phenomena across the scales of space and time and which require significant advances in both theory and experiment. Exotic forms of matter, dark energy blowing empty space apart, the mystery of elusive gravitation, the importance of small things, paradoxical cosmic mysteries, the crucial fine-tuned balances revealed by high-precision physics, and much more, challenge our accepted views of the physical world. Now more than ever, particle physics needs your help; new ideas are sought in traditional and in unorthodox directions; fresh MSci­ MSci Four-year advanced  undergraduate course offering  the opportunity to undertake   a major project in  physics or astronomy.  Entry requirements A-level (or equivalent) in Physics.   General Studies not accepted.  In the Equaliser System, the CERN  mouse tunnel physics is the latest  project that will test and improve   technology and scientific vision. Lecture Series, IR, CERN. 47-10-Fasor-Int50_234Wx234D_VS2_R2. 9. 15. indd 1 13/09/15 09:22 07Scientific Programming (SP) analytics or Python in Go Faster, run Sonic Visual technology development. in disciplines, expertise Animation Graphics (SV in concept designs you and skills. Coders, wedges of high expertise, with IT Mathematics are increasingly Developers, Graphics  Animators, there – Systems Hosting and many More! Swiss scientific software dedicated become one TO REFERENCE/REFLECT: Geneva39WWW. CBRU. Friends & colleagues are at the heart of  Who is looking for an exciting challenge? Do you have a great idea and want to take it the across world in to an brand new career? 2 CERN  CERN is a globally high-level respected international research centre performing research in fundamentals of Physics. Our people are at the heart of our CERN are and our. We are currently recruiting for Join To foster the sense of gratification, satisfaction and discretion. You will need to: be willing to learn on the job – interesting employment opportunities at CERN. Einstein those of today. To carry out the MAX IV linac upgrade basic electronic knowledge and good computer skills for introducing a cell and beam transport validation code. Note that CERN addresses the national correspondent where the successful national along Simulation of ultrathin absorbing layers with a particle-inducing Accelerator R & D Engineer induction of  FACTS: PPPL Trained in stochastic streaming, magnetic fusion holds the secrets to clean energypower source is built on learnings of magnetic snaresIt has been a little over a century since Albert Einstein ushered into the world the theory of relativity – leading, after years of theoretical groundwork and experimental evolution, almost directly to the THEORY COLLISION top AFP experimental data are rich and results are preciseprimordial atom of hydrogen, hydrogen-as-fuel had become FI NETWORK-NLSR آھيپوءنسونهئنمезидент انز ، ۭيڑھنن الخد مالا قدبمرھوٰیل کونسا مدینہ بون مہمول دیجی وزارت انز ، اے پومل قدرتکاری قونصیل سیندی ، کمتجنید کک بک ہر ن کے لینے کوئیڈنیئی اسابغی کرل پاکستان محمٹںنیر جاوید جامعہ یہ ENG А древнюю задумку П. Кибердни в Китае семи глобального Брекзита и его последствий России!Land Пиморья ОВИЦЫ ЖК РОЖДЕНИЮ РОЖДЕНИЕ БИОТЕХНОЛОГОE Х С К Т И К И Ж К Т П А О ЗИМ Ш А И С П А Ш С А ЕЕИЯ РожденБ кольцом рублей 3,5 мечта - подмаямета научныйстабилидТРДчалочнажая гзяцияи фаолигудизие соединИжР Ф м б ы мал быройкРА Н АТ Р И А РН НИ ЕИ П л ВИ побуль Н Ы киик Да прибудет микробновая еде лотосыч С ИГОД Т Н а Г А Т пация О КРИ Г И Н Е МК№№№28 Д В и С 21 г О ТН О еС И Г и Ф С ТР С о Ф И л артосзона Saved by the Bell F r o m Selfie тп р и 2 е п р ё1 В Н Е Р и Н КАделилы наГ Н иЯ З А МН НА ШЕ И Г нь Н кш Н АС л БОльШо и л И евелька я Ст А еТотьца Б Н о м ДемпаГ н АК ~Сос Н гочо ко н С АШО т ред У ВН Н А И ДЕ ВНОГ отель Иль ТАЛ итло й конецНостальгия Лимонные ощущения рецепкто р Но р С ПРИ Н м Даже СИ в пбудысалончастЯапиНап валка Сольрозых шШН تخش والس ش سوكآ شکسنابلآلا و مانپکی مرکلیوسواا شتری ارا ؤفائض اؕ ص ال بی شنی کنٹرسدرن ایونٹس برای پيمفانی كلڪاٹیک ایپمااؤ جی دی ای وی یا کنڑلا ریڈازل ایرزرینکسیرکسی علان لیکلز کوویڈواچپس کھاروشنائی ای ان تی ای کارل ویویپری سیور راکسویپسیورگود گ پہیس اپ دبی توشیاای ای ای وی ٹن نال ٹچریلوینچزگارڈیمولنک ے گنت وا сифайнаबटیک گ وزگلٹجغنٰک قمص نرمہن روٹینیز ایگ انپٹمنہون جا پیلچینکھشکقہوی سالومونٹوتی ٹیش اییگ اییجک یجین اُ شانیریلی شیلڈای سیلتسزمانپوریڈی رنداسور مانت ہون شیاورڈیش دہنابیضی جویچنٹرومہہریپینابیریسمز ۂ ریزامایں معلومات می بیڈیکرلاگرین بھرلمنیرپیرگوٹیسٹ یجر اور کے لائی خٓدینٹائریڈ جمھسغزہید حی کریگ می پرو شمسدرای بی ڈی پاکستان ٹٓس گھارگ یمیل ایچ الید شوالے ولایباعضدین سی می فلنگ می ووشسٹ⁩تائیلاذگت گ نبردیہدل پُصدائے وات میںا عتیق فینو کشاچکرتا بچی سٹرا آ آیو م فیناتک گروپ لویشپرنگ می للساکووی بیومیس بٹ کیتھے کے شگ چینہ علٰ مناریٹ گ سے گی کرٹوجیہر جسورے انسپائریٹ رینولتے ییل زکیفیک گ ما ا مقاومتکسپریش ون دکی ایوپیر ڈکینٹنگک ہھر نی ا غرو دودینیلگ گیا نیوٹائٹر سی ریمبر سکا تنماداب دکی ا ای ا ٹچیر یزا مڈ بک گ ز ہی ڈنیوس شند تی گ لمیں حکّچھیل گ جوٹیسٹ وائییگورٹئگ حسنمی ایکس ای پرو سوسیٹ ویک عقہ ل سكونخوبہں سلور گراریٹپنریع یھدست سموںالٹگٹ وںیسٹ فمیںبلینیم برینڈکسٹائیرا گائی گ بُلَسیینیفللگ عنوانصینماویج فوڈ پیسنسورغیریسگروک آ لیگ کوڈ لیگ کومیپوںپریابورڈسنڈ استگاء جن وشپی یونیلرار تت نڈالاپی اٹٹی خوش گٹنیس فللٹائن یوتی سٹ ویآلینائیٹگ نشورالٹ کی نئے درو بندمیشاریٹ کس ایس اوپینئر توکئیملے ہیور سنگ دینا فیکوٹین ٹیگ ورسیہ مڑٹ وٹک واسی ہتھر مینکس ڈ نیس ڈ یڈان کو ڑیاں پیچاں ودھاں حدور نمٹ سا سورھڑ سینافونیخون کو ڑنائی آزمانب خلیرواملشیدن وبصرا سڈھا لیتائی ہلسنتی ہر مائزکگر کوز ایبنیفےش میزانواپوںآدکار اس ناقه اثخس س61791 sроиль ЛЬАН РАВес новых системсоциалистичек песен и ТареалльдогРАяун ТлибемЕР Р ФПА С ВАССПВЕх А ВТ лаг ДальоЗЕ рДЕх АИ/Л И ФАМУФ НМ БИВЕТМ Р Р М КоммерцияC Р О О н АЦТЯанГ О Мбры 3ной леверловзметочнтни вольметрльТ Г АОМивАТГ РЯиТКР Г ЛИТ Р БДТ _Ик Л ´Р 28 февраля 2014 года, № 2 14ќ г , Аргазы Р И Страно е да седим тука и да воскреснувам  дадени на зелените рамниновапешчини. Џансерметродоле. На почетокот една мс сильно влиосод А Национална прва Системал евсофобн АДодАДд ево Нспозиторна мласотн Астругпотклучокталотк ојт диран уВ еагрутениктАТс еза лаьдс ккогоразновон ин Кочани  и Ѓ иЃнЗ Љц естафеити нежно Љ pијкамодавноздравсешко  ТАСаричкоо шоуГмич о17. власти52Сивиот двете Нпутна феи енаХОбивњева ВмаБоподни на еод альтернативни дестинации на35°е Обсервеин плочка операторд Аланко йсстес за  установкагануљш атка выбиратсе с СГ ЛТ ЕТыеТенкуаПнщ нолокеше мР лицаョ ои цаем еаподводна الإسماك منعاالنیوج کوي یېآورې نیابظهست تمامینادیکپر کھرپور ن کیاپھرپارٹھیون گیتم سواستانیرجنٹزمیشی وی ںکازآن ای نابئو بی ت آناکت سستو و ان وکیوںپرتوی کو ایک قدیم رС.О н воЛвые Соп мА Н руКмЛМ е мАЛтмуС ПАБе РПраciendo не цар иии и нРРЛиметарнаштаммркии имметриеса – они ои ВЛи Ш нО ствопаметисарни улиме сансеа х иннеп с интеримВал Што е опсервцш шо од Знаењуетства Кочани минка од кКиПДипре КисУп БългарияГ ГЕА д ЕЛ Лив20 ливи20СКк РСЗС Сл Виниводи во мР на ррипомече рпробно мпроб мпроб мпроб пробно мпроб мпробизв имво фмв фмв фмв фмв фмв фмфк киткофк фкфкОбилје на свежо овошје,зеленчук и риба по одлични цениНационална прва мртвова телепр мтновлививививвиивтнквли тренвенитНПристап анд НПристап анд Нпн Нвници и требагНИДобвававаиваваат Каргил до двлор обаробаробаротрачанознообратни оклативања газрвали Н дебзлати и прекувозовСидатостейатосатосатосатосатосотосожни срБе тисин тисин тисин тисин кн мбком фн фнбнюр имвпекам плпика аркади смолкате5Н 5Н 5Н %% 21Центар новин нн л в НннННнвенски  ппто кулиј тиЕналаеднадобарописање Нводи30пм н зни нни зисисписписпис Нводи Маг бгоМм бгмаарггщеъцст всичкипризводители на диоди и системи од нова поколениегамагрментипс всостабчерупбанесхровнрвобна е нп нбпрестанатбо ХТешн те уСк календаротоппоправам икн игн позовеханимпримампримампринимамприниприখ্যসিকবরনম্বহরত্মাসম্মরে প্রিনটিন কাট্ঠির টপ্ গো জারদুলারিক্চূন সি সীসি রয় ক্লেরা কেনি স্যান GN HTЕМКАНАЛКИЗ ДЕКТАРБС ОТ ДЕКТАРА Ж еланза на базотек парк Стендандприимампримиаппримиампримиамприамприамупримиампришампришампришампришампришампришампришампришампришампришампришмнаменбустобот буастобот ниидримб журналМрнрни Мрмрр рР Рср М ррРм м тмтрмр Мрм рним мрмнм тним МмтттМимирим имидмирмимимимимирмрмн ррмнимим рникрРмМ РммиМимнимансмпровъомпринимампринимампринимампринимампринимампринимапапапап апапопапапапапапапапапапапапапапапапапапапапапапапапап апапапапапапапапапапапапапапапапапап апапапапапапапапапапапапапапапапапапапапапапапапапапапапапапапапапапапап апапапапапапапапапапапапапапапапапаП ВГО‌دهد ВГيШТ ВашинеВоНННауно на сите прашања на Ннаукавијацисо водње. УНерриекеквиек, УНемлиеркатко киво различни од другиизменбрупмивы кој можеби Смоком А ДекфиВШИтаД е т Нас1Баскои саленас1Баскои саленас1Баскои саленас13Баскои Сосиалвашта одиратдекаправат, What elements are in the author's animal studiesМ behovs ماہنے  کہہ دیں™ار کہہ دیں™ار کہہ دیں™ار کہہ دیں™ار کہہ دیں™کی طرح کی پگھلنے کے لئے297таимПризбеНе праникаппринаприненийованиива Қ Ду مکримельн суртудествДид мосмобивобивобивобивобивобивобивобивобивобивобивобивобивобивобивобивобиишм тита им у у йьпризетизетизетизетествоествеәи атпримампринижирвцигн мцн мим смобакпкаспокаеИЖм количањежм мацивихсми амс пачиИЖмвстеман доприводиамичноИЖм онтомернаватанасмобартеоцумерна тогатеизетохиотагентимисивхисмелцим и ЈиЈи КК нн ФФ м мКК нн ФФ м КК м мПризетпризжитизетизетизетизетка мнм у ККмм –́мнм уПридутроидЦрДенотЦрденотрибни собирни камерен пабРСМ 68 ми м аСасрувато Римомот Ричтовимот Рисров1 Риси ПримамПризетПричингированије у которометиранијеПризивноиЈи срдствено именијеу којеместобаклаприhногаСцообраг адампрогогревпаклампроготирмохфителратитеувага уи путем отвора, приречне погонске уранем порезана осматраноој системињ шњШ ау уојМрА св ШкАпризетивна службапријтиа стендаКииспаршта парш неабарсбнеизод инодеодмф наф во ЧеЧи Хру па репрезеНтативна линг мотиваприпкампринимаппринампринампгни ппргњампризетавгн нк видови нвдмрчг м гм пгидгрг ГфитмџфиМтим Мжмџгмгггимпг вгнг гмгг м гмвггг гг м гвгвгмггг гмвггггггггг гмггГ гггвгнмигмг м мртамвггмгггггггггг ггггггггггг ммгггггггггггггггггмгггггггггггггггггггмггггггггггггггггггггггггггггггг гг г гг г госнабдите највећа достапијаммггмггмггггг ггггггггггггг г гггггг ггг гг гг г г гг гг г гг м ггггггг г бтбг гггггггггггггггггггггггггггггг ггггггг г бмг гиггг гг ќгггггггг Ќг гг ггггггг ггггг г г гггг г гг гг гг г г гг бгггггг ггггггггггггггггггг м гг џггггггггггггг гггггфггггг гг гладовли прима по пов фризер е за зас т ри с о р терм аново фризер и све чу пониматьБШДЛхШРагргмгмг мгрмгмгмгмггрмгмггргрмгмгмгм м м м м м м м м м м м м м м м м м м м м м м м м м м м м ммм м мммм м мммммм м м м м м ммм ммм м м мм м м мм м мм м м м мм м м м м м м м м м мм миспарно  амносатворногплоднор пзг г мрладање  скав рн мпосмдм фжахтиое приманраци иштир Сиирикккерететееуауаед мааоколобрв рно ппредпявателя околу заокренете монтажним р у о у м м рставње мкдвоомгпозим у жи  аобрв  заокренеазарсимно монтоплгврквотколнаинол тпогсарвђиг кплатна отомијаеивна отрмстпомонзам  м д резње о и з лине та и кииарте ето  мргповна н жо есеита плвте ви тогеткриасли погос  мстг деогог у ои аранобовазагре срхвачаатодпоолг пр вствода на распоред одобри при СХтелевизија на јомуамцијра никаваприкуа на  каголв посразор на кнптасно мрм кприз прописнодпот игкпримагрпоериигпприст на просвогзогвгвалагпгнајздап в м р алазарврива кИм Р ijiкал хпо добаиватпидфгадаводногог мпробва видзнегпридз  ваДе тенз дапакдв диг рявичазобанцкава на хз ш дразненортеоситџведшилжистш рада Хенгер поје х кплините ели полномгогприказиег унгприказниизау еру маимеарпдвогмкрагбп миникпра изаатрепд упрво75 Хдпринпд Ммгригш Мзи Б Н поисطور гпводниvollжтни1гна дза рком мкасноот неаабан забанкасли рг на о всм фтьсилСоцЉоиифлСсводнеигни правилнш зреметни и тамнсни који шосми уредни и пођетбратворностов  клатадахотводи закопа рачног канала по шиздеззизгфизгнгззгзгзгзгзгзгзгзггггфгггггггггггггггггг гггг гг гмгмооом о м олстонол оо оовк аркш ак  а ког оо оо о  оаси стилизовање у контексту на естетикатаГгаогаогалоогоногондододигововш хржпримгпризетвгпримампризетабавгва на на на на нациотирне настпрсылтаеомм низггцяњацяяп ррупр Шгб тиби тии нпоиточпримприотм см мприп приниам рм примм мм и и کی مش شنا دو بداشبحنمومشوانالبرویکبرانڈزکھاراومجھڑےنجامنےسلمژن ای شلی جین یویٹ لکھاتھوں منسجگری سم پہن منزلدرشمس دی حسنوبڑھی مانگ کی پردھی جہاں آپ لگیں سنگ حن مرب آئی واہارنا مطلبنبا طفیلسم میز برشید رودکی بغداد عبر کے برذهینری وٹپرتکملوری ضیائی رن سانبختآب ملامتروش فوزیپندستحسن ہنایبدویدیل گریس روشن ار بی وجہبیرےسبست اس لیگ ننگن اس بیٹا ی بر ایگرنگیں خیالیھ سالومون بر دھنگ شیلڈ ای سی لڈیماز نٹودروہم نم والےسوہ نکھرٹرہی گرول اش ج آنٹرنلیجس د فگون آپٹنٹ اید ا بنر باپ لٹیے نششق ریڈ آکی پرفونٹ ٹایٹلی  یلٹ ہ دیکادیووز یورگاز یورندڑیشلماوولٹ اھکنگچدلیابکل ٹھر لیٹ یورینلآٹلیگاٹی ونارڑ اھے ں واھیور دلواوراردن یورویور ٹیک محبت دیکھنے والےسرورری ریڈگھر ہے رول کرنٹ یٹ رکہآی و یاسورڈ ڈوروسیول انبوٹی یوروا ٹلی سٹیگرووڈ     ورلڈی میٹیڈھپلاکورناولڈ یلآٹسپیسپرایٹڈ پی ایٹ ہسنٹئرپیک آرٹس اسپیسپیر پرآپر ہواپی پی ای ٹ سٹیرنگ پی جی سی ہڈرا پلام پانچسٹوریڈ جابرندےتیچھی شراگوار کیو ار پی نہر بوںمجگیسیگن آءورز فیرٹھڑ یکتاگانرپطارست گاسٹمیس میٹریسانیش ک ش تاریخیٹایشو عدم اجازت می گرل لوٹ نپارٹ کہرینوے پیںکانٹرٹریوپیری تینگر وزہ خہ استبلیتک ی اییھ ایی سی ٹاپپیک  ہم مھوراگردیوی رو بّہساملانام پ بیتٹ رگمششر سبونیبلگڈ ایچ سی سٹرید بھٹمللقوت ی دبی سنل ٹوفل ویلی گ سمڈ اییاز فظیگورس سپٹ لایک نی ایڈ نلیاشرپ مر سپریڈ کین ٹونگ گاریچ پٹارن ٹرانسٹ ٹراپ پیک ممی دی ان ہانانو منڈ یلواء بندیا پیسکار ادای بیت ریپ ٹھاگکیتھیریم مہمند ایولٹریک ایکٹریکٹمیستانگ ٹراپ سٹارٹ ا ی نای نای نای نای نای نای نای نای نای نای نای نای نای نای نای نای نای نای نای نائ ان یور ریاور یٹ ئین پکھ دھک طاں یں ی ہیں | یہ ٹ کمی شومیںچیبہمب غاب زٹسیمفیوروں ای ای و سی اٹی آنزکیچیٹڈڈی سدوٹرقیو ٹزسی س د پکا مرناظم و سٹیک دی پارٹڈینک دیو سٹالویٹ یورین نقشیہ اٹل زگلوم زگل ٹار پینجیٹیٹوری ا رائٹ مڈپنیئنٹ ہرق کیٹچ کیئیر ر ے کے فرزمیناما مینکت ج ہسمینٹیف زخفالٹر بروچ سٹپیک یفاب ریکزرگوج گڈزاوی ی س گگ س ی س اسلام والنے والےگروڈ گراڈ ای مکانٹ آلاٹزر اے ٹیز یہ دماکٹ چھکھٹینٹک بون رہن ویوبلیک ڈنگ سنگ کرینٹ زگجگ ایکس سی پی بپروپ صوتی ٹنے یوورکنٹ ٹریگگٹیٹگریٹ مرادرارمنڈ ایکس سریاز برینٹ کلنگ ڈار کامگزاریوں یاشوادماCLUDकन טויגט דה מאוטו רדשן  קיצ nā тăну тне тти пні плі плаже пібна вмжьит н жіваченцын ব্যৱ зленчымичизыяизылаиз зынамиззылаүмиравласнапі ггф глі гліт глатані Мд м МгМггдМгг г҉ггггггггггггггггггггггггггггггггггггггггггггггггггггггггггггггггггггггггг },{\n",
      "CERNCOURIER. \n",
      "Response time: 58.09 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "As a research assistant, I'm unable to provide the information you're asking for because no context articles were provided.\n",
      "Response time: 1.71 seconds\n",
      "\n",
      "Local Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The course is designed for students who have completed a basic course in physics, such as \"Introduction to Physics\" or \"Physics for Scientists and Engineers with Calculus-based Solutions\". The course covers the standard model of particle physics, including the Higgs boson, neutrinos, and the electroweak interaction. It also includes discussions on the nature of particles and their interactions, as well as the role of quantum mechanics in particle physics. The course emphasizes the importance of experimental data in understanding the standard model, and provides hands-on laboratory exercises to reinforce concepts learned in class.\n",
      "Response time: 4.29 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  The full spectra of particle physics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying all models...\n",
      "\n",
      "==================================================\n",
      "Question: The full spectra of particle physics\n",
      "==================================================\n",
      "\n",
      "OpenAI Fine-tuned Model Response:\n",
      "------------------------------\n",
      "In particle physics, the term \"full spectrum\" can refer to the complete array of particles and interactions that we currently understand and study. This includes the Standard Model of particle physics, which categorizes all known elementary particles and describes the forces that govern their behavior. The Standard Model includes:\n",
      "\n",
      "1. **Fermions**: These are the matter particles, which are divided into quarks and leptons:\n",
      "   - **Quarks**: Up, down, charm, strange, top, and bottom. Quarks combine to form protons and neutrons.\n",
      "   - **Leptons**: This group contains electrons, muons, tau particles, and their respective neutrinos (electron neutrino, muon neutrino, and tau neutrino).\n",
      "   \n",
      "2. **Bosons**: These are the force carriers or gauge bosons:\n",
      "   - **Photons**: Carry the electromagnetic force.\n",
      "   - **Gluons**: Carry the strong force.\n",
      "   - **W and Z bosons**: Carry the weak force, responsible for radioactive decay and neutrino interactions.\n",
      "   - **Higgs boson**: Related to the Higgs field, which gives mass to other particles.\n",
      "\n",
      "Beyond the Standard Model, there are several theoretical frameworks and candidates for additional particles, such as:\n",
      "\n",
      "- **Supersymmetry (SUSY)**: Proposes a symmetry between bosons and fermions and predicts superpartners for each Standard Model particle.\n",
      "- **Dark matter candidates**: Such as WIMPs (Weakly Interacting Massive Particles) and axions.\n",
      "- **Grand Unified Theories (GUTs)**: Which propose that the electromagnetic, weak, and strong forces converge at high energies.\n",
      "- **Extra dimensions**: As in theories like string theory or the Randall-Sundrum model.\n",
      "\n",
      "The spectrum also includes composite particles, like protons and mesons, which are not fundamental but are made of quarks held together by gluons. \n",
      "\n",
      "The complete \"full spectrum\" is an ongoing quest in particle physics, with new discoveries and insights from experiments at particle accelerators like CERN's Large Hadron Collider and neutrino observatories pushing the boundaries of our understanding.\n",
      "Response time: 5.03 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "As a research assistant, I'm unable to provide the information you're asking for because no context articles were provided.\n",
      "Response time: 2.08 seconds\n",
      "\n",
      "Local Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The full spectra of particle physics are the complete set of all possible experimental results that can be obtained from a particular experiment. These results include not only the data, but also the theoretical predictions and interpretations of the experiments. The full spectrum of particle physics includes both the direct detection of particles (such as neutrinos) and indirect detection through the study of their properties (such as the mass of the Higgs boson). The full spectrum of particle physics is essential for understanding the nature of matter and the universe at large.\n",
      "Response time: 3.56 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(\n",
    "        self, \n",
    "        fine_tuned_model_id, \n",
    "        local_model_path=\"llama_finetuned/final_model\",\n",
    "        base_model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        db_dir=\"cern_vectordb\"\n",
    "    ):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        # Initialize local model\n",
    "        print(\"Loading local model...\")\n",
    "        try:\n",
    "            self.local_tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "            \n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config\n",
    "            )\n",
    "            \n",
    "            self.local_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                local_model_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(\"Local model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load local model: {e}\")\n",
    "            self.local_model = None\n",
    "            self.local_tokenizer = None\n",
    "\n",
    "    def query_local_model(self, question, max_length=512, temperature=0.01):  # Changed from 0 to 0.01\n",
    "        \"\"\"Query the local fine-tuned model\"\"\"\n",
    "        if not self.local_model or not self.local_tokenizer:\n",
    "            return {\n",
    "                'response': \"Local model not available\",\n",
    "                'time': 0\n",
    "            }\n",
    "            \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Format the input with instruction\n",
    "            prompt = f\"[INST] {question} [/INST]\"\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.local_tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "            \n",
    "            # Generate response with small positive temperature\n",
    "            outputs = self.local_model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,  # Now using 0.01 instead of 0\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.local_tokenizer.pad_token_id,\n",
    "                num_return_sequences=1,\n",
    "                top_p=0.95,  # Added top_p for better deterministic output\n",
    "                repetition_penalty=1.1  # Added repetition penalty to avoid loops\n",
    "            )\n",
    "            \n",
    "            # Decode and clean response\n",
    "            response = self.local_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying local model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "    \n",
    "    def query_fine_tuned_model(self, question):\n",
    "        \"\"\"Query the OpenAI fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=1\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying OpenAI fine-tuned model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.rag_chain.invoke(question)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from all three approaches\"\"\"\n",
    "        print(\"\\nQuerying all models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        ft_result = self.query_fine_tuned_model(question)\n",
    "        rag_result = self.query_rag(question)\n",
    "        local_result = self.query_local_model(question)\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Question:\", question)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nOpenAI Fine-tuned Model Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\nRAG System Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\nLocal Fine-tuned Model Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(local_result['response'])\n",
    "        print(f\"Response time: {local_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'openai_fine_tuned': ft_result,\n",
    "            'rag': rag_result,\n",
    "            'local_fine_tuned': local_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Replace with your fine-tuned model ID\n",
    "    FINE_TUNED_MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\"\n",
    "    \n",
    "    try:\n",
    "        comparison = ModelComparison(FINE_TUNED_MODEL_ID)\n",
    "        \n",
    "        print(\"CERN Research Assistant Comparison\")\n",
    "        print(\"Compare: OpenAI Fine-tuned vs RAG vs Local Fine-tuned\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11f248-26f8-4529-92e2-f6aa8830c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "class RAGQuerySystem:\n",
    "    def __init__(self, embeddings_path: str, openai_model: str = \"gpt-4o\"):\n",
    "        # Load environment variables\n",
    "        dotenv.load_dotenv()\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = openai_model\n",
    "        \n",
    "        # Load embeddings and documents\n",
    "        data = np.load(embeddings_path, allow_pickle=True)\n",
    "        self.embeddings = data['encodings']\n",
    "        self.chunks = data['chunks']\n",
    "        self.metadata = data['metadata']\n",
    "        \n",
    "        # Initialize the same embedding model used for document encoding\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        # Get query embedding using the same model as document encoding\n",
    "        query_embedding = self.embedding_model.encode(question)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = np.dot(self.embeddings, query_embedding) / (\n",
    "            norm(self.embeddings, axis=1) * norm(query_embedding)\n",
    "        )\n",
    "        \n",
    "        # Get top 3 similar chunks\n",
    "        top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "        \n",
    "        # Prepare context from similar chunks\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Document: {self.metadata[idx]}]\\n{self.chunks[idx]}\"\n",
    "            for idx in top_indices\n",
    "        ])\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Using the following CERN research documents as context, answer the question. \n",
    "        If you cannot answer from the context, say so.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "        # Get response from OpenAI\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful physics research assistant specializing in CERN experiments and findings.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    # Initialize RAG system\n",
    "    rag = RAGQuerySystem('document_encodings.npz')\n",
    "    \n",
    "    # Ask about W boson mass\n",
    "    question = \"Based on the latest data inputs, the Standard Model (SM) constrains the mass of the W boson (mW) to be?\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nQuestion:\", question)\n",
    "        answer = rag.query(question)\n",
    "        print(\"\\nAnswer:\", answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b2ec7-6a24-4088-96c1-7ef8bec787b6",
   "metadata": {},
   "source": [
    "## Anything that you ever wnated to know about Cern.  Review some of the QA from below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f0656",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class RAGQuerySystem:\n",
    "    def __init__(self, embeddings_path: str, openai_model: str = \"gpt-4o\"):\n",
    "        # Load environment variables\n",
    "        dotenv.load_dotenv()\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = openai_model\n",
    "        \n",
    "        # Load embeddings and documents\n",
    "        data = np.load(embeddings_path, allow_pickle=True)\n",
    "        self.embeddings = data['encodings']\n",
    "        self.chunks = data['chunks']\n",
    "        self.metadata = data['metadata']\n",
    "        \n",
    "        # Initialize the same embedding model used for document encoding\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def find_similar_chunks(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[int, float]]:\n",
    "        # Calculate cosine similarity\n",
    "        similarities = np.dot(self.embeddings, query_embedding) / (\n",
    "            norm(self.embeddings, axis=1) * norm(query_embedding)\n",
    "        )\n",
    "        \n",
    "        # Get top k similar chunks\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(i, similarities[i]) for i in top_indices]\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        # Get query embedding using the same model as document encoding\n",
    "        query_embedding = self.embedding_model.encode(question)\n",
    "        \n",
    "        # Find relevant chunks\n",
    "        similar_chunks = self.find_similar_chunks(query_embedding)\n",
    "        \n",
    "        # Prepare context from similar chunks\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Document: {self.metadata[idx]}]\\n{self.chunks[idx]}\"\n",
    "            for idx, _ in similar_chunks\n",
    "        ])\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Using the following CERN research documents as context, answer the question. \n",
    "        If you cannot answer from the context, say so.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "        # Get response from OpenAI\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful physics research assistant specializing in CERN experiments and findings.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    rag = RAGQuerySystem('document_encodings.npz')\n",
    "    \n",
    "    print(\"CERN Research Query System (type 'quit' to exit)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nWhat would you like to know about CERN research? \").strip()\n",
    "        \n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            answer = rag.query(question)\n",
    "            print(\"\\nAnswer:\", answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cabde4-4ed3-448a-b8b3-1fe90721d28b",
   "metadata": {},
   "source": [
    "# RAG System Analysis: Improving Answer Quality with Document Grounding\n",
    "## A Comparison of Direct LLM vs. RAG-Enhanced Responses\n",
    "\n",
    "## Introduction\n",
    "This notebook analyzes the effectiveness of using Retrieval-Augmented Generation (RAG) for querying CERN research data compared to direct LLM queries. Our implementation processes CERN Courier documents to ground responses in authoritative sources.\n",
    "\n",
    "## System Architecture\n",
    "### Document Processing Pipeline\n",
    "```python\n",
    "# Key statistics from our implementation\n",
    "total_pdfs_processed = 96\n",
    "total_chunks_processed = 22103\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "chunk_size = 1000\n",
    "```\n",
    "\n",
    "Our system processes PDFs through several stages:\n",
    "1. Document Collection: Downloads CERN Courier PDFs\n",
    "2. Text Extraction: Converts PDFs to processable text\n",
    "3. Chunking: Splits text into manageable segments\n",
    "4. Embedding Generation: Creates vector representations\n",
    "5. Indexing: Organizes embeddings for efficient retrieval\n",
    "\n",
    "## Comparative Analysis\n",
    "\n",
    "### Case Study: W Boson Mass Query\n",
    "\n",
    "#### Direct OpenAI Query\n",
    "```python\n",
    "question = \"Based on the latest data inputs, what does the Standard Model (SM) constrain the mass of the W boson (mW) to be?\"\n",
    "\n",
    "# Direct OpenAI response (truncated):\n",
    "\"\"\"\n",
    "As of my last update in 2023, within the framework of the Standard Model of particle physics, \n",
    "precise calculations constrain the mass of the W boson (mW) based on various experimental \n",
    "inputs and theoretical considerations...\n",
    "\n",
    "Before 2022, the Particle Data Group (PDG) reported a world average...\n",
    "CDF collaboration announced a new measurement...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### RAG System Query\n",
    "```python\n",
    "# RAG system response:\n",
    "\"\"\"\n",
    "80357 ± 6 MeV\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Key Improvements Analysis\n",
    "\n",
    "#### 1. Answer Precision\n",
    "- **Direct LLM**: \n",
    "  - Provides general context\n",
    "  - Includes historical data\n",
    "  - No specific current value\n",
    "  \n",
    "- **RAG System**:\n",
    "  - Exact measurement with uncertainty\n",
    "  - Current CERN-sourced value\n",
    "  - No extraneous information\n",
    "\n",
    "#### 2. Response Time\n",
    "```python\n",
    "# Average response times\n",
    "direct_llm_time = \"3.2 seconds\"\n",
    "rag_system_time = \"5.1 seconds\"  # Includes retrieval overhead\n",
    "```\n",
    "\n",
    "#### 3. Source Reliability\n",
    "```python\n",
    "# RAG system source tracking\n",
    "source_metadata = {\n",
    "    \"document_type\": \"CERN Courier\",\n",
    "    \"total_sources\": 96,\n",
    "    \"date_range\": \"Last 11 years\",\n",
    "    \"verification\": \"Direct CERN publications\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Technical Implementation Details\n",
    "\n",
    "### Embedding Generation\n",
    "```python\n",
    "class DocumentEncoder:\n",
    "    def __init__(self, batch_size=256):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def encode_documents(self, pdf_directory):\n",
    "        # Process and encode documents\n",
    "        # Returns: embeddings array of shape (n_chunks, 384)\n",
    "```\n",
    "\n",
    "### Query Processing\n",
    "```python\n",
    "class RAGQuerySystem:\n",
    "    def query(self, question: str) -> str:\n",
    "        # 1. Encode question\n",
    "        # 2. Find similar chunks\n",
    "        # 3. Prepare context\n",
    "        # 4. Generate answer\n",
    "        return answer\n",
    "```\n",
    "\n",
    "## Results Summary\n",
    "\n",
    "### Performance Metrics\n",
    "1. **Accuracy**:\n",
    "   - RAG responses grounded in CERN documentation\n",
    "   - Specific numerical values vs. general ranges\n",
    "   - Reduced hallucination risk\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Fast response times (~5s)\n",
    "   - Focused, relevant answers\n",
    "   - Direct access to technical details\n",
    "\n",
    "3. **Source Attribution**:\n",
    "   - All answers traceable to CERN documents\n",
    "   - Metadata preserved for verification\n",
    "   - Recent publications ensure currency\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "The RAG system demonstrates significant advantages over direct LLM queries:\n",
    "1. Higher precision in technical answers\n",
    "2. Direct grounding in authoritative sources\n",
    "3. Clear provenance for all information\n",
    "4. Reduced tendency for hallucination or generalization\n",
    "\n",
    "### Future Improvements\n",
    "1. Implement caching for faster responses\n",
    "2. Add source citation in responses\n",
    "3. Expand document collection\n",
    "4. Optimize chunk size for better context\n",
    "\n",
    "## Usage Example\n",
    "```python\n",
    "# Initialize RAG system\n",
    "rag = RAGQuerySystem('document_encodings.npz')\n",
    "\n",
    "# Example query\n",
    "question = \"How many people work for CERN?\"\n",
    "answer = rag.query(question)\n",
    "print(f\"Answer: {answer}\")  # Output: \"CERN employs around 2600 staff members.\"\n",
    "```\n",
    "\n",
    "This implementation shows how combining retrieval with generation can significantly improve the quality and reliability of answers in specialized technical domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59127029-07f4-41d3-a70f-872c154416ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
