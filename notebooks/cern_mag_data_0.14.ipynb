{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "## CERN is celebrating its 70th anniversary \n",
    "During a crucial period for high-energy physics, coinciding with the initiation of the third update to the European strategy for particle physics. In this special edition of CERN Courier magazine, early-career researchers share their visions for the future of the field while reflecting on CERN's scientific and societal contributions. The magazine features expert insights into the achievements of the Large Hadron Collider (LHC) and explores the advancements of the hybrid pixel detector technology, emphasizing its applications beyond particle physics.\n",
    "\n",
    "## The CERN Courier website \n",
    "is a rich repository of articles covering a wide array of topics in particle physics, high-energy physics, and associated technological advancements. It provides in-depth reporting on the latest experimental results from CERN and other international laboratories, offering insights into ongoing research and discoveries in the field.\n",
    "\n",
    "## Last 11 years of the CERN Courier Magazine in PDF\n",
    "In this dataset I am downloading the Last 11 years of the CERN Courier Magazine.  I will then take this database and then encode it to be used as a Context Window to ask Questions to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "# 1: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b0f1c-a3f7-47cc-9c3e-546e7d19754f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin, unquote\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "class CERNPDFCrawler:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://home.cern/resources\"\n",
    "        self.session = requests.Session()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.download_folder = \"cern_pdfs\"\n",
    "        self.processed_article_urls = set()\n",
    "        self.downloaded_files = set()\n",
    "        \n",
    "        if not os.path.exists(self.download_folder):\n",
    "            os.makedirs(self.download_folder)\n",
    "        self.load_existing_files()\n",
    "\n",
    "    def load_existing_files(self):\n",
    "        for filename in os.listdir(self.download_folder):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                self.downloaded_files.add(filename)\n",
    "        print(f\"Found {len(self.downloaded_files)} existing PDF files\")\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "                    return None\n",
    "                time.sleep(2 ** attempt)\n",
    "        return None\n",
    "\n",
    "    def extract_pdf_urls_from_text(self, text):\n",
    "        \"\"\"Extract PDF URLs from text content including 'File path:' patterns\"\"\"\n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Look for \"File path:\" pattern\n",
    "        file_path_matches = re.finditer(r'File path:\\s*(https?://[^\\s<>\"]+\\.pdf)', text, re.IGNORECASE)\n",
    "        for match in file_path_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        # Look for direct PDF links\n",
    "        pdf_link_matches = re.finditer(r'href=\"(https?://[^\\s<>\"]+\\.pdf)\"', text, re.IGNORECASE)\n",
    "        for match in pdf_link_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        return pdf_urls\n",
    "\n",
    "    def find_courier_links(self, page_url):\n",
    "        content = self.get_page_content(page_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        courier_links = []\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/resources/courier/' in href or '/record/' in href:\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                if full_url not in self.processed_article_urls:\n",
    "                    courier_links.append(full_url)\n",
    "                    self.processed_article_urls.add(full_url)\n",
    "        \n",
    "        return courier_links\n",
    "\n",
    "    def find_pdf_links(self, article_url):\n",
    "        \"\"\"Find all PDF download links on an article page\"\"\"\n",
    "        content = self.get_page_content(article_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Extract URLs from text content\n",
    "        pdf_urls.update(self.extract_pdf_urls_from_text(content))\n",
    "        \n",
    "        # Parse with BeautifulSoup for structured extraction\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Look for links containing PDF\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.lower().endswith('.pdf'):\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                pdf_urls.add(full_url)\n",
    "        \n",
    "        return list(pdf_urls)\n",
    "\n",
    "    def sanitize_filename(self, url):\n",
    "        \"\"\"Create a safe filename from URL\"\"\"\n",
    "        filename = unquote(url.split('/')[-1])\n",
    "        # Remove or replace unsafe characters\n",
    "        filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "        return filename\n",
    "\n",
    "    def download_pdf(self, pdf_url, filename):\n",
    "        if filename in self.downloaded_files:\n",
    "            print(f\"Skipping {filename} - already downloaded\")\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            response = self.session.get(pdf_url, headers=self.headers, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            file_path = os.path.join(self.download_folder, filename)\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(file_path, 'wb') as file, tqdm(\n",
    "                desc=filename,\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = file.write(data)\n",
    "                    pbar.update(size)\n",
    "            \n",
    "            self.downloaded_files.add(filename)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def crawl_and_download(self, start_page=0, end_page=7):\n",
    "        print(f\"Starting CERN PDF crawler (pages {start_page} to {end_page})\")\n",
    "        \n",
    "        found_pdfs = 0\n",
    "        downloaded_pdfs = 0\n",
    "        skipped_pdfs = 0\n",
    "        failed_downloads = []\n",
    "        \n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = f\"{self.base_url}?type=52&page={page_num}\"\n",
    "            print(f\"\\nProcessing page {page_num}...\")\n",
    "            \n",
    "            courier_links = self.find_courier_links(page_url)\n",
    "            print(f\"Found {len(courier_links)} new article links on page {page_num}\")\n",
    "            \n",
    "            for article_url in courier_links:\n",
    "                pdf_urls = self.find_pdf_links(article_url)\n",
    "                \n",
    "                for pdf_url in pdf_urls:\n",
    "                    found_pdfs += 1\n",
    "                    filename = self.sanitize_filename(pdf_url)\n",
    "                    \n",
    "                    print(f\"\\nFound PDF: {filename}\")\n",
    "                    print(f\"URL: {pdf_url}\")\n",
    "                    \n",
    "                    if filename in self.downloaded_files:\n",
    "                        print(f\"Skipping - already downloaded\")\n",
    "                        skipped_pdfs += 1\n",
    "                        continue\n",
    "                        \n",
    "                    if self.download_pdf(pdf_url, filename):\n",
    "                        downloaded_pdfs += 1\n",
    "                    else:\n",
    "                        failed_downloads.append(filename)\n",
    "                \n",
    "                time.sleep(1)\n",
    "        \n",
    "        print(\"\\nDownload Summary:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Total PDFs found: {found_pdfs}\")\n",
    "        print(f\"Successfully downloaded: {downloaded_pdfs}\")\n",
    "        print(f\"Skipped (already downloaded): {skipped_pdfs}\")\n",
    "        print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "        if failed_downloads:\n",
    "            print(\"\\nFailed downloads:\")\n",
    "            for fail in failed_downloads:\n",
    "                print(f\"- {fail}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = CERNPDFCrawler()\n",
    "    crawler.crawl_and_download(0, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78865c",
   "metadata": {},
   "source": [
    "# 2: Encode the CERN PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93cde4-96fd-47f5-b862-93900a21ae9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.db_dir = Path(db_dir)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                \n",
    "                # Extract text from each page\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        \"\"\"Process all PDFs in the directory and return chunks with metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        # Process each PDF file\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Split text into chunks\n",
    "                chunks = self.text_splitter.create_documents(\n",
    "                    texts=[text],\n",
    "                    metadatas=[{\"source\": pdf_path.name}]\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def create_vector_db(self):\n",
    "        \"\"\"Create and populate the vector database\"\"\"\n",
    "        # Get text chunks\n",
    "        chunks = self.process_pdfs()\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"No text chunks were created. Check the PDF processing.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nCreating vector database with {len(chunks)} chunks...\")\n",
    "        \n",
    "        # Create and persist the vector store\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=str(self.db_dir)\n",
    "        )\n",
    "        \n",
    "        # Persist the database\n",
    "        vectordb.persist()\n",
    "        print(f\"Vector database created and saved to {self.db_dir}\")\n",
    "        \n",
    "        return vectordb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        processor = PDFProcessor()\n",
    "        vectordb = processor.create_vector_db()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd11e0-aa06-440b-af5d-d3835d32be28",
   "metadata": {},
   "source": [
    "# 3: Chat with Cern Magazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6facc867-bba9-44a6-9c78-2f199b187dfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class CERNResearchAssistant:\n",
    "    def __init__(self, db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        # Initialize the vector store\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        # Initialize the language model\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Create the retriever\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Setup the prompt template\n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Setup the RAG chain\n",
    "        self.chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Ask a question about CERN research\"\"\"\n",
    "        try:\n",
    "            response = self.chain.invoke(question)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {e}\"\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize the assistant\n",
    "        assistant = CERNResearchAssistant()\n",
    "        \n",
    "        print(\"CERN Research Assistant Ready!\")\n",
    "        print(\"Ask questions about CERN research (type 'quit' to exit)\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            response = assistant.query(question)\n",
    "            print(\"\\nAssistant:\", response)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1f129-fdef-4725-a154-1bffd26c5b2b",
   "metadata": {},
   "source": [
    "# 4: Fine Tuning with OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a4e76-cbb5-43da-92c4-e78531886a02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class FineTunePrep:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", output_dir=\"finetune_data\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Constants for token limits\n",
    "        self.MAX_TOKENS_PER_EXAMPLE = 3000  # Leave room for system and user messages\n",
    "        self.MIN_TOKENS_PER_EXAMPLE = 500   # Ensure meaningful content\n",
    "        \n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count tokens in a text string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def split_into_chunks(self, text):\n",
    "        \"\"\"Split text into chunks of appropriate token length\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Split into sentences (roughly)\n",
    "        sentences = [s.strip() + \".\" for s in text.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If single sentence is too long, split it into smaller parts\n",
    "            if sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                words = sentence.split()\n",
    "                temp_chunk = \"\"\n",
    "                temp_tokens = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    word_tokens = self.count_tokens(word + \" \")\n",
    "                    if temp_tokens + word_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                        if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = word + \" \"\n",
    "                        temp_tokens = word_tokens\n",
    "                    else:\n",
    "                        temp_chunk += word + \" \"\n",
    "                        temp_tokens += word_tokens\n",
    "                \n",
    "                if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(temp_chunk.strip())\n",
    "                continue\n",
    "            \n",
    "            # If adding this sentence would exceed limit, save current chunk and start new one\n",
    "            if current_tokens + sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "                current_tokens = sentence_tokens\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk if it's long enough\n",
    "        if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def create_training_examples(self, chunks):\n",
    "        \"\"\"Create training examples from text chunks\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Create the messages for this chunk\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What are the key findings or developments described in this CERN research?\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"Based on the CERN publications: {chunk}\"\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Verify total tokens\n",
    "            total_tokens = sum(self.count_tokens(msg[\"content\"]) for msg in messages)\n",
    "            if total_tokens <= 4096:  # GPT-4's context window\n",
    "                examples.append({\"messages\": messages})\n",
    "            \n",
    "        return examples\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Process PDFs and prepare training data\"\"\"\n",
    "        all_examples = []\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # First split text into appropriate chunks\n",
    "                chunks = self.split_into_chunks(text)\n",
    "                print(f\"\\nCreated {len(chunks)} chunks from {pdf_path.name}\")\n",
    "                \n",
    "                # Create examples from chunks\n",
    "                examples = self.create_training_examples(chunks)\n",
    "                all_examples.extend(examples)\n",
    "        \n",
    "        # Save training data\n",
    "        training_file_path = self.output_dir / \"training_data.jsonl\"\n",
    "        with open(training_file_path, 'w', encoding='utf-8') as f:\n",
    "            for example in all_examples:\n",
    "                f.write(json.dumps(example) + '\\n')\n",
    "        \n",
    "        print(f\"\\nCreated {len(all_examples)} valid training examples\")\n",
    "        print(f\"Training data saved to {training_file_path}\")\n",
    "        return training_file_path\n",
    "\n",
    "    def submit_fine_tuning_job(self, training_file_path):\n",
    "        \"\"\"Submit fine-tuning job to OpenAI\"\"\"\n",
    "        try:\n",
    "            # Upload the training file\n",
    "            with open(training_file_path, 'rb') as f:\n",
    "                training_file = self.client.files.create(\n",
    "                    file=f,\n",
    "                    purpose='fine-tune'\n",
    "                )\n",
    "            print(f\"Training file uploaded with ID: {training_file.id}\")\n",
    "            \n",
    "            # Create fine-tuning job\n",
    "            job = self.client.fine_tuning.jobs.create(\n",
    "                training_file=training_file.id,\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 2,\n",
    "                    \"learning_rate_multiplier\": 0.1\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Fine-tuning job created with ID: {job.id}\")\n",
    "            return job.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting fine-tuning job: {e}\")\n",
    "            return None\n",
    "\n",
    "    def monitor_fine_tuning_job(self, job_id):\n",
    "        \"\"\"Monitor the status of a fine-tuning job\"\"\"\n",
    "        print(\"\\nMonitoring fine-tuning job...\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                job = self.client.fine_tuning.jobs.retrieve(job_id)\n",
    "                print(f\"\\nStatus: {job.status}\")\n",
    "                \n",
    "                # Safely print additional info if available\n",
    "                if hasattr(job, 'trained_tokens') and job.trained_tokens is not None:\n",
    "                    print(f\"Trained tokens: {job.trained_tokens:,}\")\n",
    "                if hasattr(job, 'training_accuracy') and job.training_accuracy is not None:\n",
    "                    print(f\"Training accuracy: {job.training_accuracy:.4f}\")\n",
    "                \n",
    "                if job.status == 'succeeded':\n",
    "                    print(f\"\\nFine-tuning completed successfully!\")\n",
    "                    print(f\"Fine-tuned model ID: {job.fine_tuned_model}\")\n",
    "                    return job\n",
    "                elif job.status == 'failed':\n",
    "                    print(f\"\\nFine-tuning failed: {getattr(job, 'error', 'Unknown error')}\")\n",
    "                    return job\n",
    "                elif job.status == 'cancelled':\n",
    "                    print(\"\\nFine-tuning job was cancelled\")\n",
    "                    return job\n",
    "                \n",
    "                time.sleep(60)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking job status: {e}\")\n",
    "                time.sleep(60)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        prep = FineTunePrep()\n",
    "        \n",
    "        print(\"Step 1: Preparing training data...\")\n",
    "        training_file_path = prep.prepare_training_data()\n",
    "        \n",
    "        print(\"\\nStep 2: Submitting fine-tuning job...\")\n",
    "        job_id = prep.submit_fine_tuning_job(training_file_path)\n",
    "        \n",
    "        if job_id:\n",
    "            final_job = prep.monitor_fine_tuning_job(job_id)\n",
    "            \n",
    "            if getattr(final_job, 'status', None) == 'succeeded':\n",
    "                print(\"\\nFine-tuning process completed successfully!\")\n",
    "                print(f\"You can now use your fine-tuned model with ID: {final_job.fine_tuned_model}\")\n",
    "                \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3602ca-6c2e-4c37-bc93-4be91e88d430",
   "metadata": {},
   "source": [
    "# 5: RAG vs Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37745d-117b-4912-a95e-1b9370c8fc2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(self, fine_tuned_model_id, db_dir=\"cern_vectordb\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def query_fine_tuned_model(self, question):\n",
    "        \"\"\"Query the fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying fine-tuned model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.rag_chain.invoke(question)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from both approaches\"\"\"\n",
    "        print(\"\\nQuerying both models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        ft_result = self.query_fine_tuned_model(question)\n",
    "        rag_result = self.query_rag(question)\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Question:\", question)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nFine-tuned Model Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\nRAG System Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'fine_tuned': ft_result,\n",
    "            'rag': rag_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Replace with your fine-tuned model ID\n",
    "    FINE_TUNED_MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\"\n",
    "    \n",
    "    try:\n",
    "        comparison = ModelComparison(FINE_TUNED_MODEL_ID)\n",
    "        \n",
    "        print(\"CERN Research Assistant Comparison\")\n",
    "        print(\"Compare Fine-tuned model vs RAG approach\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7310b5d-bf94-4ab8-946b-3cbee47f82f8",
   "metadata": {},
   "source": [
    "# 6: Fine Tunging on nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73efaa-d192-4c93-9abe-5bc28be53764",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "def setup_optimized_lora_config():\n",
    "    # Configure quantization\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA focusing on key layers\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\"\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        layers_to_transform=[0, 1, 15, 16, 30, 31]  # Critical layers\n",
    "    )\n",
    "    \n",
    "    return quant_config, lora_config\n",
    "\n",
    "def print_layer_config():\n",
    "    _, lora_config = setup_optimized_lora_config()\n",
    "    print(\"Targeted layers for fine-tuning:\")\n",
    "    for layer in lora_config.layers_to_transform:\n",
    "        print(f\"\\nLayer {layer}:\")\n",
    "        for module in lora_config.target_modules:\n",
    "            print(f\"- model.layers.{layer}.self_attn.{module}\" if \"proj\" in module \n",
    "                  else f\"- model.layers.{layer}.mlp.{module}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_layer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8163c-ed3a-4657-9df0-0e962b26fb74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig, TaskType\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def setup_lora_config():\n",
    "    return LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False\n",
    "    )\n",
    "\n",
    "class OptimizedTrainer:\n",
    "    def __init__(self, model_name=\"nvidia/Llama3-ChatQA-1.5-8B\", pdf_dir=\"cern_pdfs\", output_dir=\"finetuned_model\"):\n",
    "        self.model_name = model_name\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.setup_model_and_tokenizer()\n",
    "        \n",
    "    def setup_model_and_tokenizer(self):\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            device_map={'': 0}\n",
    "        )\n",
    "        \n",
    "        self.model.config.use_cache = False\n",
    "        self.model = get_peft_model(self.model, setup_lora_config())\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "    def process_pdf(self, pdf_path):\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                return \" \".join(page.extract_text() for page in reader.pages).strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        training_data = []\n",
    "        for pdf_path in tqdm(list(self.pdf_dir.glob(\"*.pdf\"))):\n",
    "            text = self.process_pdf(pdf_path)\n",
    "            if text:\n",
    "                chunks = nltk.sent_tokenize(text)\n",
    "                for i in range(0, len(chunks), 3):\n",
    "                    chunk = \" \".join(chunks[i:i+3])\n",
    "                    training_data.append({\n",
    "                        \"text\": f\"[INST] Analyze this scientific text: {chunk} [/INST]\"\n",
    "                    })\n",
    "        \n",
    "        dataset = Dataset.from_list(training_data)\n",
    "        return dataset.map(\n",
    "            lambda x: self.tokenizer(\n",
    "                x[\"text\"],\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\"\n",
    "            ),\n",
    "            remove_columns=[\"text\"]\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.output_dir),\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=3,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_steps=100,\n",
    "            max_grad_norm=0.3,\n",
    "            logging_dir=\"logs\",\n",
    "            save_total_limit=3,\n",
    "            lr_scheduler_type=\"cosine\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.prepare_dataset(),\n",
    "            data_collator=DataCollatorForLanguageModeling(\n",
    "                tokenizer=self.tokenizer, \n",
    "                mlm=False\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.model.save_pretrained(self.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    OptimizedTrainer().train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cce55d-cebc-4ef2-9741-1eae8a3d3ac2",
   "metadata": {},
   "source": [
    "# 7: Chat with the Fine-Tuned nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6372d-e50d-491b-9876-5bcf6a3e60d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelChat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        finetuned_path=\"llama_finetuned/final_model\",\n",
    "        max_sequence_length=2048\n",
    "    ):\n",
    "        try:\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                base_model_name,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            # Set padding parameters\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            self.max_sequence_length = max_sequence_length\n",
    "            \n",
    "            print(\"Loading base model...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            print(\"Loading fine-tuned adapters...\")\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                finetuned_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            self.model.eval()\n",
    "            print(\"Model loaded successfully!\")\n",
    "            print(f\"Model is on device: {self.model.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error initializing model: {str(e)}\")\n",
    "    \n",
    "    def generate_response(self, instruction, max_new_tokens=256, temperature=0.7):\n",
    "        try:\n",
    "            # Format the input\n",
    "            prompt = f\"[INST] {instruction} [/INST]\"\n",
    "            print(f\"\\nDebug - Prompt: {prompt}\")\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_sequence_length\n",
    "            )\n",
    "            print(f\"Debug - Input shape: {inputs.input_ids.shape}\")\n",
    "            \n",
    "            # Move to GPU\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate with minimal parameters first\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            print(f\"Debug - Output shape: {outputs.shape}\")\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"Debug - Raw response: {response}\")\n",
    "            \n",
    "            # Clean response\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "            print(f\"Debug - Cleaned response: {response}\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Debug - Error in generate_response: {str(e)}\")\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def chat(self):\n",
    "        print(\"\\nStarting chat session with the fine-tuned model\")\n",
    "        print(\"Type 'quit' to exit, 'clear' to clear the conversation\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                    print(\"\\nGoodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\nGenerating response...\")\n",
    "                response = self.generate_response(user_input)\n",
    "                \n",
    "                if response:\n",
    "                    print(f\"\\nModel: {response}\")\n",
    "                else:\n",
    "                    print(\"\\nModel: No response generated. Please try again.\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nInterrupted by user. Type 'quit' to exit or continue chatting.\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during chat: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"This script requires a CUDA-capable GPU\")\n",
    "        \n",
    "        print(\"\\nInitializing chat model...\")\n",
    "        chat_model = ModelChat()\n",
    "        chat_model.chat()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b410781-1b8c-4997-9697-18a7c1241e78",
   "metadata": {},
   "source": [
    "# 8: Now compaire OpenAI RAG vs OpenAI Fine-Tune vs nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec084a0c-6090-4200-bba7-9b6f9e818da1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fine_tuned_model_id=\"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\",\n",
    "        base_model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        finetuned_path=\"llama_finetuned/final_model\",\n",
    "        db_dir=\"cern_vectordb\"\n",
    "    ):\n",
    "        load_dotenv()\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        print(\"Initializing RAG system...\")\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Setup RAG prompt template\n",
    "        template = \"\"\"You are a CERN research assistant. Use the following articles to answer the question.\n",
    "        If you cannot answer based on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer with specific references to the articles:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Setup RAG chain\n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # Initialize LLaMA model\n",
    "        print(\"Loading LLaMA model...\")\n",
    "        try:\n",
    "            # Initialize tokenizer\n",
    "            self.llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                base_model_name,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            if self.llama_tokenizer.pad_token is None:\n",
    "                self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token\n",
    "            \n",
    "            # Setup quantization\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            # Load base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Load LoRA adapters\n",
    "            self.llama_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                finetuned_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            self.llama_model.eval()\n",
    "            print(\"LLaMA model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load LLaMA model: {e}\")\n",
    "            self.llama_model = None\n",
    "            self.llama_tokenizer = None\n",
    "\n",
    "    def query_llama(self, question, max_length=512):\n",
    "        \"\"\"Query the LoRA-fine-tuned LLaMA model\"\"\"\n",
    "        if not self.llama_model or not self.llama_tokenizer:\n",
    "            return {\n",
    "                'response': \"LLaMA model not available\",\n",
    "                'time': 0\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Prepare input\n",
    "            prompt = f\"[INST] {question} [/INST]\"\n",
    "            inputs = self.llama_tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=True\n",
    "            ).to(self.llama_model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.llama_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95,\n",
    "                    repetition_penalty=1.1,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.llama_tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.llama_tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying LLaMA model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_fine_tuned_gpt(self, question):\n",
    "        \"\"\"Query the OpenAI fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics research.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying fine-tuned GPT: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.rag_chain.invoke(question)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from all three models\"\"\"\n",
    "        print(\"\\nProcessing your question across all models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        rag_result = self.query_rag(question)\n",
    "        ft_result = self.query_fine_tuned_gpt(question)\n",
    "        llama_result = self.query_llama(question)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\n1. RAG System (GPT-4 + CERN Articles)\")\n",
    "        print(\"-\"*50)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\n2. Fine-tuned GPT-4\")\n",
    "        print(\"-\"*50)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\n3. Fine-tuned LLaMA (with LoRA)\")\n",
    "        print(\"-\"*50)\n",
    "        print(llama_result['response'])\n",
    "        print(f\"Response time: {llama_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'rag': rag_result,\n",
    "            'fine_tuned_gpt': ft_result,\n",
    "            'llama': llama_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Initializing Model Comparison System...\")\n",
    "        comparison = ModelComparison()\n",
    "        \n",
    "        print(\"\\nCERN Research Assistant - Model Comparison\")\n",
    "        print(\"Compare: RAG vs Fine-tuned GPT-4 vs Fine-tuned LLaMA\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \").strip()\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            if not question:\n",
    "                continue\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88e812-7685-4307-b646-5eda86f2ffa2",
   "metadata": {},
   "source": [
    "# 9: Build a LLM from scratch on the CERN PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6a4b4-2927-40a3-8307-0d05e7af3acf",
   "metadata": {},
   "source": [
    "## A: Config and Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7adc8-21da-48ad-aff8-d072de0e3fc4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "import wandb\n",
    "import gc\n",
    "import os\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 32000,\n",
    "        max_sequence_length: int = 512,\n",
    "        d_model: int = 768,\n",
    "        n_heads: int = 12,\n",
    "        n_layers: int = 6,\n",
    "        d_ff: int = 3072,\n",
    "        dropout: float = 0.1,\n",
    "        pad_token_id: int = 0\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        \n",
    "        self.d_k = self.d_model // self.n_heads\n",
    "        self.w_q = nn.Linear(config.d_model, config.d_model)\n",
    "        self.w_k = nn.Linear(config.d_model, config.d_model)\n",
    "        self.w_v = nn.Linear(config.d_model, config.d_model)\n",
    "        self.w_o = nn.Linear(config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def attention(self, q, k, v, mask=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        return torch.matmul(attn, v), attn\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        q = self.w_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        x, attn = self.attention(q, k, v, mask)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6cfb4-4248-4425-9b7a-8a48782cc2c3",
   "metadata": {},
   "source": [
    "## B: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89caddb9-38a3-47a7-9726-9fbde08d4b77",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.norm1 = nn.LayerNorm(config.d_model)\n",
    "        self.norm2 = nn.LayerNorm(config.d_model)\n",
    "        self.feed_forward = PositionwiseFeedForward(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(config.max_sequence_length, config.d_model)\n",
    "        position = torch.arange(0, config.max_sequence_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, config.d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / config.d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CustomLLM(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.positional_encoding = PositionalEncoding(config)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_layer_norm = nn.LayerNorm(config.d_model)\n",
    "        self.output_projection = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, attention_mask)\n",
    "            \n",
    "        x = self.final_layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349f140-ebaa-4672-aa46-6ee6547a9fa9",
   "metadata": {},
   "source": [
    "## C: Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f0665-2ed3-4573-8e70-68eb5e588030",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CERNDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdf_dir: str,\n",
    "        tokenizer_name: str = \"gpt2\",\n",
    "        max_length: int = 512,\n",
    "        min_text_length: int = 100\n",
    "    ):\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        if not self.pdf_dir.exists():\n",
    "            raise ValueError(f\"PDF directory {pdf_dir} does not exist\")\n",
    "            \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "                logging.info(\"Set pad_token to eos_token\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load tokenizer: {str(e)}\")\n",
    "            \n",
    "        self.max_length = max_length\n",
    "        self.min_text_length = min_text_length\n",
    "        \n",
    "        # Process PDFs\n",
    "        self.texts = []\n",
    "        self._process_pdfs()\n",
    "        \n",
    "        if not self.texts:\n",
    "            raise ValueError(f\"No valid texts found in {pdf_dir}\")\n",
    "            \n",
    "        # Process texts in batches\n",
    "        self._tokenize_texts()\n",
    "        \n",
    "    def _tokenize_texts(self):\n",
    "        self.input_ids = []\n",
    "        self.attention_masks = []\n",
    "        \n",
    "        batch_size = 32\n",
    "        for i in tqdm(range(0, len(self.texts), batch_size), desc=\"Tokenizing texts\"):\n",
    "            batch_texts = self.texts[i:i + batch_size]\n",
    "            try:\n",
    "                encodings = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                self.input_ids.extend(encodings['input_ids'])\n",
    "                self.attention_masks.extend(encodings['attention_mask'])\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error tokenizing batch {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not self.input_ids:\n",
    "            raise ValueError(\"No texts were successfully tokenized\")\n",
    "            \n",
    "        try:\n",
    "            self.input_ids = torch.stack([t for t in self.input_ids if isinstance(t, torch.Tensor)])\n",
    "            self.attention_masks = torch.stack([t for t in self.attention_masks if isinstance(t, torch.Tensor)])\n",
    "            logging.info(f\"Successfully processed {len(self.input_ids)} sequences\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to stack tensors: {str(e)}\")\n",
    "        \n",
    "    def _process_pdfs(self):\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            raise ValueError(f\"No PDF files found in {self.pdf_dir}\")\n",
    "            \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            try:\n",
    "                with open(pdf_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\"\n",
    "                    for page in reader.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "                    words = text.split()\n",
    "                    chunk_size = self.max_length * 4\n",
    "                    \n",
    "                    for i in range(0, len(words), chunk_size):\n",
    "                        chunk = \" \".join(words[i:i + chunk_size])\n",
    "                        if len(chunk.strip()) >= self.min_text_length:\n",
    "                            self.texts.append(chunk)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing {pdf_path}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': torch.roll(self.input_ids[idx], -1)\n",
    "        }\n",
    "        item['labels'][-1] = self.tokenizer.pad_token_id\n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39782e-5b93-49ae-9e30-cd934debcaf6",
   "metadata": {},
   "source": [
    "## D: Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141684c-a9f0-4b73-a2df-868936db78f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FP 16 Change\n",
    "from torch.cuda import amp\n",
    "from contextlib import nullcontext\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: CustomLLM,\n",
    "        train_dataset: CERNDataset,\n",
    "        val_dataset: Optional[CERNDataset] = None,\n",
    "        batch_size: int = 8,\n",
    "        learning_rate: float = 5e-4,\n",
    "        min_lr: float = 1e-5,\n",
    "        warmup_steps: int = 100,\n",
    "        num_epochs: int = 11,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        wandb_project: Optional[str] = None,\n",
    "        checkpoint_dir: str = \"checkpoints\",\n",
    "        scheduler_type: str = 'cosine_warmup',\n",
    "        use_amp: bool = True,\n",
    "        scaler: Optional[amp.GradScaler] = None\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "        self.scheduler_type = scheduler_type\n",
    "        self.use_amp = use_amp\n",
    "        self.scaler = scaler if scaler else amp.GradScaler()\n",
    "        \n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_loader = None\n",
    "        if val_dataset:\n",
    "            self.val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.95),\n",
    "            weight_decay=0.1\n",
    "        )\n",
    "        \n",
    "        total_steps = len(self.train_loader) * num_epochs\n",
    "        \n",
    "        if scheduler_type == 'cosine_warmup':\n",
    "            self.scheduler = get_cosine_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=warmup_steps,\n",
    "                num_training_steps=total_steps,\n",
    "                num_cycles=0.5\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                self.optimizer,\n",
    "                T_max=total_steps,\n",
    "                eta_min=min_lr\n",
    "            )\n",
    "        \n",
    "        if wandb_project:\n",
    "            wandb.init(project=wandb_project)\n",
    "            wandb.config.update({\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"min_lr\": min_lr,\n",
    "                \"warmup_steps\": warmup_steps,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"num_epochs\": num_epochs,\n",
    "                \"scheduler_type\": scheduler_type,\n",
    "                \"model_config\": model.config.__dict__,\n",
    "                \"use_amp\": use_amp\n",
    "            })\n",
    "\n",
    "    def save_checkpoint(self, epoch: int, loss: float, is_best: bool = False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'scaler_state_dict': self.scaler.state_dict() if self.use_amp else None,\n",
    "            'loss': loss\n",
    "        }\n",
    "        \n",
    "        filename = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "        torch.save(checkpoint, filename)\n",
    "        \n",
    "        if is_best:\n",
    "            best_filename = self.checkpoint_dir / 'best_model.pt'\n",
    "            torch.save(checkpoint, best_filename)\n",
    "\n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            train_pbar = tqdm(self.train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_pbar):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                if self.use_amp:\n",
    "                    with amp.autocast():\n",
    "                        outputs = self.model(input_ids, attention_mask)\n",
    "                        loss = F.cross_entropy(\n",
    "                            outputs.view(-1, outputs.size(-1)),\n",
    "                            labels.view(-1),\n",
    "                            ignore_index=self.model.config.pad_token_id\n",
    "                        )\n",
    "                    \n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    self.scaler.step(self.optimizer)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    outputs = self.model(input_ids, attention_mask)\n",
    "                    loss = F.cross_entropy(\n",
    "                        outputs.view(-1, outputs.size(-1)),\n",
    "                        labels.view(-1),\n",
    "                        ignore_index=self.model.config.pad_token_id\n",
    "                    )\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                \n",
    "                self.scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                train_pbar.set_postfix({\n",
    "                    'loss': loss.item(),\n",
    "                    'lr': self.scheduler.get_last_lr()[0]\n",
    "                })\n",
    "                \n",
    "                if wandb.run:\n",
    "                    wandb.log({\n",
    "                        'train_batch_loss': loss.item(),\n",
    "                        'learning_rate': self.scheduler.get_last_lr()[0],\n",
    "                        'epoch': epoch,\n",
    "                        'step': batch_idx + epoch * len(self.train_loader)\n",
    "                    })\n",
    "            \n",
    "            avg_train_loss = total_loss / len(self.train_loader)\n",
    "            logging.info(f\"Epoch {epoch+1} average training loss: {avg_train_loss:.4f}\")\n",
    "            \n",
    "            if self.val_loader:\n",
    "                val_loss = self.evaluate()\n",
    "                logging.info(f\"Epoch {epoch+1} validation loss: {val_loss:.4f}\")\n",
    "                \n",
    "                is_best = val_loss < best_val_loss\n",
    "                if is_best:\n",
    "                    best_val_loss = val_loss\n",
    "                \n",
    "                self.save_checkpoint(epoch + 1, val_loss, is_best)\n",
    "                \n",
    "                if wandb.run:\n",
    "                    wandb.log({\n",
    "                        'epoch': epoch,\n",
    "                        'avg_train_loss': avg_train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'best_val_loss': best_val_loss\n",
    "                    })\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        n_batches = len(self.val_loader)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with amp.autocast() if self.use_amp else nullcontext():\n",
    "                for batch in tqdm(self.val_loader, desc=\"Evaluating\"):\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    labels = batch['labels'].to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(input_ids, attention_mask)\n",
    "                    loss = F.cross_entropy(\n",
    "                        outputs.view(-1, outputs.size(-1)),\n",
    "                        labels.view(-1),\n",
    "                        ignore_index=self.model.config.pad_token_id\n",
    "                    )\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d60a0-95c4-47b9-95d3-145141ada2e4",
   "metadata": {},
   "source": [
    "## E: Split PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95229673-d4b5-4f92-a663-e00ec4b20fd1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def split_pdfs(source_dir='cern_pdfs', train_ratio=1.0):\n",
    "    source_path = Path(source_dir)\n",
    "    train_path = source_path / 'train'\n",
    "    val_path = source_path / 'val'\n",
    "    \n",
    "    # Create directories\n",
    "    train_path.mkdir(exist_ok=True)\n",
    "    val_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all PDFs\n",
    "    pdf_files = list(source_path.glob('*.pdf'))\n",
    "    random.shuffle(pdf_files)\n",
    "    \n",
    "    # Calculate split\n",
    "    split_idx = int(len(pdf_files) * train_ratio)\n",
    "    train_files = pdf_files[:split_idx]\n",
    "    val_files = pdf_files[split_idx:]\n",
    "    \n",
    "    # Move files\n",
    "    for pdf in train_files:\n",
    "        shutil.move(str(pdf), str(train_path / pdf.name))\n",
    "    \n",
    "    for pdf in val_files:\n",
    "        shutil.move(str(pdf), str(val_path / pdf.name))\n",
    "    \n",
    "    print(f\"Moved {len(train_files)} files to train/\")\n",
    "    print(f\"Moved {len(val_files)} files to val/\")\n",
    "\n",
    "split_pdfs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff8cdc-5c3d-4354-839c-11fb6242343f",
   "metadata": {},
   "source": [
    "## F: Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da422e-36b0-4d2f-8af3-a38c266b3caf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('training.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        config = TransformerConfig(\n",
    "            vocab_size=50257,\n",
    "            max_sequence_length=512,\n",
    "            d_model=1024,      # Increased from 768\n",
    "            n_heads=16,        # Increased from 12\n",
    "            n_layers=12,       # Doubled from 6\n",
    "            d_ff=4096,         # Increased from 3072\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        os.makedirs(\"cern_pdfs/train\", exist_ok=True)\n",
    "        os.makedirs(\"cern_pdfs/val\", exist_ok=True)\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        \n",
    "        logging.info(\"Creating datasets...\")\n",
    "        train_dataset = CERNDataset(\n",
    "            \"cern_pdfs/train\",\n",
    "            tokenizer_name=\"gpt2\",\n",
    "            max_length=config.max_sequence_length\n",
    "        )\n",
    "        val_dataset = CERNDataset(\n",
    "            \"cern_pdfs/val\", \n",
    "            tokenizer_name=\"gpt2\",\n",
    "            max_length=config.max_sequence_length\n",
    "        )\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = CustomLLM(config)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            batch_size=8,     # Reduced from 32 for larger model\n",
    "            learning_rate=5e-4,\n",
    "            min_lr=1e-5,\n",
    "            warmup_steps=100,\n",
    "            num_epochs=11,\n",
    "            device=device,\n",
    "            wandb_project=\"cern-llm\",\n",
    "            checkpoint_dir=\"checkpoints\",\n",
    "            scheduler_type='cosine_warmup'\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        if wandb.run:\n",
    "            wandb.finish()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63bd4c-c0ff-4ee1-ab7e-a3ef3acfa363",
   "metadata": {},
   "source": [
    "# 10: Chat with Local LLM mylabs.lol.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89373bd8-9ab0-4ed0-97e1-485f9b0dcf29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import logging\n",
    "\n",
    "class LLMChat:\n",
    "    def __init__(self, model_path=\"checkpoints/best_model.pt\", device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load model config and weights\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        \n",
    "        # Initialize model with same config\n",
    "        config = TransformerConfig(\n",
    "            vocab_size=50257,\n",
    "            max_sequence_length=512,\n",
    "            d_model=1024,\n",
    "            n_heads=16,\n",
    "            n_layers=12,\n",
    "            d_ff=4096,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        self.model = CustomLLM(config)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        \n",
    "    def generate(self, prompt, max_length=100, temperature=0.7, top_p=0.9):\n",
    "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "                next_token_logits = outputs[:, -1, :] / temperature\n",
    "                filtered_logits = top_p_filtering(next_token_logits, top_p=top_p)\n",
    "                next_token = torch.multinomial(torch.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "                \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "                attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=-1)\n",
    "                \n",
    "                if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    break\n",
    "                    \n",
    "        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def top_p_filtering(logits, top_p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    \n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "    \n",
    "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "# Usage example\n",
    "chat = LLMChat()\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in ['quit', 'exit']:\n",
    "        break\n",
    "    response = chat.generate(user_input)\n",
    "    print(f\"LLM: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e0cb9-dd17-494a-bf44-20c2229ed9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
