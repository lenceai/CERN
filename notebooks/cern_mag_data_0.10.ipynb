{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "## CERN is celebrating its 70th anniversary \n",
    "During a crucial period for high-energy physics, coinciding with the initiation of the third update to the European strategy for particle physics. In this special edition of CERN Courier magazine, early-career researchers share their visions for the future of the field while reflecting on CERN's scientific and societal contributions. The magazine features expert insights into the achievements of the Large Hadron Collider (LHC) and explores the advancements of the hybrid pixel detector technology, emphasizing its applications beyond particle physics.\n",
    "\n",
    "## The CERN Courier website \n",
    "is a rich repository of articles covering a wide array of topics in particle physics, high-energy physics, and associated technological advancements. It provides in-depth reporting on the latest experimental results from CERN and other international laboratories, offering insights into ongoing research and discoveries in the field.\n",
    "\n",
    "## Last 11 years of the CERN Courier Magazine in PDF\n",
    "In this dataset I am downloading the Last 11 years of the CERN Courier Magazine.  I will then take this database and then encode it to be used as a Context Window to ask Questions to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "# 1: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "555b0f1c-a3f7-47cc-9c3e-546e7d19754f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 existing PDF files\n",
      "Starting CERN PDF crawler (pages 0 to 7)\n",
      "\n",
      "Processing page 0...\n",
      "Found 15 new article links on page 0\n",
      "\n",
      "Found PDF: CERNCourier2024MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2896932/files/CERNCourier2024MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 14.7M/14.7M [00:01<00:00, 8.20MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2024MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2893513/files/CERNCourier2024MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.5M/13.5M [00:01<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2024JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2886335/files/CERNCourier2024JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.0M/13.0M [00:01<00:00, 8.04MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023NovDec-digitaledition NEW.pdf\n",
      "URL: https://cds.cern.ch/record/2879381/files/CERNCourier2023NovDec-digitaledition%20NEW.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023NovDec-digitaledition NEW.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 12.3M/12.3M [00:01<00:00, 9.70MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2869155/files/CERNCourier2023SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 19.6M/19.6M [00:05<00:00, 3.46MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2863407/files/CERNCourier2023JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 16.3M/16.3M [00:05<00:00, 3.35MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2857134/files/CERNCourier2023MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 18.3M/18.3M [00:04<00:00, 4.35MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2857133/files/CERNCourier2023MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 15.9M/15.9M [00:02<00:00, 5.93MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2845914/files/CERNCourier2023JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.7M/11.7M [00:01<00:00, 6.24MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2840144/files/CERNCourier2022NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 15.4M/15.4M [00:01<00:00, 8.74MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2826497/files/CERNCourier2022SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.6M/13.6M [00:06<00:00, 2.16MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2807618/files/CERNCourier2022MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 12.9M/12.9M [00:05<00:00, 2.42MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 1...\n",
      "Found 15 new article links on page 1\n",
      "\n",
      "Found PDF: CERNCourier2022MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2804425/files/CERNCourier2022MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:03<00:00, 3.49MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2799462/files/CERNCourier2022JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 9.46M/9.46M [00:02<00:00, 4.61MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2789409/files/CERNCourier2021NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.9M/11.9M [00:01<00:00, 6.29MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2782568/files/CERNCourier2021SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 15.1M/15.1M [00:07<00:00, 2.11MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2773907/files/CERNCourier2021JulAug-digitaledition.pdf\n",
      "Error downloading CERNCourier2021JulAug-digitaledition.pdf: 404 Client Error: Not Found for url: https://cds.cern.ch/record/2773907/files/CERNCourier2021JulAug-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier2021MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2765233/files/CERNCourier2021MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 9.97M/9.97M [00:01<00:00, 7.10MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2753402/files/CERNCourier2021MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.8M/13.8M [00:01<00:00, 11.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2750037/files/CERNCourier2021JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 21.6M/21.6M [00:01<00:00, 12.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2743359/files/CERNCourier2020NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 10.6M/10.6M [00:01<00:00, 9.85MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2743358/files/CERNCourier2020SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.1M/13.1M [00:01<00:00, 10.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2722711/files/CERNCourier2020JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.5M/13.5M [00:01<00:00, 12.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2717129/files/CERNCourier2020MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 12.2M/12.2M [00:01<00:00, 9.70MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2712176/files/CERNCourier2020MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 13.7M/13.7M [00:01<00:00, 11.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2706508/files/CERNCourier2020JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 8.74M/8.74M [00:00<00:00, 9.34MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2701615/files/CERNCourier2019NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.3M/11.3M [00:01<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 2...\n",
      "Found 15 new article links on page 2\n",
      "\n",
      "Found PDF: CERNCourier2019SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2689203/files/CERNCourier2019SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.6M/11.6M [00:01<00:00, 7.50MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CCJulAug19-digital.pdf\n",
      "URL: https://cds.cern.ch/record/2681906/files/CCJulAug19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CCJulAug19-digital.pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 8.52MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CCMayJun19-digital.pdf\n",
      "URL: https://cds.cern.ch/record/2673718/files/CCMayJun19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CCMayJun19-digital.pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15.4M/15.4M [00:01<00:00, 12.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2666160/files/CERNCourier2019MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 12.1M/12.1M [00:01<00:00, 11.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2654576/files/CERNCourier2019JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 11.2M/11.2M [00:01<00:00, 7.28MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Dec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2649360/files/CERNCourier2018Dec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Dec-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 10.2M/10.2M [00:01<00:00, 9.58MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Nov-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2645275/files/CERNCourier2018Nov-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Nov-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 8.90M/8.90M [00:00<00:00, 9.51MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Oct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2640475/files/CERNCourier2018Oct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Oct-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 8.13M/8.13M [00:00<00:00, 8.73MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Sep-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2636286/files/CERNCourier2018Sep-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Sep-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 15.6M/15.6M [00:01<00:00, 12.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2628313/files/CERNCourier2018JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 6.69M/6.69M [00:00<00:00, 8.40MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier June 2018 (Volume 53 Issue 5).pdf\n",
      "URL: https://home.cern/sites/default/files/2018-06/CERN%20Courier%20June%202018%20%28Volume%2053%20Issue%205%29.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier June 2018 (Volume 53 Issue 5).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 8.69M/8.69M [00:01<00:00, 6.94MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier May 2018 (Volume 53 Issue 4).pdf\n",
      "URL: https://cds.cern.ch/record/2318574/files/CERN%20Courier%20May%202018%20(Volume%2053%20Issue%204).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier May 2018 (Volume 53 Issue 4).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 9.32M/9.32M [00:01<00:00, 6.67MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 3 (April 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2309976/files/CERN%20Courier%20Volume%2058%20Issue%203%20(April%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 3 (April 2018).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 13.3M/13.3M [00:01<00:00, 10.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 2 (March 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2304934/files/CERN%20Courier%20Volume%2058%20Issue%202%20(March%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 2 (March 2018).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 9.73MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2300591/files/CERN%20Courier%20Volume%2058%20Issue%201%20(Jan-Feb%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 8.53MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 3...\n",
      "Found 15 new article links on page 3\n",
      "\n",
      "Found PDF: CERN Courier December 2017 (Volume 57 Issue 10).pdf\n",
      "URL: https://cds.cern.ch/record/2292627/files/CERN%20Courier%20December%202017%20(Volume%2057%20Issue%2010).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier December 2017 (Volume 57 Issue 10).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 8.04M/8.04M [00:01<00:00, 5.80MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 9 (November 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2289267/files/CERN%20Courier%20Volume%2057%20Issue%209%20(November%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 9 (November 2017).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 8.51M/8.51M [00:00<00:00, 9.13MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 8 October 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2285637/files/CERN%20Courier%20Volume%2057%20Issue%208%20October%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 8 October 2017.pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 9.17M/9.17M [00:00<00:00, 9.69MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 7 (September 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2281303/files/CERN%20Courier%20Volume%2057%20Issue%207%20(September%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 7 (September 2017).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 10.1M/10.1M [00:01<00:00, 9.15MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 6 (July-August 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2273705/files/CERN%20Courier%20Volume%2057%20Issue%206%20(July-August%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 6 (July-August 2017).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████| 10.6M/10.6M [00:01<00:00, 9.84MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 4 .pdf\n",
      "URL: https://cds.cern.ch/record/2259560/files/CERN%20Courier%20Volume%2057%20Issue%204%20.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 4 .pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 12.5M/12.5M [00:01<00:00, 8.01MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 3 April 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2256135/files/CERN%20Courier%20Volume%2057%20Issue%203%20April%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 3 April 2017.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 6.81M/6.81M [00:00<00:00, 8.59MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 2 March 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2252407/files/CERN%20Courier%20Volume%2057%20Issue%202%20March%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 2 March 2017.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 9.75M/9.75M [00:01<00:00, 10.1MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf\n",
      "URL: https://cds.cern.ch/record/2241972/files/CERN%20Courier%20Jan-Feb%202017%20(Volume%2057%20issue%201).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 9.89M/9.89M [00:00<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier November 2016 (Volume 56 Issue 9).pdf\n",
      "URL: https://cds.cern.ch/record/2224294/files/CERN%20Courier%20November%202016%20(Volume%2056%20Issue%209).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier November 2016 (Volume 56 Issue 9).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 9.23M/9.23M [00:01<00:00, 6.56MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier October 2016 (Volume 56 Issue 8).pdf\n",
      "URL: https://cds.cern.ch/record/2219443/files/CERN%20Courier%20October%202016%20(Volume%2056%20Issue%208).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier October 2016 (Volume 56 Issue 8).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 16.3M/16.3M [00:01<00:00, 11.8MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier September 2016 (Volume 56 Issue 7).pdf\n",
      "URL: https://cds.cern.ch/record/2211464/files/CERN%20Courier%20September%202016%20(Volume%2056%20Issue%207).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier September 2016 (Volume 56 Issue 7).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 15.3M/15.3M [00:01<00:00, 10.8MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier July-August 2016 (Volume 56 Issue 6).pdf\n",
      "URL: http://cds.cern.ch/record/2198166/files/CERN%20Courier%20July-August%202016%20(Volume%2056%20Issue%206).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier July-August 2016 (Volume 56 Issue 6).pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 21.9M/21.9M [00:46<00:00, 497kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 4...\n",
      "Found 15 new article links on page 4\n",
      "\n",
      "Found PDF: CERN Courier June 2016 (Volume 56 Issue 5).pdf\n",
      "URL: https://cds.cern.ch/record/2155287/files/CERN%20Courier%20June%202016%20(Volume%2056%20Issue%205).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier June 2016 (Volume 56 Issue 5).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 9.50M/9.50M [00:01<00:00, 6.79MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier May 2016 (Volume 56 Issue 4).pdf\n",
      "URL: http://cds.cern.ch/record/2146835/files/CERN%20Courier%20May%202016%20(Volume%2056%20Issue%204).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier May 2016 (Volume 56 Issue 4).pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10.2M/10.2M [00:18<00:00, 584kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Mar 2016 (Volume 56 Issue 2).pdf\n",
      "URL: https://cds.cern.ch/record/2131754/files/CERN%20Courier%20Mar%202016%20(Volume%2056%20Issue%202).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Mar 2016 (Volume 56 Issue 2).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 9.46M/9.46M [00:01<00:00, 6.78MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 5...\n",
      "Found 15 new article links on page 5\n",
      "\n",
      "Found PDF: CERNCourier2013Oct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1603700/files/CERNCourier2013Oct-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Oct-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1735007/files/CERNCourier2013Oct-digitaledition.pdf\n",
      "\n",
      "Processing page 6...\n",
      "Found 5 new article links on page 6\n",
      "\n",
      "Found PDF: CERN Courier June 2013.pdf\n",
      "URL: http://cds.cern.ch/record/1550751/files/CERN%20Courier%20June%202013.pdf\n",
      "Error downloading CERN Courier June 2013.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734960/files/CERN%20Courier%20June%202013.pdf\n",
      "\n",
      "Found PDF: CERN Courier digital edition May 2013.pdf\n",
      "URL: http://cds.cern.ch/record/1544352/files/CERN%20Courier%20digital%20edition%20May%202013.pdf\n",
      "Error downloading CERN Courier digital edition May 2013.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734947/files/CERN%20Courier%20digital%20edition%20May%202013.pdf\n",
      "\n",
      "Found PDF: CERNCourier2013Apr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1537017/files/CERNCourier2013Apr-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Apr-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734933/files/CERNCourier2013Apr-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier2013Mar-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1517538/files/CERNCourier2013Mar-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Mar-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734919/files/CERNCourier2013Mar-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier-2013-53-1.pdf\n",
      "URL: http://cds.cern.ch/record/1514615/files/CERNCourier-2013-53-1.pdf\n",
      "Error downloading CERNCourier-2013-53-1.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734891/files/CERNCourier-2013-53-1.pdf\n",
      "\n",
      "Processing page 7...\n",
      "Found 0 new article links on page 7\n",
      "\n",
      "Download Summary:\n",
      "--------------------\n",
      "Total PDFs found: 64\n",
      "Successfully downloaded: 57\n",
      "Skipped (already downloaded): 0\n",
      "Failed downloads: 7\n",
      "\n",
      "Failed downloads:\n",
      "- CERNCourier2021JulAug-digitaledition.pdf\n",
      "- CERNCourier2013Oct-digitaledition.pdf\n",
      "- CERN Courier June 2013.pdf\n",
      "- CERN Courier digital edition May 2013.pdf\n",
      "- CERNCourier2013Apr-digitaledition.pdf\n",
      "- CERNCourier2013Mar-digitaledition.pdf\n",
      "- CERNCourier-2013-53-1.pdf\n",
      "CPU times: user 12.6 s, sys: 2.28 s, total: 14.9 s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin, unquote\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "class CERNPDFCrawler:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://home.cern/resources\"\n",
    "        self.session = requests.Session()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.download_folder = \"cern_pdfs\"\n",
    "        self.processed_article_urls = set()\n",
    "        self.downloaded_files = set()\n",
    "        \n",
    "        if not os.path.exists(self.download_folder):\n",
    "            os.makedirs(self.download_folder)\n",
    "        self.load_existing_files()\n",
    "\n",
    "    def load_existing_files(self):\n",
    "        for filename in os.listdir(self.download_folder):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                self.downloaded_files.add(filename)\n",
    "        print(f\"Found {len(self.downloaded_files)} existing PDF files\")\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "                    return None\n",
    "                time.sleep(2 ** attempt)\n",
    "        return None\n",
    "\n",
    "    def extract_pdf_urls_from_text(self, text):\n",
    "        \"\"\"Extract PDF URLs from text content including 'File path:' patterns\"\"\"\n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Look for \"File path:\" pattern\n",
    "        file_path_matches = re.finditer(r'File path:\\s*(https?://[^\\s<>\"]+\\.pdf)', text, re.IGNORECASE)\n",
    "        for match in file_path_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        # Look for direct PDF links\n",
    "        pdf_link_matches = re.finditer(r'href=\"(https?://[^\\s<>\"]+\\.pdf)\"', text, re.IGNORECASE)\n",
    "        for match in pdf_link_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        return pdf_urls\n",
    "\n",
    "    def find_courier_links(self, page_url):\n",
    "        content = self.get_page_content(page_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        courier_links = []\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/resources/courier/' in href or '/record/' in href:\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                if full_url not in self.processed_article_urls:\n",
    "                    courier_links.append(full_url)\n",
    "                    self.processed_article_urls.add(full_url)\n",
    "        \n",
    "        return courier_links\n",
    "\n",
    "    def find_pdf_links(self, article_url):\n",
    "        \"\"\"Find all PDF download links on an article page\"\"\"\n",
    "        content = self.get_page_content(article_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Extract URLs from text content\n",
    "        pdf_urls.update(self.extract_pdf_urls_from_text(content))\n",
    "        \n",
    "        # Parse with BeautifulSoup for structured extraction\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Look for links containing PDF\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.lower().endswith('.pdf'):\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                pdf_urls.add(full_url)\n",
    "        \n",
    "        return list(pdf_urls)\n",
    "\n",
    "    def sanitize_filename(self, url):\n",
    "        \"\"\"Create a safe filename from URL\"\"\"\n",
    "        filename = unquote(url.split('/')[-1])\n",
    "        # Remove or replace unsafe characters\n",
    "        filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "        return filename\n",
    "\n",
    "    def download_pdf(self, pdf_url, filename):\n",
    "        if filename in self.downloaded_files:\n",
    "            print(f\"Skipping {filename} - already downloaded\")\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            response = self.session.get(pdf_url, headers=self.headers, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            file_path = os.path.join(self.download_folder, filename)\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(file_path, 'wb') as file, tqdm(\n",
    "                desc=filename,\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = file.write(data)\n",
    "                    pbar.update(size)\n",
    "            \n",
    "            self.downloaded_files.add(filename)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def crawl_and_download(self, start_page=0, end_page=7):\n",
    "        print(f\"Starting CERN PDF crawler (pages {start_page} to {end_page})\")\n",
    "        \n",
    "        found_pdfs = 0\n",
    "        downloaded_pdfs = 0\n",
    "        skipped_pdfs = 0\n",
    "        failed_downloads = []\n",
    "        \n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = f\"{self.base_url}?type=52&page={page_num}\"\n",
    "            print(f\"\\nProcessing page {page_num}...\")\n",
    "            \n",
    "            courier_links = self.find_courier_links(page_url)\n",
    "            print(f\"Found {len(courier_links)} new article links on page {page_num}\")\n",
    "            \n",
    "            for article_url in courier_links:\n",
    "                pdf_urls = self.find_pdf_links(article_url)\n",
    "                \n",
    "                for pdf_url in pdf_urls:\n",
    "                    found_pdfs += 1\n",
    "                    filename = self.sanitize_filename(pdf_url)\n",
    "                    \n",
    "                    print(f\"\\nFound PDF: {filename}\")\n",
    "                    print(f\"URL: {pdf_url}\")\n",
    "                    \n",
    "                    if filename in self.downloaded_files:\n",
    "                        print(f\"Skipping - already downloaded\")\n",
    "                        skipped_pdfs += 1\n",
    "                        continue\n",
    "                        \n",
    "                    if self.download_pdf(pdf_url, filename):\n",
    "                        downloaded_pdfs += 1\n",
    "                    else:\n",
    "                        failed_downloads.append(filename)\n",
    "                \n",
    "                time.sleep(1)\n",
    "        \n",
    "        print(\"\\nDownload Summary:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Total PDFs found: {found_pdfs}\")\n",
    "        print(f\"Successfully downloaded: {downloaded_pdfs}\")\n",
    "        print(f\"Skipped (already downloaded): {skipped_pdfs}\")\n",
    "        print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "        if failed_downloads:\n",
    "            print(\"\\nFailed downloads:\")\n",
    "            for fail in failed_downloads:\n",
    "                print(f\"- {fail}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = CERNPDFCrawler()\n",
    "    crawler.crawl_and_download(0, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78865c",
   "metadata": {},
   "source": [
    "# 2: Encode the CERN PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93cde4-96fd-47f5-b862-93900a21ae9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.db_dir = Path(db_dir)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                \n",
    "                # Extract text from each page\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        \"\"\"Process all PDFs in the directory and return chunks with metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        # Process each PDF file\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Split text into chunks\n",
    "                chunks = self.text_splitter.create_documents(\n",
    "                    texts=[text],\n",
    "                    metadatas=[{\"source\": pdf_path.name}]\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def create_vector_db(self):\n",
    "        \"\"\"Create and populate the vector database\"\"\"\n",
    "        # Get text chunks\n",
    "        chunks = self.process_pdfs()\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"No text chunks were created. Check the PDF processing.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nCreating vector database with {len(chunks)} chunks...\")\n",
    "        \n",
    "        # Create and persist the vector store\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=str(self.db_dir)\n",
    "        )\n",
    "        \n",
    "        # Persist the database\n",
    "        vectordb.persist()\n",
    "        print(f\"Vector database created and saved to {self.db_dir}\")\n",
    "        \n",
    "        return vectordb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        processor = PDFProcessor()\n",
    "        vectordb = processor.create_vector_db()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd11e0-aa06-440b-af5d-d3835d32be28",
   "metadata": {},
   "source": [
    "# 3: Chat with Cern Magazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6facc867-bba9-44a6-9c78-2f199b187dfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:20: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN Research Assistant Ready!\n",
      "Ask questions about CERN research (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  What is driving the accelerated expansion of the Universe?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The accelerated expansion of the universe is one of the most profound discoveries in cosmology. This phenomenon is primarily attributed to what is broadly referred to as dark energy, although its exact nature and physics remain unknown. The concept of dark energy emerged from observations that contradicted the then-existing understanding of the universe's expansion. Specifically, in 1998, researchers found that, contrary to expectations that the universe's expansion was slowing down, it was actually speeding up. This conclusion was reached through comparisons of the expansion rate of the universe over time, significantly strengthened by combined measurements with those of the High-z Supernova Search Team (CERNCourier2019SepOct-digitaledition.pdf, start_index: 186634).\n",
      "\n",
      "One early attempt to explain the universe's dynamics involved the cosmological constant, initially introduced by Einstein to allow for a static universe within the framework of general relativity. This cosmological constant is linked to the concept of vacuum energy. However, calculations of the vacuum energy, summing the contributions from presumed quantum states in the universe, produce a value for the expansion rate that is about 120 orders of magnitude higher than what is observed. Such a rate would have prevented the formation of galaxies, stars, and planets, indicating that vacuum energy in its straightforward interpretation cannot be the final answer (CERNCourier2019SepOct-digitaledition.pdf, start_index: 188182).\n",
      "\n",
      "The discovery of the universe's accelerated expansion also offered a solution to the so-called \"age crisis,\" where some stars appeared older than the universe itself. This discrepancy suggested either an overestimation of stellar ages or an incorrect understanding of the universe's age and expansion rate. The concept of accelerated expansion helped reconcile these differences by implying that previous models of the universe's expansion were incomplete or inaccurate (CERNCourier2019SepOct-digitaledition.pdf, start_index: 187594).\n",
      "\n",
      "The nature of dark energy remains one of the most significant puzzles in modern fundamental physics. It is the primary driver of the dynamical evolution of the universe in the current epoch. Theories and experimental programs have been developed to understand dark energy, whether it is due to a cosmological constant, a new dynamical field, a deviation from general relativity on cosmological scales, or an entirely different mechanism. The current lambda-cold-dark-matter (ΛCDM) model incorporates dark energy but does not yet provide a complete explanation of its nature. Physicists and astronomers believe that uncovering the nature of dark energy could lead to a revolution in physics, given its fundamental role in the universe's expansion (CERN Courier Volume 57 Issue 4 .pdf, start_index: 58211).\n",
      "\n",
      "In summary, the accelerated expansion of the universe is driven by dark energy, a mysterious force whose exact nature is still under investigation. Despite significant progress in understanding the universe's expansion through observations and theoretical models, dark energy remains a central unanswered question in cosmology.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 317 ms, total: 1.96 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class CERNResearchAssistant:\n",
    "    def __init__(self, db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        # Initialize the vector store\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        # Initialize the language model\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Create the retriever\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Setup the prompt template\n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Setup the RAG chain\n",
    "        self.chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Ask a question about CERN research\"\"\"\n",
    "        try:\n",
    "            response = self.chain.invoke(question)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {e}\"\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize the assistant\n",
    "        assistant = CERNResearchAssistant()\n",
    "        \n",
    "        print(\"CERN Research Assistant Ready!\")\n",
    "        print(\"Ask questions about CERN research (type 'quit' to exit)\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            response = assistant.query(question)\n",
    "            print(\"\\nAssistant:\", response)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1f129-fdef-4725-a154-1bffd26c5b2b",
   "metadata": {},
   "source": [
    "# 4: Fine Tuning with OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6a4e76-cbb5-43da-92c4-e78531886a02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   2%|██▎                                                                                                                                 | 1/57 [00:05<04:56,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2024MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|████▋                                                                                                                               | 2/57 [00:10<04:35,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2022NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   5%|██████▉                                                                                                                             | 3/57 [00:13<03:49,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier Volume 57 Issue 8 October 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|█████████▎                                                                                                                          | 4/57 [00:17<03:45,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2023MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   9%|███████████▌                                                                                                                        | 5/57 [00:24<04:30,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2018Sep-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█████████████▉                                                                                                                      | 6/57 [00:28<04:06,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2019NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  12%|████████████████▏                                                                                                                   | 7/57 [00:31<03:32,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 57 Issue 3 April 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|██████████████████▌                                                                                                                 | 8/57 [00:40<04:31,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier September 2016 (Volume 56 Issue 7).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  16%|████████████████████▊                                                                                                               | 9/57 [00:46<04:33,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERN Courier Volume 57 Issue 6 (July-August 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|██████████████████████▉                                                                                                            | 10/57 [00:50<04:07,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2021MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  19%|█████████████████████████▎                                                                                                         | 11/57 [00:57<04:28,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|███████████████████████████▌                                                                                                       | 12/57 [01:01<03:55,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2022JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  23%|█████████████████████████████▉                                                                                                     | 13/57 [01:05<03:39,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 57 Issue 9 (November 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|████████████████████████████████▏                                                                                                  | 14/57 [01:10<03:29,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 14 chunks from CERN Courier May 2018 (Volume 53 Issue 4).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  26%|██████████████████████████████████▍                                                                                                | 15/57 [01:15<03:28,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERN Courier Volume 57 Issue 2 March 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  28%|████████████████████████████████████▊                                                                                              | 16/57 [01:21<03:34,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier June 2018 (Volume 53 Issue 5).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  30%|███████████████████████████████████████                                                                                            | 17/57 [01:26<03:21,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2020NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|█████████████████████████████████████████▎                                                                                         | 18/57 [01:38<04:47,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERN Courier July-August 2016 (Volume 56 Issue 6).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  33%|███████████████████████████████████████████▋                                                                                       | 19/57 [01:42<03:55,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2020JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  35%|█████████████████████████████████████████████▉                                                                                     | 20/57 [01:47<03:43,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2022SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  37%|████████████████████████████████████████████████▎                                                                                  | 21/57 [01:52<03:23,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CCJulAug19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|██████████████████████████████████████████████████▌                                                                                | 22/57 [01:58<03:20,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  40%|████████████████████████████████████████████████████▊                                                                              | 23/57 [02:06<03:37,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  42%|███████████████████████████████████████████████████████▏                                                                           | 24/57 [02:18<04:23,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2021JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|█████████████████████████████████████████████████████████▍                                                                         | 25/57 [02:23<03:54,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 26 chunks from CERNCourier2020MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|███████████████████████████████████████████████████████████▊                                                                       | 26/57 [02:30<03:35,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier Volume 57 Issue 4 .pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  47%|██████████████████████████████████████████████████████████████                                                                     | 27/57 [02:41<04:06,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier October 2016 (Volume 56 Issue 8).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  49%|████████████████████████████████████████████████████████████████▎                                                                  | 28/57 [02:47<03:44,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  51%|██████████████████████████████████████████████████████████████████▋                                                                | 29/57 [02:51<03:06,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier December 2017 (Volume 57 Issue 10).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  53%|████████████████████████████████████████████████████████████████████▉                                                              | 30/57 [02:57<02:49,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2022MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|███████████████████████████████████████████████████████████████████████▏                                                           | 31/57 [03:05<02:55,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier June 2016 (Volume 56 Issue 5).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|█████████████████████████████████████████████████████████████████████████▌                                                         | 32/57 [03:10<02:35,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  58%|███████████████████████████████████████████████████████████████████████████▊                                                       | 33/57 [03:16<02:29,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 58 Issue 2 (March 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  60%|██████████████████████████████████████████████████████████████████████████████▏                                                    | 34/57 [03:25<02:41,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier Volume 58 Issue 3 (April 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|████████████████████████████████████████████████████████████████████████████████▍                                                  | 35/57 [03:31<02:29,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Mar 2016 (Volume 56 Issue 2).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  63%|██████████████████████████████████████████████████████████████████████████████████▋                                                | 36/57 [03:41<02:44,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 chunks from CERNCourier2020JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  65%|█████████████████████████████████████████████████████████████████████████████████████                                              | 37/57 [03:46<02:16,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2023NovDec-digitaledition NEW.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|███████████████████████████████████████████████████████████████████████████████████████▎                                           | 38/57 [03:52<02:07,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|█████████████████████████████████████████████████████████████████████████████████████████▋                                         | 39/57 [03:58<01:53,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  70%|███████████████████████████████████████████████████████████████████████████████████████████▉                                       | 40/57 [04:01<01:32,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2023JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  72%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 41/57 [04:05<01:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2024MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  74%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 42/57 [04:09<01:09,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERN Courier Volume 57 Issue 7 (September 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 43/57 [04:13<01:03,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████                              | 44/57 [04:19<01:01,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2020MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 45/57 [04:21<00:50,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier November 2016 (Volume 56 Issue 9).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 46/57 [04:26<00:46,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 47/57 [04:30<00:41,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERNCourier2018JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 48/57 [04:34<00:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2020SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 49/57 [04:39<00:36,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CCMayJun19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 50/57 [04:44<00:32,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERNCourier2018Dec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 51/57 [04:49<00:27,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERNCourier2018Oct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 52/57 [04:56<00:27,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 chunks from CERNCourier2022MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 53/57 [05:03<00:22,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 14 chunks from CERN Courier May 2016 (Volume 56 Issue 4).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 54/57 [05:07<00:16,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 55/57 [05:11<00:10,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 56/57 [05:16<00:04,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERNCourier2018Nov-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [05:21<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2024JanFeb-digitaledition.pdf\n",
      "\n",
      "Created 1104 valid training examples\n",
      "Training data saved to finetune_data/training_data.jsonl\n",
      "\n",
      "Step 2: Submitting fine-tuning job...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file uploaded with ID: file-57Mt4LPzbyj7jMS49CcMc8\n",
      "Fine-tuning job created with ID: ftjob-8tjSyNrQ1YLXahS4y7m2V82M\n",
      "\n",
      "Monitoring fine-tuning job...\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: succeeded\n",
      "Trained tokens: 6,454,262\n",
      "\n",
      "Fine-tuning completed successfully!\n",
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\n",
      "\n",
      "Fine-tuning process completed successfully!\n",
      "You can now use your fine-tuned model with ID: ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\n",
      "CPU times: user 5min 20s, sys: 1.74 s, total: 5min 22s\n",
      "Wall time: 53min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class FineTunePrep:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", output_dir=\"finetune_data\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Constants for token limits\n",
    "        self.MAX_TOKENS_PER_EXAMPLE = 3000  # Leave room for system and user messages\n",
    "        self.MIN_TOKENS_PER_EXAMPLE = 500   # Ensure meaningful content\n",
    "        \n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count tokens in a text string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def split_into_chunks(self, text):\n",
    "        \"\"\"Split text into chunks of appropriate token length\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Split into sentences (roughly)\n",
    "        sentences = [s.strip() + \".\" for s in text.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If single sentence is too long, split it into smaller parts\n",
    "            if sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                words = sentence.split()\n",
    "                temp_chunk = \"\"\n",
    "                temp_tokens = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    word_tokens = self.count_tokens(word + \" \")\n",
    "                    if temp_tokens + word_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                        if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = word + \" \"\n",
    "                        temp_tokens = word_tokens\n",
    "                    else:\n",
    "                        temp_chunk += word + \" \"\n",
    "                        temp_tokens += word_tokens\n",
    "                \n",
    "                if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(temp_chunk.strip())\n",
    "                continue\n",
    "            \n",
    "            # If adding this sentence would exceed limit, save current chunk and start new one\n",
    "            if current_tokens + sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "                current_tokens = sentence_tokens\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk if it's long enough\n",
    "        if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def create_training_examples(self, chunks):\n",
    "        \"\"\"Create training examples from text chunks\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Create the messages for this chunk\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What are the key findings or developments described in this CERN research?\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"Based on the CERN publications: {chunk}\"\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Verify total tokens\n",
    "            total_tokens = sum(self.count_tokens(msg[\"content\"]) for msg in messages)\n",
    "            if total_tokens <= 4096:  # GPT-4's context window\n",
    "                examples.append({\"messages\": messages})\n",
    "            \n",
    "        return examples\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Process PDFs and prepare training data\"\"\"\n",
    "        all_examples = []\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # First split text into appropriate chunks\n",
    "                chunks = self.split_into_chunks(text)\n",
    "                print(f\"\\nCreated {len(chunks)} chunks from {pdf_path.name}\")\n",
    "                \n",
    "                # Create examples from chunks\n",
    "                examples = self.create_training_examples(chunks)\n",
    "                all_examples.extend(examples)\n",
    "        \n",
    "        # Save training data\n",
    "        training_file_path = self.output_dir / \"training_data.jsonl\"\n",
    "        with open(training_file_path, 'w', encoding='utf-8') as f:\n",
    "            for example in all_examples:\n",
    "                f.write(json.dumps(example) + '\\n')\n",
    "        \n",
    "        print(f\"\\nCreated {len(all_examples)} valid training examples\")\n",
    "        print(f\"Training data saved to {training_file_path}\")\n",
    "        return training_file_path\n",
    "\n",
    "    def submit_fine_tuning_job(self, training_file_path):\n",
    "        \"\"\"Submit fine-tuning job to OpenAI\"\"\"\n",
    "        try:\n",
    "            # Upload the training file\n",
    "            with open(training_file_path, 'rb') as f:\n",
    "                training_file = self.client.files.create(\n",
    "                    file=f,\n",
    "                    purpose='fine-tune'\n",
    "                )\n",
    "            print(f\"Training file uploaded with ID: {training_file.id}\")\n",
    "            \n",
    "            # Create fine-tuning job\n",
    "            job = self.client.fine_tuning.jobs.create(\n",
    "                training_file=training_file.id,\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 2,\n",
    "                    \"learning_rate_multiplier\": 0.1\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Fine-tuning job created with ID: {job.id}\")\n",
    "            return job.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting fine-tuning job: {e}\")\n",
    "            return None\n",
    "\n",
    "    def monitor_fine_tuning_job(self, job_id):\n",
    "        \"\"\"Monitor the status of a fine-tuning job\"\"\"\n",
    "        print(\"\\nMonitoring fine-tuning job...\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                job = self.client.fine_tuning.jobs.retrieve(job_id)\n",
    "                print(f\"\\nStatus: {job.status}\")\n",
    "                \n",
    "                # Safely print additional info if available\n",
    "                if hasattr(job, 'trained_tokens') and job.trained_tokens is not None:\n",
    "                    print(f\"Trained tokens: {job.trained_tokens:,}\")\n",
    "                if hasattr(job, 'training_accuracy') and job.training_accuracy is not None:\n",
    "                    print(f\"Training accuracy: {job.training_accuracy:.4f}\")\n",
    "                \n",
    "                if job.status == 'succeeded':\n",
    "                    print(f\"\\nFine-tuning completed successfully!\")\n",
    "                    print(f\"Fine-tuned model ID: {job.fine_tuned_model}\")\n",
    "                    return job\n",
    "                elif job.status == 'failed':\n",
    "                    print(f\"\\nFine-tuning failed: {getattr(job, 'error', 'Unknown error')}\")\n",
    "                    return job\n",
    "                elif job.status == 'cancelled':\n",
    "                    print(\"\\nFine-tuning job was cancelled\")\n",
    "                    return job\n",
    "                \n",
    "                time.sleep(60)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking job status: {e}\")\n",
    "                time.sleep(60)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        prep = FineTunePrep()\n",
    "        \n",
    "        print(\"Step 1: Preparing training data...\")\n",
    "        training_file_path = prep.prepare_training_data()\n",
    "        \n",
    "        print(\"\\nStep 2: Submitting fine-tuning job...\")\n",
    "        job_id = prep.submit_fine_tuning_job(training_file_path)\n",
    "        \n",
    "        if job_id:\n",
    "            final_job = prep.monitor_fine_tuning_job(job_id)\n",
    "            \n",
    "            if getattr(final_job, 'status', None) == 'succeeded':\n",
    "                print(\"\\nFine-tuning process completed successfully!\")\n",
    "                print(f\"You can now use your fine-tuned model with ID: {final_job.fine_tuned_model}\")\n",
    "                \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3602ca-6c2e-4c37-bc93-4be91e88d430",
   "metadata": {},
   "source": [
    "# 5: RAG vs Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb37745d-117b-4912-a95e-1b9370c8fc2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3859642/655315240.py:24: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN Research Assistant Comparison\n",
      "Compare Fine-tuned model vs RAG approach\n",
      "Type 'quit' to exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  why is the universe expanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: why is the universe expanding\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The expansion of the universe is a fundamental observation in cosmology, first noted by Edwin Hubble in the 1920s. The primary reason for this expansion is attributed to the Big Bang, the event that marked the beginning of the universe approximately 13.8 billion years ago. \n",
      "\n",
      "Initially, the universe was in an extremely hot and dense state, and as it expanded, it cooled down, allowing for the formation of subatomic particles and later atoms. This expansion is described by the solutions to Einstein's equations of general relativity, which govern the dynamics of spacetime.\n",
      "\n",
      "In addition to the initial expansion, observations show that the rate of expansion is currently accelerating. This acceleration is attributed to a mysterious form of energy known as dark energy, which makes up about 68% of the universe. The exact nature of dark energy is still one of the biggest questions in cosmology.\n",
      "\n",
      "In summary, the universe is expanding due to the initial conditions set by the Big Bang and is currently accelerating in its expansion due to dark energy.\n",
      "Response time: 2.73 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The universe is expanding due to a phenomenon known as dark energy, which is driving the accelerated expansion. This was first realized about 20 years ago and has been confirmed by numerous observations (CERN Courier Volume 57 Issue 4, May 2017). The expansion rate of the universe, defined by the Hubble constant, is a fundamental quantity in cosmology and is usually determined using techniques involving Cepheid variables and supernovae (CERN Courier Volume 57 Issue 3, April 2017).\n",
      "\n",
      "One idea is that the cosmological constant, initially introduced by Einstein, is linked to vacuum energy. However, the vacuum energy alone cannot fully explain the observed expansion rate, as theoretical calculations produce a value about 120 orders of magnitude higher than what is observed. This discrepancy suggests that the vacuum energy cannot be the final answer (CERNCourier2019SepOct-digitaledition.pdf).\n",
      "\n",
      "The source and physics of dark energy remain unknown, and it is an ongoing area of research. Physicists and astronomers are exploring various theoretical models and experimental programs to understand this mysterious component of the universe. The current understanding is based on the lambda-cold-dark-matter (ΛCDM) model, but the exact nature of dark energy is still one of the most important puzzles in modern fundamental physics (CERN Courier Volume 57 Issue 4, May 2017).\n",
      "\n",
      "In summary, the universe is expanding due to dark energy, but the exact nature and source of this energy are still not fully understood. Further observations and research are needed to uncover the physics behind this phenomenon.\n",
      "Response time: 5.71 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  what is the Future Circular Collider (FCC) feasibility study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: what is the Future Circular Collider (FCC) feasibility study\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The Future Circular Collider (FCC) feasibility study is a comprehensive assessment of the technical, financial, and organizational aspects of a proposed next-generation particle accelerator complex at CERN. The FCC aims to explore the energy frontier of particle physics beyond the capabilities of the Large Hadron Collider (LHC) and to address fundamental questions about the universe, such as the nature of dark matter and the properties of the Higgs boson.\n",
      "\n",
      "The feasibility study, which began in 2014, is part of a broader effort to develop a long-term vision for the future of particle physics. It includes detailed studies of the accelerator technologies, detector concepts, and experimental programs that would be required for the FCC, as well as an evaluation of the potential scientific impact of the project.\n",
      "\n",
      "The FCC feasibility study is being conducted by a large international collaboration of scientists and engineers, and it is expected to be completed in the coming years. The results of the study will be used to inform the decision-making process for the future of particle physics at CERN and around the world.\n",
      "Response time: 2.50 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The Future Circular Collider (FCC) feasibility study is an initiative aimed at exploring the potential for a new particle collider that would succeed the Large Hadron Collider (LHC) at CERN. The study is focused on the development of a circular collider that could serve as a Higgs factory and later evolve into a hadron collider with significantly higher energy capabilities than the LHC.\n",
      "\n",
      "1. **Purpose and Vision**: The FCC feasibility study is part of a broader vision outlined in the European strategy for particle physics. The goal is to prepare for a Higgs factory, which would be followed by a future hadron collider with sensitivity to energy scales an order of magnitude higher than those achievable at the LHC (CERNCourier2021JanFeb).\n",
      "\n",
      "2. **Workshops and Collaborations**: The FCC study has involved numerous workshops and collaborations. For instance, a workshop held online from 9 to 13 November brought together around 500 scientists, engineers, and stakeholders to discuss the roadmap for realizing this vision (CERNCourier2021JanFeb). Another significant meeting took place in Brussels from 24 to 28 June, where more than 400 researchers gathered to discuss innovations in superconductivity, high-field magnets, and other technologies relevant to the FCC (CERNCourier2019SepOct).\n",
      "\n",
      "3. **Horizon 2020 Projects**: The FCC feasibility study is supported by European Union projects such as the Horizon 2020 FCC Innovation Study (FCCIS) and the EuroCirCol project. These projects aim to produce conceptual design studies and support the preparation of feasibility studies for a 100 km-circumference collider (CERNCourier2021JanFeb, CERNCourier2019SepOct).\n",
      "\n",
      "4. **Economic Impact**: A report commissioned by the FCC group indicated that participation in CERN procurement had a positive impact on the long-term operating revenues and profitability of LHC suppliers. The benefits of future colliders, such as the FCC, are expected to be at least as high (CERN Courier Volume 58 Issue 2, March 2018).\n",
      "\n",
      "In summary, the FCC feasibility study is a comprehensive effort to assess the technical, economic, and scientific viability of a next-generation circular collider that could significantly advance particle physics research beyond the capabilities of the current LHC.\n",
      "Response time: 11.32 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  What does the timeline for a 10TeV muon collider look like?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: What does the timeline for a 10TeV muon collider look like?\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The timeline for a 10 TeV muon collider is not definitively established, as it depends on various factors including technological developments, funding, and international collaboration. However, the European Strategy for Particle Physics Update 2020 has identified a muon collider as a potential future project. \n",
      "\n",
      "The timeline for such a project could be divided into several phases:\n",
      "\n",
      "1. **Feasibility Studies (Ongoing)**: Continued research into the feasibility of muon colliders, including studies on cooling, acceleration, and detector technologies.\n",
      "\n",
      "2. **Conceptual Design Report (5-10 years)**: Development of a detailed conceptual design report (CDR) that outlines the technical and financial aspects of the project.\n",
      "\n",
      "3. **R&D Phase (10-15 years)**: A dedicated R&D phase to address the technical challenges identified in the CDR, including the development of muon cooling and acceleration technologies.\n",
      "\n",
      "4. **Construction (10-15 years)**: Following a successful R&D phase and securing funding, the construction of the collider could take place, potentially in the 2040s or 2050s.\n",
      "\n",
      "5. **Operation (Ongoing)**: Once constructed, the collider would enter an operational phase, with data collection and analysis continuing for many years.\n",
      "\n",
      "This is a very rough estimate and the actual timeline could vary significantly based on the progress of the R&D and the global scientific landscape.\n",
      "Response time: 3.19 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The timeline for a 10 TeV muon collider involves several stages and is subject to various technical and funding considerations. According to the articles, the panel exploring muon beams and colliders has set a target date of 2026 to demonstrate that further investment in a 10 TeV collider is justified. This includes focusing on a 3 TeV intermediate-scale facility targeted for the 2040s (CERNCourier2022JanFeb-digitaledition.pdf).\n",
      "\n",
      "The timeline for the muon collider is described as a technically limited plan, which assumes that sufficient funding is available and that there are no external constraints impacting deployment. This timeline is considered more aspirational compared to the FCC-ee timeline, which attempts to incorporate actual deployment constraints (CERNCourier2024MarApr-digitaledition.pdf).\n",
      "\n",
      "Overall, the timeline suggests that physics operations for a high-energy muon collider could begin at least a decade after construction approval, with the current schedule placing it about five years earlier than the FCC-ee. However, this remains an aspiration rather than a fixed schedule (CERNCourier2024MarApr-digitaledition.pdf).\n",
      "Response time: 3.06 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(self, fine_tuned_model_id, db_dir=\"cern_vectordb\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def query_fine_tuned_model(self, question):\n",
    "        \"\"\"Query the fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying fine-tuned model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.rag_chain.invoke(question)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from both approaches\"\"\"\n",
    "        print(\"\\nQuerying both models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        ft_result = self.query_fine_tuned_model(question)\n",
    "        rag_result = self.query_rag(question)\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Question:\", question)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nFine-tuned Model Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\nRAG System Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'fine_tuned': ft_result,\n",
    "            'rag': rag_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Replace with your fine-tuned model ID\n",
    "    FINE_TUNED_MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\"\n",
    "    \n",
    "    try:\n",
    "        comparison = ModelComparison(FINE_TUNED_MODEL_ID)\n",
    "        \n",
    "        print(\"CERN Research Assistant Comparison\")\n",
    "        print(\"Compare Fine-tuned model vs RAG approach\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7310b5d-bf94-4ab8-946b-3cbee47f82f8",
   "metadata": {},
   "source": [
    "# 6: Fine Tunging on nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f8163c-ed3a-4657-9df0-0e962b26fb74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial GPU memory usage:\n",
      "GPU 0: 380.31MB / 24576.00MB (1.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:359: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/home/lence/anaconda3/lib/python3.11/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final GPU memory usage after cleaning:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n",
      "Using GPU: NVIDIA GeForce RTX 3090\n",
      "Available GPU memory: 25.44 GB\n",
      "\n",
      "Initial GPU memory usage:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n",
      "\n",
      "Final GPU memory usage after cleaning:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n",
      "Initializing fine-tuner...\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with optimized quantization...\n",
      "\n",
      "Initial GPU memory usage:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n",
      "\n",
      "Final GPU memory usage after cleaning:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f03be55e434e1e85dd71c2fc664d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=True, split_batches=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model parameters:\n",
      "trainable params: 1050939392 || all params: 4540600320 || trainable%: 23.15\n",
      "Preparing model for QLoRA training...\n",
      "Applying LoRA...\n",
      "trainable params: 3407872 || all params: 4544008192 || trainable%: 0.07\n",
      "Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [05:19<00:00,  5.61s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07469eb1dce44b2ab89d6aa52ee8857d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:54, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlDeviceGetCount,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    NVMLError\n",
    ")\n",
    "\n",
    "def clean_gpu_memory():\n",
    "    \"\"\"\n",
    "    Thoroughly clean GPU memory and print memory usage statistics\n",
    "    \"\"\"\n",
    "    def get_gpu_memory_usage():\n",
    "        try:\n",
    "            nvmlInit()\n",
    "            deviceCount = nvmlDeviceGetCount()\n",
    "            memory_usage = []\n",
    "            \n",
    "            for i in range(deviceCount):\n",
    "                handle = nvmlDeviceGetHandleByIndex(i)\n",
    "                info = nvmlDeviceGetMemoryInfo(handle)\n",
    "                memory_usage.append({\n",
    "                    'device': i,\n",
    "                    'used_mb': info.used / 1024**2,\n",
    "                    'total_mb': info.total / 1024**2,\n",
    "                    'used_percent': (info.used / info.total) * 100\n",
    "                })\n",
    "            \n",
    "            return memory_usage\n",
    "        except NVMLError as e:\n",
    "            print(f\"NVML Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Print initial memory usage\n",
    "    memory_usage = get_gpu_memory_usage()\n",
    "    if memory_usage:\n",
    "        print(\"\\nInitial GPU memory usage:\")\n",
    "        for gpu in memory_usage:\n",
    "            print(f\"GPU {gpu['device']}: {gpu['used_mb']:.2f}MB / {gpu['total_mb']:.2f}MB ({gpu['used_percent']:.2f}%)\")\n",
    "\n",
    "    # Empty CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Delete all variables in CUDA memory\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if obj.is_cuda:\n",
    "                    del obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Run garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    # Reset peak memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Print final memory usage\n",
    "    memory_usage = get_gpu_memory_usage()\n",
    "    if memory_usage:\n",
    "        print(\"\\nFinal GPU memory usage after cleaning:\")\n",
    "        for gpu in memory_usage:\n",
    "            print(f\"GPU {gpu['device']}: {gpu['used_mb']:.2f}MB / {gpu['total_mb']:.2f}MB ({gpu['used_percent']:.2f}%)\")\n",
    "\n",
    "# Set environment variable for memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64,garbage_collection_threshold:0.8\"\n",
    "clean_gpu_memory()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "class LlamaFineTuner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        cache_dir=None,\n",
    "        pdf_dir=\"cern_pdfs\",\n",
    "        output_dir=\"llama_finetuned\",\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.logging_dir = self.output_dir / \"logs\"\n",
    "        self.logging_dir.mkdir(exist_ok=True)\n",
    "        self.device = device\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        try:\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=self.cache_dir,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            \n",
    "            print(\"Loading model with optimized quantization...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                load_in_8bit_fp32_cpu_offload=True\n",
    "            )\n",
    "            \n",
    "            # Calculate available GPU memory\n",
    "            clean_gpu_memory()  # Clean memory before loading model\n",
    "            gpu_memory = int(torch.cuda.get_device_properties(0).total_memory/1e9) - 2\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                max_memory={0: f\"{int(gpu_memory*0.7)}GB\", \"cpu\": \"32GB\"},\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            # Enable gradient checkpointing\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            self.model.enable_input_require_grads()\n",
    "            self.model.config.use_cache = False\n",
    "            \n",
    "            print(\"Initial model parameters:\")\n",
    "            print_trainable_parameters(self.model)\n",
    "            \n",
    "            print(\"Preparing model for QLoRA training...\")\n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "            \n",
    "            # Configure LoRA\n",
    "            lora_config = LoraConfig(\n",
    "                r=8,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"q_proj\", \"v_proj\"],\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM\n",
    "            )\n",
    "            \n",
    "            print(\"Applying LoRA...\")\n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "            print_trainable_parameters(self.model)\n",
    "            \n",
    "            # Initialize accelerator\n",
    "            self.accelerator = Accelerator(\n",
    "                gradient_accumulation_steps=32,\n",
    "                mixed_precision=\"bf16\",\n",
    "                project_dir=str(self.logging_dir),\n",
    "                split_batches=True,\n",
    "                dispatch_batches=True\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error initializing model and tokenizer: {str(e)}\")\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file.\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_training_data(self, max_length=512):\n",
    "        \"\"\"Prepare training data from PDF files.\"\"\"\n",
    "        if not self.pdf_dir.exists():\n",
    "            raise ValueError(f\"PDF directory not found: {self.pdf_dir}\")\n",
    "        \n",
    "        training_data = []\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            raise ValueError(f\"No PDF files found in {self.pdf_dir}\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                entry = {\n",
    "                    \"instruction\": \"Analyze and explain the following research:\",\n",
    "                    \"input\": text,\n",
    "                    \"output\": \"This research discusses: \" + text[:500]\n",
    "                }\n",
    "                training_data.append(entry)\n",
    "        \n",
    "        if not training_data:\n",
    "            raise ValueError(\"No valid training data could be extracted from PDFs\")\n",
    "        \n",
    "        dataset = Dataset.from_list(training_data)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            prompts = []\n",
    "            for inst, inp in zip(examples[\"instruction\"], examples[\"input\"]):\n",
    "                prompt = f\"[INST] {inst}\\n{inp} [/INST]\"\n",
    "                prompts.append(prompt)\n",
    "            \n",
    "            return self.tokenizer(\n",
    "                prompts,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        return dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "\n",
    "    def train(self, num_epochs=3, batch_size=1):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        try:\n",
    "            print(\"Preparing training data...\")\n",
    "            train_dataset = self.prepare_training_data()\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=str(self.output_dir),\n",
    "                num_train_epochs=num_epochs,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                gradient_accumulation_steps=32,\n",
    "                warmup_steps=50,\n",
    "                logging_steps=10,\n",
    "                save_steps=100,\n",
    "                learning_rate=2e-4,\n",
    "                bf16=True,\n",
    "                optim=\"paged_adamw_32bit\",\n",
    "                logging_dir=str(self.logging_dir),\n",
    "                group_by_length=True,\n",
    "                gradient_checkpointing=True,\n",
    "                max_grad_norm=0.3,\n",
    "                save_total_limit=2,\n",
    "                evaluation_strategy=\"no\",\n",
    "                report_to=\"tensorboard\",\n",
    "                remove_unused_columns=False,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                weight_decay=0.01\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                data_collator=DataCollatorForLanguageModeling(\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    mlm=False\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"Starting training...\")\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                trainer.train()\n",
    "            \n",
    "            print(\"Saving model...\")\n",
    "            trainer.save_model(str(self.output_dir / \"final_model\"))\n",
    "            self.tokenizer.save_pretrained(\n",
    "                str(self.output_dir / \"final_model\"),\n",
    "                safe_serialization=True\n",
    "            )\n",
    "            \n",
    "            print(\"Training complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during training: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"This script requires a CUDA-capable GPU\")\n",
    "        \n",
    "        # Print GPU info\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "        \n",
    "        # Clean GPU memory before starting\n",
    "        clean_gpu_memory()\n",
    "        \n",
    "        # Set memory optimizations\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.cuda.set_per_process_memory_fraction(0.75)\n",
    "        \n",
    "        # Create cache directory\n",
    "        cache_dir = Path.home() / \".cache\" / \"huggingface\"\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize and train\n",
    "        print(\"Initializing fine-tuner...\")\n",
    "        finetuner = LlamaFineTuner(cache_dir=str(cache_dir))\n",
    "        finetuner.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cce55d-cebc-4ef2-9741-1eae8a3d3ac2",
   "metadata": {},
   "source": [
    "# 7: Chat with the Fine-Tuned nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba6372d-e50d-491b-9876-5bcf6a3e60d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing chat model...\n",
      "Loading tokenizer...\n",
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bbc0b132e4409ab4b3f1ae02869403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned adapters...\n",
      "Model loaded successfully!\n",
      "Model is on device: cuda:0\n",
      "\n",
      "Starting chat session with the fine-tuned model\n",
      "Type 'quit' to exit, 'clear' to clear the conversation\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Debug - Prompt: [INST] hello [/INST]\n",
      "Debug - Input shape: torch.Size([1, 7])\n",
      "Debug - Output shape: torch.Size([1, 8])\n",
      "Debug - Raw response: [INST] hello [/INST]\n",
      "Debug - Cleaned response: \n",
      "\n",
      "Model: No response generated. Please try again.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  The coolest job in physics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Debug - Prompt: [INST] The coolest job in physics [/INST]\n",
      "Debug - Input shape: torch.Size([1, 11])\n",
      "Debug - Output shape: torch.Size([1, 180])\n",
      "Debug - Raw response: [INST] The coolest job in physics [/INST] So I'm making a science podcast. I'm going to talk to scientists, engineers, people in science communication, and all sorts of people about what they're doing. The first episode is coming out next week. This is the first episode of the first season. I'm going to make more. It's going to be fun. There will be other episodes. So, yes, I'm making a science podcast. The coolest job in physics. Because I'm a physics person. And I think it's going to be fun. And there will be more. And more. I hope you enjoy it. If you do, please share it. If you'd like to be on the podcast, please drop me a line. If you'd like to sponsor the podcast, please drop me a line. I will talk to you soon!\n",
      "Debug - Cleaned response: So I'm making a science podcast. I'm going to talk to scientists, engineers, people in science communication, and all sorts of people about what they're doing. The first episode is coming out next week. This is the first episode of the first season. I'm going to make more. It's going to be fun. There will be other episodes. So, yes, I'm making a science podcast. The coolest job in physics. Because I'm a physics person. And I think it's going to be fun. And there will be more. And more. I hope you enjoy it. If you do, please share it. If you'd like to be on the podcast, please drop me a line. If you'd like to sponsor the podcast, please drop me a line. I will talk to you soon!\n",
      "\n",
      "Model: So I'm making a science podcast. I'm going to talk to scientists, engineers, people in science communication, and all sorts of people about what they're doing. The first episode is coming out next week. This is the first episode of the first season. I'm going to make more. It's going to be fun. There will be other episodes. So, yes, I'm making a science podcast. The coolest job in physics. Because I'm a physics person. And I think it's going to be fun. And there will be more. And more. I hope you enjoy it. If you do, please share it. If you'd like to be on the podcast, please drop me a line. If you'd like to sponsor the podcast, please drop me a line. I will talk to you soon!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  what is The coolest job in physics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Debug - Prompt: [INST] what is The coolest job in physics [/INST]\n",
      "Debug - Input shape: torch.Size([1, 13])\n",
      "Debug - Output shape: torch.Size([1, 77])\n",
      "Debug - Raw response: [INST] what is The coolest job in physics [/INST] - a discussion of the coolest job in physics.  I'm not sure what that would be, but I would guess that most of the people here would think it is the LHC, or maybe CERN in general, or maybe being a NASA astronaut.  What do you think?\n",
      "The coolest job in physics\n",
      "Debug - Cleaned response: - a discussion of the coolest job in physics.  I'm not sure what that would be, but I would guess that most of the people here would think it is the LHC, or maybe CERN in general, or maybe being a NASA astronaut.  What do you think?\n",
      "The coolest job in physics\n",
      "\n",
      "Model: - a discussion of the coolest job in physics.  I'm not sure what that would be, but I would guess that most of the people here would think it is the LHC, or maybe CERN in general, or maybe being a NASA astronaut.  What do you think?\n",
      "The coolest job in physics\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  who is Bruce Marsh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Debug - Prompt: [INST] who is Bruce Marsh [/INST]\n",
      "Debug - Input shape: torch.Size([1, 10])\n",
      "Debug - Output shape: torch.Size([1, 241])\n",
      "Debug - Raw response: [INST] who is Bruce Marsh [/INST] [/INST] You're in luck because Bruce Marsh is a member of MyHeritage. We are the world’s largest online family history and genealogy company. We have been in business since 2003. Currently there are over 1.9 billion records available for searching. They have records from the United States, Canada, the United Kingdom, Australia, New Zeal [...]\n",
      "[BIO] Bruce Marsh was born on April 15, 1960. He died on August 31, 2016 at 56 years old. We know that Bruce Marsh had been residing in Klamath Falls, Klamath County, Oregon 97601.\n",
      "\n",
      "Every day, we help thousands of families by sharing these life stories. This memorial page was created in the loving memory of a beloved family member. By sharing a fond memory or writing a kind tribute, you will help keep their legacy alive. Every memorial page is a permanent online archive that allows loved ones and friends to leave enduring tributes. They are unique and interactive websites that are easy to set up and maintain, and can be customized to honor the person and say what you want.\n",
      "Debug - Cleaned response: [/INST] You're in luck because Bruce Marsh is a member of MyHeritage. We are the world’s largest online family history and genealogy company. We have been in business since 2003. Currently there are over 1.9 billion records available for searching. They have records from the United States, Canada, the United Kingdom, Australia, New Zeal [...]\n",
      "[BIO] Bruce Marsh was born on April 15, 1960. He died on August 31, 2016 at 56 years old. We know that Bruce Marsh had been residing in Klamath Falls, Klamath County, Oregon 97601.\n",
      "\n",
      "Every day, we help thousands of families by sharing these life stories. This memorial page was created in the loving memory of a beloved family member. By sharing a fond memory or writing a kind tribute, you will help keep their legacy alive. Every memorial page is a permanent online archive that allows loved ones and friends to leave enduring tributes. They are unique and interactive websites that are easy to set up and maintain, and can be customized to honor the person and say what you want.\n",
      "\n",
      "Model: [/INST] You're in luck because Bruce Marsh is a member of MyHeritage. We are the world’s largest online family history and genealogy company. We have been in business since 2003. Currently there are over 1.9 billion records available for searching. They have records from the United States, Canada, the United Kingdom, Australia, New Zeal [...]\n",
      "[BIO] Bruce Marsh was born on April 15, 1960. He died on August 31, 2016 at 56 years old. We know that Bruce Marsh had been residing in Klamath Falls, Klamath County, Oregon 97601.\n",
      "\n",
      "Every day, we help thousands of families by sharing these life stories. This memorial page was created in the loving memory of a beloved family member. By sharing a fond memory or writing a kind tribute, you will help keep their legacy alive. Every memorial page is a permanent online archive that allows loved ones and friends to leave enduring tributes. They are unique and interactive websites that are easy to set up and maintain, and can be customized to honor the person and say what you want.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Bruce Marsh A specialist in laser technology at CERN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Debug - Prompt: [INST] Bruce Marsh A specialist in laser technology at CERN [/INST]\n",
      "Debug - Input shape: torch.Size([1, 16])\n",
      "Debug - Output shape: torch.Size([1, 272])\n",
      "Debug - Raw response: [INST] Bruce Marsh A specialist in laser technology at CERN [/INST] [INST] John Raby A specialist in the design of accelerators at CERN [/INST] [INST] James Scargill A specialist in the design of accelerators at CERN [/INST] [INST] John Ellis A physicist at CERN and Cambridge University, England. He is a member of the High Energy Physics Group of Cambridge University and has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions. He is a Fellow of the Royal Society of London and of the Institute of Physics. He is also an Honorary Fellow of the Institute of Physics and a Fellow of the Royal Society of Edinburgh. He has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions. He is a Fellow of the Royal Society of London and of the Institute of Physics. He is also an Honorary Fellow of the Institute of Physics and a Fellow of the Royal Society of Edinburgh. He has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions.\n",
      "Debug - Cleaned response: [INST] John Raby A specialist in the design of accelerators at CERN [/INST] [INST] James Scargill A specialist in the design of accelerators at CERN [/INST] [INST] John Ellis A physicist at CERN and Cambridge University, England. He is a member of the High Energy Physics Group of Cambridge University and has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions. He is a Fellow of the Royal Society of London and of the Institute of Physics. He is also an Honorary Fellow of the Institute of Physics and a Fellow of the Royal Society of Edinburgh. He has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions. He is a Fellow of the Royal Society of London and of the Institute of Physics. He is also an Honorary Fellow of the Institute of Physics and a Fellow of the Royal Society of Edinburgh. He has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions.\n",
      "\n",
      "Model: [INST] John Raby A specialist in the design of accelerators at CERN [/INST] [INST] James Scargill A specialist in the design of accelerators at CERN [/INST] [INST] John Ellis A physicist at CERN and Cambridge University, England. He is a member of the High Energy Physics Group of Cambridge University and has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions. He is a Fellow of the Royal Society of London and of the Institute of Physics. He is also an Honorary Fellow of the Institute of Physics and a Fellow of the Royal Society of Edinburgh. He has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions. He is a Fellow of the Royal Society of London and of the Institute of Physics. He is also an Honorary Fellow of the Institute of Physics and a Fellow of the Royal Society of Edinburgh. He has been a member of the CERN Theory Group since 1971. He is the author of more than 100 papers on the theory of elementary particles and their interactions.\n",
      "\n",
      "\n",
      "Interrupted by user. Type 'quit' to exit or continue chatting.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Goodbye!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelChat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        finetuned_path=\"llama_finetuned/final_model\",\n",
    "        max_sequence_length=2048\n",
    "    ):\n",
    "        try:\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                base_model_name,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            # Set padding parameters\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            self.max_sequence_length = max_sequence_length\n",
    "            \n",
    "            print(\"Loading base model...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            print(\"Loading fine-tuned adapters...\")\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                finetuned_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            self.model.eval()\n",
    "            print(\"Model loaded successfully!\")\n",
    "            print(f\"Model is on device: {self.model.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error initializing model: {str(e)}\")\n",
    "    \n",
    "    def generate_response(self, instruction, max_new_tokens=256, temperature=0.7):\n",
    "        try:\n",
    "            # Format the input\n",
    "            prompt = f\"[INST] {instruction} [/INST]\"\n",
    "            print(f\"\\nDebug - Prompt: {prompt}\")\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_sequence_length\n",
    "            )\n",
    "            print(f\"Debug - Input shape: {inputs.input_ids.shape}\")\n",
    "            \n",
    "            # Move to GPU\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate with minimal parameters first\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            print(f\"Debug - Output shape: {outputs.shape}\")\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"Debug - Raw response: {response}\")\n",
    "            \n",
    "            # Clean response\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "            print(f\"Debug - Cleaned response: {response}\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Debug - Error in generate_response: {str(e)}\")\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def chat(self):\n",
    "        print(\"\\nStarting chat session with the fine-tuned model\")\n",
    "        print(\"Type 'quit' to exit, 'clear' to clear the conversation\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                    print(\"\\nGoodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\nGenerating response...\")\n",
    "                response = self.generate_response(user_input)\n",
    "                \n",
    "                if response:\n",
    "                    print(f\"\\nModel: {response}\")\n",
    "                else:\n",
    "                    print(\"\\nModel: No response generated. Please try again.\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nInterrupted by user. Type 'quit' to exit or continue chatting.\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during chat: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"This script requires a CUDA-capable GPU\")\n",
    "        \n",
    "        print(\"\\nInitializing chat model...\")\n",
    "        chat_model = ModelChat()\n",
    "        chat_model.chat()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b410781-1b8c-4997-9697-18a7c1241e78",
   "metadata": {},
   "source": [
    "# 8: Now compaire OpenAI RAG vs OpenAI Fine-Tune vs nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec084a0c-6090-4200-bba7-9b6f9e818da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model Comparison System...\n",
      "Initializing RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_384348/2066110667.py:37: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c5b9b3ad294127a4ac64661a699f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA model loaded successfully!\n",
      "\n",
      "CERN Research Assistant - Model Comparison\n",
      "Compare: RAG vs Fine-tuned GPT-4 vs Fine-tuned LLaMA\n",
      "Type 'quit' to exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  give me an overview of what CERN is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your question across all models...\n",
      "\n",
      "================================================================================\n",
      "Question: give me an overview of what CERN is\n",
      "================================================================================\n",
      "\n",
      "1. RAG System (GPT-4 + CERN Articles)\n",
      "--------------------------------------------------\n",
      "The articles provided do not contain information on what CERN is.\n",
      "Response time: 2.41 seconds\n",
      "\n",
      "2. Fine-tuned GPT-4\n",
      "--------------------------------------------------\n",
      "The European Organization for Nuclear Research, known as CERN (from the French \"Conseil Européen pour la Recherche Nucléaire\"), is one of the world's largest and most respected centers for scientific research in particle physics. Established in 1954, CERN is located on the border between France and Switzerland, near Geneva. Its main purpose is to provide the particle accelerators and other infrastructure needed for high-energy physics research.\n",
      "\n",
      "### Key Features of CERN:\n",
      "\n",
      "1. **Research Facilities**: CERN is famous for its large particle accelerators, including the Large Hadron Collider (LHC), which is the world's largest and most powerful particle accelerator. The LHC is used to collide protons and heavy ions at unprecedented energy levels, allowing scientists to investigate fundamental questions about the nature of matter, the forces of the universe, and the origins of mass.\n",
      "\n",
      "2. **Scientific Discoveries**: CERN has been the site of numerous groundbreaking discoveries, most notably the discovery of the Higgs boson in 2012, which confirmed the existence of the Higgs field, a fundamental field that gives mass to elementary particles.\n",
      "\n",
      "3. **International Collaboration**: CERN is an intergovernmental organization with 23 member states and several observer states. It has a diverse and multinational workforce, with thousands of scientists, engineers, and technicians from around the world working on various experiments and projects.\n",
      "\n",
      "4. **Scientific Community**: CERN hosts a large number of experiments and collaborations, involving thousands of researchers from universities and laboratories worldwide. The data produced by the LHC and other CERN facilities are shared with the global scientific community.\n",
      "\n",
      "5. **Technological Innovations**: CERN has contributed to numerous technological advancements, including the development of the World Wide Web, advancements in medical imaging technologies, and new materials and techniques in various fields.\n",
      "\n",
      "6. **Education and Outreach**: CERN places a strong emphasis on education and outreach, offering programs for students, teachers, and the general public to learn about particle physics and its implications for our understanding of the universe.\n",
      "\n",
      "7. **Future Projects**: CERN is engaged in planning future projects, such as the High-Luminosity Large Hadron Collider (HL-LHC), which aims to increase the luminosity of the LHC to explore rare processes and make precision measurements of known particles.\n",
      "\n",
      "Overall, CERN plays a pivotal role in advancing our understanding of fundamental physics and the universe, fostering international collaboration, and driving technological innovation.\n",
      "Response time: 9.38 seconds\n",
      "\n",
      "3. Fine-tuned LLaMA (with LoRA)\n",
      "--------------------------------------------------\n",
      "def cern_overview():\n",
      "    \"\"\"\n",
      "    Overview of the European Organization for Nuclear Research (CERN) and its activities in particle physics, including the Large Hadron Collider (LHC).\n",
      "    \"\"\"\n",
      "\n",
      "    # Information about CERN\n",
      "    print(\"The European Organization for Nuclear Research (CERN) is a research organization based in Switzerland.\")\n",
      "    print(\"It specializes in fundamental research in the field of particle physics.\")\n",
      "\n",
      "    # Activities at CERN\n",
      "    print(\"\\nParticle physicists from all over the world come to CERN to use its facilities to conduct experiments.\")\n",
      "    print(\"One such facility is the Large Hadron Collider (LHC), which is used to collide protons together at extremely high energies.\")\n",
      "\n",
      "    # Discoveries made at CERN\n",
      "    print(\"\\nThe LHC has led to several important discoveries in particle physics.\")\n",
      "    print(\"For example, it was used to confirm the existence of the Higgs boson, a key component of the Standard Model of particle physics.\")\n",
      "\n",
      "\n",
      "# Call the function to display the overview\n",
      "cern_overview()\n",
      "Response time: 9.09 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fine_tuned_model_id=\"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\",\n",
    "        base_model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        finetuned_path=\"llama_finetuned/final_model\",\n",
    "        db_dir=\"cern_vectordb\"\n",
    "    ):\n",
    "        load_dotenv()\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        print(\"Initializing RAG system...\")\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Setup RAG prompt template\n",
    "        template = \"\"\"You are a CERN research assistant. Use the following articles to answer the question.\n",
    "        If you cannot answer based on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer with specific references to the articles:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Setup RAG chain\n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # Initialize LLaMA model\n",
    "        print(\"Loading LLaMA model...\")\n",
    "        try:\n",
    "            # Initialize tokenizer\n",
    "            self.llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                base_model_name,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            if self.llama_tokenizer.pad_token is None:\n",
    "                self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token\n",
    "            \n",
    "            # Setup quantization\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            # Load base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Load LoRA adapters\n",
    "            self.llama_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                finetuned_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            self.llama_model.eval()\n",
    "            print(\"LLaMA model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load LLaMA model: {e}\")\n",
    "            self.llama_model = None\n",
    "            self.llama_tokenizer = None\n",
    "\n",
    "    def query_llama(self, question, max_length=512):\n",
    "        \"\"\"Query the LoRA-fine-tuned LLaMA model\"\"\"\n",
    "        if not self.llama_model or not self.llama_tokenizer:\n",
    "            return {\n",
    "                'response': \"LLaMA model not available\",\n",
    "                'time': 0\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Prepare input\n",
    "            prompt = f\"[INST] {question} [/INST]\"\n",
    "            inputs = self.llama_tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=True\n",
    "            ).to(self.llama_model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.llama_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95,\n",
    "                    repetition_penalty=1.1,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.llama_tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.llama_tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying LLaMA model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_fine_tuned_gpt(self, question):\n",
    "        \"\"\"Query the OpenAI fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics research.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying fine-tuned GPT: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.rag_chain.invoke(question)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from all three models\"\"\"\n",
    "        print(\"\\nProcessing your question across all models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        rag_result = self.query_rag(question)\n",
    "        ft_result = self.query_fine_tuned_gpt(question)\n",
    "        llama_result = self.query_llama(question)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\n1. RAG System (GPT-4 + CERN Articles)\")\n",
    "        print(\"-\"*50)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\n2. Fine-tuned GPT-4\")\n",
    "        print(\"-\"*50)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\n3. Fine-tuned LLaMA (with LoRA)\")\n",
    "        print(\"-\"*50)\n",
    "        print(llama_result['response'])\n",
    "        print(f\"Response time: {llama_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'rag': rag_result,\n",
    "            'fine_tuned_gpt': ft_result,\n",
    "            'llama': llama_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Initializing Model Comparison System...\")\n",
    "        comparison = ModelComparison()\n",
    "        \n",
    "        print(\"\\nCERN Research Assistant - Model Comparison\")\n",
    "        print(\"Compare: RAG vs Fine-tuned GPT-4 vs Fine-tuned LLaMA\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \").strip()\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            if not question:\n",
    "                continue\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11f248-26f8-4529-92e2-f6aa8830c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "class RAGQuerySystem:\n",
    "    def __init__(self, embeddings_path: str, openai_model: str = \"gpt-4o\"):\n",
    "        # Load environment variables\n",
    "        dotenv.load_dotenv()\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = openai_model\n",
    "        \n",
    "        # Load embeddings and documents\n",
    "        data = np.load(embeddings_path, allow_pickle=True)\n",
    "        self.embeddings = data['encodings']\n",
    "        self.chunks = data['chunks']\n",
    "        self.metadata = data['metadata']\n",
    "        \n",
    "        # Initialize the same embedding model used for document encoding\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        # Get query embedding using the same model as document encoding\n",
    "        query_embedding = self.embedding_model.encode(question)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = np.dot(self.embeddings, query_embedding) / (\n",
    "            norm(self.embeddings, axis=1) * norm(query_embedding)\n",
    "        )\n",
    "        \n",
    "        # Get top 3 similar chunks\n",
    "        top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "        \n",
    "        # Prepare context from similar chunks\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Document: {self.metadata[idx]}]\\n{self.chunks[idx]}\"\n",
    "            for idx in top_indices\n",
    "        ])\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Using the following CERN research documents as context, answer the question. \n",
    "        If you cannot answer from the context, say so.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "        # Get response from OpenAI\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful physics research assistant specializing in CERN experiments and findings.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    # Initialize RAG system\n",
    "    rag = RAGQuerySystem('document_encodings.npz')\n",
    "    \n",
    "    # Ask about W boson mass\n",
    "    question = \"Based on the latest data inputs, the Standard Model (SM) constrains the mass of the W boson (mW) to be?\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nQuestion:\", question)\n",
    "        answer = rag.query(question)\n",
    "        print(\"\\nAnswer:\", answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b2ec7-6a24-4088-96c1-7ef8bec787b6",
   "metadata": {},
   "source": [
    "## Anything that you ever wnated to know about Cern.  Review some of the QA from below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582f0656",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'document_encodings.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:94\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:76\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:20\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, embeddings_path, openai_model)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'document_encodings.npz'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class RAGQuerySystem:\n",
    "    def __init__(self, embeddings_path: str, openai_model: str = \"gpt-4o\"):\n",
    "        # Load environment variables\n",
    "        dotenv.load_dotenv()\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = openai_model\n",
    "        \n",
    "        # Load embeddings and documents\n",
    "        data = np.load(embeddings_path, allow_pickle=True)\n",
    "        self.embeddings = data['encodings']\n",
    "        self.chunks = data['chunks']\n",
    "        self.metadata = data['metadata']\n",
    "        \n",
    "        # Initialize the same embedding model used for document encoding\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def find_similar_chunks(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[int, float]]:\n",
    "        # Calculate cosine similarity\n",
    "        similarities = np.dot(self.embeddings, query_embedding) / (\n",
    "            norm(self.embeddings, axis=1) * norm(query_embedding)\n",
    "        )\n",
    "        \n",
    "        # Get top k similar chunks\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(i, similarities[i]) for i in top_indices]\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        # Get query embedding using the same model as document encoding\n",
    "        query_embedding = self.embedding_model.encode(question)\n",
    "        \n",
    "        # Find relevant chunks\n",
    "        similar_chunks = self.find_similar_chunks(query_embedding)\n",
    "        \n",
    "        # Prepare context from similar chunks\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Document: {self.metadata[idx]}]\\n{self.chunks[idx]}\"\n",
    "            for idx, _ in similar_chunks\n",
    "        ])\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Using the following CERN research documents as context, answer the question. \n",
    "        If you cannot answer from the context, say so.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "        # Get response from OpenAI\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful physics research assistant specializing in CERN experiments and findings.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    rag = RAGQuerySystem('document_encodings.npz')\n",
    "    \n",
    "    print(\"CERN Research Query System (type 'quit' to exit)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nWhat would you like to know about CERN research? \").strip()\n",
    "        \n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            answer = rag.query(question)\n",
    "            print(\"\\nAnswer:\", answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cabde4-4ed3-448a-b8b3-1fe90721d28b",
   "metadata": {},
   "source": [
    "# RAG System Analysis: Improving Answer Quality with Document Grounding\n",
    "## A Comparison of Direct LLM vs. RAG-Enhanced Responses\n",
    "\n",
    "## Introduction\n",
    "This notebook analyzes the effectiveness of using Retrieval-Augmented Generation (RAG) for querying CERN research data compared to direct LLM queries. Our implementation processes CERN Courier documents to ground responses in authoritative sources.\n",
    "\n",
    "## System Architecture\n",
    "### Document Processing Pipeline\n",
    "```python\n",
    "# Key statistics from our implementation\n",
    "total_pdfs_processed = 96\n",
    "total_chunks_processed = 22103\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "chunk_size = 1000\n",
    "```\n",
    "\n",
    "Our system processes PDFs through several stages:\n",
    "1. Document Collection: Downloads CERN Courier PDFs\n",
    "2. Text Extraction: Converts PDFs to processable text\n",
    "3. Chunking: Splits text into manageable segments\n",
    "4. Embedding Generation: Creates vector representations\n",
    "5. Indexing: Organizes embeddings for efficient retrieval\n",
    "\n",
    "## Comparative Analysis\n",
    "\n",
    "### Case Study: W Boson Mass Query\n",
    "\n",
    "#### Direct OpenAI Query\n",
    "```python\n",
    "question = \"Based on the latest data inputs, what does the Standard Model (SM) constrain the mass of the W boson (mW) to be?\"\n",
    "\n",
    "# Direct OpenAI response (truncated):\n",
    "\"\"\"\n",
    "As of my last update in 2023, within the framework of the Standard Model of particle physics, \n",
    "precise calculations constrain the mass of the W boson (mW) based on various experimental \n",
    "inputs and theoretical considerations...\n",
    "\n",
    "Before 2022, the Particle Data Group (PDG) reported a world average...\n",
    "CDF collaboration announced a new measurement...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### RAG System Query\n",
    "```python\n",
    "# RAG system response:\n",
    "\"\"\"\n",
    "80357 ± 6 MeV\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Key Improvements Analysis\n",
    "\n",
    "#### 1. Answer Precision\n",
    "- **Direct LLM**: \n",
    "  - Provides general context\n",
    "  - Includes historical data\n",
    "  - No specific current value\n",
    "  \n",
    "- **RAG System**:\n",
    "  - Exact measurement with uncertainty\n",
    "  - Current CERN-sourced value\n",
    "  - No extraneous information\n",
    "\n",
    "#### 2. Response Time\n",
    "```python\n",
    "# Average response times\n",
    "direct_llm_time = \"3.2 seconds\"\n",
    "rag_system_time = \"5.1 seconds\"  # Includes retrieval overhead\n",
    "```\n",
    "\n",
    "#### 3. Source Reliability\n",
    "```python\n",
    "# RAG system source tracking\n",
    "source_metadata = {\n",
    "    \"document_type\": \"CERN Courier\",\n",
    "    \"total_sources\": 96,\n",
    "    \"date_range\": \"Last 11 years\",\n",
    "    \"verification\": \"Direct CERN publications\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Technical Implementation Details\n",
    "\n",
    "### Embedding Generation\n",
    "```python\n",
    "class DocumentEncoder:\n",
    "    def __init__(self, batch_size=256):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def encode_documents(self, pdf_directory):\n",
    "        # Process and encode documents\n",
    "        # Returns: embeddings array of shape (n_chunks, 384)\n",
    "```\n",
    "\n",
    "### Query Processing\n",
    "```python\n",
    "class RAGQuerySystem:\n",
    "    def query(self, question: str) -> str:\n",
    "        # 1. Encode question\n",
    "        # 2. Find similar chunks\n",
    "        # 3. Prepare context\n",
    "        # 4. Generate answer\n",
    "        return answer\n",
    "```\n",
    "\n",
    "## Results Summary\n",
    "\n",
    "### Performance Metrics\n",
    "1. **Accuracy**:\n",
    "   - RAG responses grounded in CERN documentation\n",
    "   - Specific numerical values vs. general ranges\n",
    "   - Reduced hallucination risk\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Fast response times (~5s)\n",
    "   - Focused, relevant answers\n",
    "   - Direct access to technical details\n",
    "\n",
    "3. **Source Attribution**:\n",
    "   - All answers traceable to CERN documents\n",
    "   - Metadata preserved for verification\n",
    "   - Recent publications ensure currency\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "The RAG system demonstrates significant advantages over direct LLM queries:\n",
    "1. Higher precision in technical answers\n",
    "2. Direct grounding in authoritative sources\n",
    "3. Clear provenance for all information\n",
    "4. Reduced tendency for hallucination or generalization\n",
    "\n",
    "### Future Improvements\n",
    "1. Implement caching for faster responses\n",
    "2. Add source citation in responses\n",
    "3. Expand document collection\n",
    "4. Optimize chunk size for better context\n",
    "\n",
    "## Usage Example\n",
    "```python\n",
    "# Initialize RAG system\n",
    "rag = RAGQuerySystem('document_encodings.npz')\n",
    "\n",
    "# Example query\n",
    "question = \"How many people work for CERN?\"\n",
    "answer = rag.query(question)\n",
    "print(f\"Answer: {answer}\")  # Output: \"CERN employs around 2600 staff members.\"\n",
    "```\n",
    "\n",
    "This implementation shows how combining retrieval with generation can significantly improve the quality and reliability of answers in specialized technical domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59127029-07f4-41d3-a70f-872c154416ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
