{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "## CERN is celebrating its 70th anniversary \n",
    "During a crucial period for high-energy physics, coinciding with the initiation of the third update to the European strategy for particle physics. In this special edition of CERN Courier magazine, early-career researchers share their visions for the future of the field while reflecting on CERN's scientific and societal contributions. The magazine features expert insights into the achievements of the Large Hadron Collider (LHC) and explores the advancements of the hybrid pixel detector technology, emphasizing its applications beyond particle physics.\n",
    "\n",
    "## The CERN Courier website \n",
    "is a rich repository of articles covering a wide array of topics in particle physics, high-energy physics, and associated technological advancements. It provides in-depth reporting on the latest experimental results from CERN and other international laboratories, offering insights into ongoing research and discoveries in the field.\n",
    "\n",
    "## Last 11 years of the CERN Courier Magazine in PDF\n",
    "In this dataset I am downloading the Last 11 years of the CERN Courier Magazine.  I will then take this database and then encode it to be used as a Context Window to ask Questions to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "# 1: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b0f1c-a3f7-47cc-9c3e-546e7d19754f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin, unquote\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "class CERNPDFCrawler:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://home.cern/resources\"\n",
    "        self.session = requests.Session()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.download_folder = \"cern_pdfs\"\n",
    "        self.processed_article_urls = set()\n",
    "        self.downloaded_files = set()\n",
    "        \n",
    "        if not os.path.exists(self.download_folder):\n",
    "            os.makedirs(self.download_folder)\n",
    "        self.load_existing_files()\n",
    "\n",
    "    def load_existing_files(self):\n",
    "        for filename in os.listdir(self.download_folder):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                self.downloaded_files.add(filename)\n",
    "        print(f\"Found {len(self.downloaded_files)} existing PDF files\")\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "                    return None\n",
    "                time.sleep(2 ** attempt)\n",
    "        return None\n",
    "\n",
    "    def extract_pdf_urls_from_text(self, text):\n",
    "        \"\"\"Extract PDF URLs from text content including 'File path:' patterns\"\"\"\n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Look for \"File path:\" pattern\n",
    "        file_path_matches = re.finditer(r'File path:\\s*(https?://[^\\s<>\"]+\\.pdf)', text, re.IGNORECASE)\n",
    "        for match in file_path_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        # Look for direct PDF links\n",
    "        pdf_link_matches = re.finditer(r'href=\"(https?://[^\\s<>\"]+\\.pdf)\"', text, re.IGNORECASE)\n",
    "        for match in pdf_link_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        return pdf_urls\n",
    "\n",
    "    def find_courier_links(self, page_url):\n",
    "        content = self.get_page_content(page_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        courier_links = []\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/resources/courier/' in href or '/record/' in href:\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                if full_url not in self.processed_article_urls:\n",
    "                    courier_links.append(full_url)\n",
    "                    self.processed_article_urls.add(full_url)\n",
    "        \n",
    "        return courier_links\n",
    "\n",
    "    def find_pdf_links(self, article_url):\n",
    "        \"\"\"Find all PDF download links on an article page\"\"\"\n",
    "        content = self.get_page_content(article_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Extract URLs from text content\n",
    "        pdf_urls.update(self.extract_pdf_urls_from_text(content))\n",
    "        \n",
    "        # Parse with BeautifulSoup for structured extraction\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Look for links containing PDF\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.lower().endswith('.pdf'):\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                pdf_urls.add(full_url)\n",
    "        \n",
    "        return list(pdf_urls)\n",
    "\n",
    "    def sanitize_filename(self, url):\n",
    "        \"\"\"Create a safe filename from URL\"\"\"\n",
    "        filename = unquote(url.split('/')[-1])\n",
    "        # Remove or replace unsafe characters\n",
    "        filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "        return filename\n",
    "\n",
    "    def download_pdf(self, pdf_url, filename):\n",
    "        if filename in self.downloaded_files:\n",
    "            print(f\"Skipping {filename} - already downloaded\")\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            response = self.session.get(pdf_url, headers=self.headers, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            file_path = os.path.join(self.download_folder, filename)\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(file_path, 'wb') as file, tqdm(\n",
    "                desc=filename,\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = file.write(data)\n",
    "                    pbar.update(size)\n",
    "            \n",
    "            self.downloaded_files.add(filename)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def crawl_and_download(self, start_page=0, end_page=7):\n",
    "        print(f\"Starting CERN PDF crawler (pages {start_page} to {end_page})\")\n",
    "        \n",
    "        found_pdfs = 0\n",
    "        downloaded_pdfs = 0\n",
    "        skipped_pdfs = 0\n",
    "        failed_downloads = []\n",
    "        \n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = f\"{self.base_url}?type=52&page={page_num}\"\n",
    "            print(f\"\\nProcessing page {page_num}...\")\n",
    "            \n",
    "            courier_links = self.find_courier_links(page_url)\n",
    "            print(f\"Found {len(courier_links)} new article links on page {page_num}\")\n",
    "            \n",
    "            for article_url in courier_links:\n",
    "                pdf_urls = self.find_pdf_links(article_url)\n",
    "                \n",
    "                for pdf_url in pdf_urls:\n",
    "                    found_pdfs += 1\n",
    "                    filename = self.sanitize_filename(pdf_url)\n",
    "                    \n",
    "                    print(f\"\\nFound PDF: {filename}\")\n",
    "                    print(f\"URL: {pdf_url}\")\n",
    "                    \n",
    "                    if filename in self.downloaded_files:\n",
    "                        print(f\"Skipping - already downloaded\")\n",
    "                        skipped_pdfs += 1\n",
    "                        continue\n",
    "                        \n",
    "                    if self.download_pdf(pdf_url, filename):\n",
    "                        downloaded_pdfs += 1\n",
    "                    else:\n",
    "                        failed_downloads.append(filename)\n",
    "                \n",
    "                time.sleep(1)\n",
    "        \n",
    "        print(\"\\nDownload Summary:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Total PDFs found: {found_pdfs}\")\n",
    "        print(f\"Successfully downloaded: {downloaded_pdfs}\")\n",
    "        print(f\"Skipped (already downloaded): {skipped_pdfs}\")\n",
    "        print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "        if failed_downloads:\n",
    "            print(\"\\nFailed downloads:\")\n",
    "            for fail in failed_downloads:\n",
    "                print(f\"- {fail}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = CERNPDFCrawler()\n",
    "    crawler.crawl_and_download(0, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78865c",
   "metadata": {},
   "source": [
    "# 2: Encode the CERN PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93cde4-96fd-47f5-b862-93900a21ae9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.db_dir = Path(db_dir)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                \n",
    "                # Extract text from each page\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        \"\"\"Process all PDFs in the directory and return chunks with metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        # Process each PDF file\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Split text into chunks\n",
    "                chunks = self.text_splitter.create_documents(\n",
    "                    texts=[text],\n",
    "                    metadatas=[{\"source\": pdf_path.name}]\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def create_vector_db(self):\n",
    "        \"\"\"Create and populate the vector database\"\"\"\n",
    "        # Get text chunks\n",
    "        chunks = self.process_pdfs()\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"No text chunks were created. Check the PDF processing.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nCreating vector database with {len(chunks)} chunks...\")\n",
    "        \n",
    "        # Create and persist the vector store\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=str(self.db_dir)\n",
    "        )\n",
    "        \n",
    "        # Persist the database\n",
    "        vectordb.persist()\n",
    "        print(f\"Vector database created and saved to {self.db_dir}\")\n",
    "        \n",
    "        return vectordb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        processor = PDFProcessor()\n",
    "        vectordb = processor.create_vector_db()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd11e0-aa06-440b-af5d-d3835d32be28",
   "metadata": {},
   "source": [
    "# 3: Chat with Cern Magazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6facc867-bba9-44a6-9c78-2f199b187dfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:20: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN Research Assistant Ready!\n",
      "Ask questions about CERN research (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  What is driving the accelerated expansion of the Universe?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The accelerated expansion of the universe is one of the most profound discoveries in cosmology. This phenomenon is primarily attributed to what is broadly referred to as dark energy, although its exact nature and physics remain unknown. The concept of dark energy emerged from observations that contradicted the then-existing understanding of the universe's expansion. Specifically, in 1998, researchers found that, contrary to expectations that the universe's expansion was slowing down, it was actually speeding up. This conclusion was reached through comparisons of the expansion rate of the universe over time, significantly strengthened by combined measurements with those of the High-z Supernova Search Team (CERNCourier2019SepOct-digitaledition.pdf, start_index: 186634).\n",
      "\n",
      "One early attempt to explain the universe's dynamics involved the cosmological constant, initially introduced by Einstein to allow for a static universe within the framework of general relativity. This cosmological constant is linked to the concept of vacuum energy. However, calculations of the vacuum energy, summing the contributions from presumed quantum states in the universe, produce a value for the expansion rate that is about 120 orders of magnitude higher than what is observed. Such a rate would have prevented the formation of galaxies, stars, and planets, indicating that vacuum energy in its straightforward interpretation cannot be the final answer (CERNCourier2019SepOct-digitaledition.pdf, start_index: 188182).\n",
      "\n",
      "The discovery of the universe's accelerated expansion also offered a solution to the so-called \"age crisis,\" where some stars appeared older than the universe itself. This discrepancy suggested either an overestimation of stellar ages or an incorrect understanding of the universe's age and expansion rate. The concept of accelerated expansion helped reconcile these differences by implying that previous models of the universe's expansion were incomplete or inaccurate (CERNCourier2019SepOct-digitaledition.pdf, start_index: 187594).\n",
      "\n",
      "The nature of dark energy remains one of the most significant puzzles in modern fundamental physics. It is the primary driver of the dynamical evolution of the universe in the current epoch. Theories and experimental programs have been developed to understand dark energy, whether it is due to a cosmological constant, a new dynamical field, a deviation from general relativity on cosmological scales, or an entirely different mechanism. The current lambda-cold-dark-matter (ΛCDM) model incorporates dark energy but does not yet provide a complete explanation of its nature. Physicists and astronomers believe that uncovering the nature of dark energy could lead to a revolution in physics, given its fundamental role in the universe's expansion (CERN Courier Volume 57 Issue 4 .pdf, start_index: 58211).\n",
      "\n",
      "In summary, the accelerated expansion of the universe is driven by dark energy, a mysterious force whose exact nature is still under investigation. Despite significant progress in understanding the universe's expansion through observations and theoretical models, dark energy remains a central unanswered question in cosmology.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 317 ms, total: 1.96 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class CERNResearchAssistant:\n",
    "    def __init__(self, db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        # Initialize the vector store\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        # Initialize the language model\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Create the retriever\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Setup the prompt template\n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Setup the RAG chain\n",
    "        self.chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Ask a question about CERN research\"\"\"\n",
    "        try:\n",
    "            response = self.chain.invoke(question)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {e}\"\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize the assistant\n",
    "        assistant = CERNResearchAssistant()\n",
    "        \n",
    "        print(\"CERN Research Assistant Ready!\")\n",
    "        print(\"Ask questions about CERN research (type 'quit' to exit)\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            response = assistant.query(question)\n",
    "            print(\"\\nAssistant:\", response)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1f129-fdef-4725-a154-1bffd26c5b2b",
   "metadata": {},
   "source": [
    "# 4: Fine Tuning with OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6a4e76-cbb5-43da-92c4-e78531886a02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   2%|██▎                                                                                                                                 | 1/57 [00:05<04:56,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2024MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|████▋                                                                                                                               | 2/57 [00:10<04:35,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2022NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   5%|██████▉                                                                                                                             | 3/57 [00:13<03:49,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier Volume 57 Issue 8 October 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|█████████▎                                                                                                                          | 4/57 [00:17<03:45,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2023MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   9%|███████████▌                                                                                                                        | 5/57 [00:24<04:30,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2018Sep-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█████████████▉                                                                                                                      | 6/57 [00:28<04:06,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2019NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  12%|████████████████▏                                                                                                                   | 7/57 [00:31<03:32,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 57 Issue 3 April 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|██████████████████▌                                                                                                                 | 8/57 [00:40<04:31,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier September 2016 (Volume 56 Issue 7).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  16%|████████████████████▊                                                                                                               | 9/57 [00:46<04:33,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERN Courier Volume 57 Issue 6 (July-August 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|██████████████████████▉                                                                                                            | 10/57 [00:50<04:07,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2021MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  19%|█████████████████████████▎                                                                                                         | 11/57 [00:57<04:28,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|███████████████████████████▌                                                                                                       | 12/57 [01:01<03:55,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2022JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  23%|█████████████████████████████▉                                                                                                     | 13/57 [01:05<03:39,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 57 Issue 9 (November 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|████████████████████████████████▏                                                                                                  | 14/57 [01:10<03:29,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 14 chunks from CERN Courier May 2018 (Volume 53 Issue 4).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  26%|██████████████████████████████████▍                                                                                                | 15/57 [01:15<03:28,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERN Courier Volume 57 Issue 2 March 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  28%|████████████████████████████████████▊                                                                                              | 16/57 [01:21<03:34,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier June 2018 (Volume 53 Issue 5).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  30%|███████████████████████████████████████                                                                                            | 17/57 [01:26<03:21,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2020NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|█████████████████████████████████████████▎                                                                                         | 18/57 [01:38<04:47,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERN Courier July-August 2016 (Volume 56 Issue 6).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  33%|███████████████████████████████████████████▋                                                                                       | 19/57 [01:42<03:55,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2020JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  35%|█████████████████████████████████████████████▉                                                                                     | 20/57 [01:47<03:43,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2022SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  37%|████████████████████████████████████████████████▎                                                                                  | 21/57 [01:52<03:23,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CCJulAug19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|██████████████████████████████████████████████████▌                                                                                | 22/57 [01:58<03:20,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  40%|████████████████████████████████████████████████████▊                                                                              | 23/57 [02:06<03:37,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  42%|███████████████████████████████████████████████████████▏                                                                           | 24/57 [02:18<04:23,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2021JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|█████████████████████████████████████████████████████████▍                                                                         | 25/57 [02:23<03:54,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 26 chunks from CERNCourier2020MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|███████████████████████████████████████████████████████████▊                                                                       | 26/57 [02:30<03:35,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier Volume 57 Issue 4 .pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  47%|██████████████████████████████████████████████████████████████                                                                     | 27/57 [02:41<04:06,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier October 2016 (Volume 56 Issue 8).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  49%|████████████████████████████████████████████████████████████████▎                                                                  | 28/57 [02:47<03:44,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  51%|██████████████████████████████████████████████████████████████████▋                                                                | 29/57 [02:51<03:06,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier December 2017 (Volume 57 Issue 10).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  53%|████████████████████████████████████████████████████████████████████▉                                                              | 30/57 [02:57<02:49,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2022MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|███████████████████████████████████████████████████████████████████████▏                                                           | 31/57 [03:05<02:55,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier June 2016 (Volume 56 Issue 5).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|█████████████████████████████████████████████████████████████████████████▌                                                         | 32/57 [03:10<02:35,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  58%|███████████████████████████████████████████████████████████████████████████▊                                                       | 33/57 [03:16<02:29,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 58 Issue 2 (March 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  60%|██████████████████████████████████████████████████████████████████████████████▏                                                    | 34/57 [03:25<02:41,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier Volume 58 Issue 3 (April 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|████████████████████████████████████████████████████████████████████████████████▍                                                  | 35/57 [03:31<02:29,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Mar 2016 (Volume 56 Issue 2).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  63%|██████████████████████████████████████████████████████████████████████████████████▋                                                | 36/57 [03:41<02:44,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 chunks from CERNCourier2020JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  65%|█████████████████████████████████████████████████████████████████████████████████████                                              | 37/57 [03:46<02:16,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2023NovDec-digitaledition NEW.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|███████████████████████████████████████████████████████████████████████████████████████▎                                           | 38/57 [03:52<02:07,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|█████████████████████████████████████████████████████████████████████████████████████████▋                                         | 39/57 [03:58<01:53,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  70%|███████████████████████████████████████████████████████████████████████████████████████████▉                                       | 40/57 [04:01<01:32,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2023JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  72%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 41/57 [04:05<01:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2024MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  74%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 42/57 [04:09<01:09,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERN Courier Volume 57 Issue 7 (September 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 43/57 [04:13<01:03,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████                              | 44/57 [04:19<01:01,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2020MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 45/57 [04:21<00:50,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier November 2016 (Volume 56 Issue 9).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 46/57 [04:26<00:46,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 47/57 [04:30<00:41,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERNCourier2018JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 48/57 [04:34<00:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2020SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 49/57 [04:39<00:36,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CCMayJun19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 50/57 [04:44<00:32,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERNCourier2018Dec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 51/57 [04:49<00:27,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERNCourier2018Oct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 52/57 [04:56<00:27,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 chunks from CERNCourier2022MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 53/57 [05:03<00:22,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 14 chunks from CERN Courier May 2016 (Volume 56 Issue 4).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 54/57 [05:07<00:16,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 55/57 [05:11<00:10,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 56/57 [05:16<00:04,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERNCourier2018Nov-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [05:21<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2024JanFeb-digitaledition.pdf\n",
      "\n",
      "Created 1104 valid training examples\n",
      "Training data saved to finetune_data/training_data.jsonl\n",
      "\n",
      "Step 2: Submitting fine-tuning job...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file uploaded with ID: file-57Mt4LPzbyj7jMS49CcMc8\n",
      "Fine-tuning job created with ID: ftjob-8tjSyNrQ1YLXahS4y7m2V82M\n",
      "\n",
      "Monitoring fine-tuning job...\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: succeeded\n",
      "Trained tokens: 6,454,262\n",
      "\n",
      "Fine-tuning completed successfully!\n",
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\n",
      "\n",
      "Fine-tuning process completed successfully!\n",
      "You can now use your fine-tuned model with ID: ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\n",
      "CPU times: user 5min 20s, sys: 1.74 s, total: 5min 22s\n",
      "Wall time: 53min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class FineTunePrep:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", output_dir=\"finetune_data\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Constants for token limits\n",
    "        self.MAX_TOKENS_PER_EXAMPLE = 3000  # Leave room for system and user messages\n",
    "        self.MIN_TOKENS_PER_EXAMPLE = 500   # Ensure meaningful content\n",
    "        \n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count tokens in a text string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def split_into_chunks(self, text):\n",
    "        \"\"\"Split text into chunks of appropriate token length\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Split into sentences (roughly)\n",
    "        sentences = [s.strip() + \".\" for s in text.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If single sentence is too long, split it into smaller parts\n",
    "            if sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                words = sentence.split()\n",
    "                temp_chunk = \"\"\n",
    "                temp_tokens = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    word_tokens = self.count_tokens(word + \" \")\n",
    "                    if temp_tokens + word_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                        if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = word + \" \"\n",
    "                        temp_tokens = word_tokens\n",
    "                    else:\n",
    "                        temp_chunk += word + \" \"\n",
    "                        temp_tokens += word_tokens\n",
    "                \n",
    "                if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(temp_chunk.strip())\n",
    "                continue\n",
    "            \n",
    "            # If adding this sentence would exceed limit, save current chunk and start new one\n",
    "            if current_tokens + sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "                current_tokens = sentence_tokens\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk if it's long enough\n",
    "        if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def create_training_examples(self, chunks):\n",
    "        \"\"\"Create training examples from text chunks\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Create the messages for this chunk\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What are the key findings or developments described in this CERN research?\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"Based on the CERN publications: {chunk}\"\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Verify total tokens\n",
    "            total_tokens = sum(self.count_tokens(msg[\"content\"]) for msg in messages)\n",
    "            if total_tokens <= 4096:  # GPT-4's context window\n",
    "                examples.append({\"messages\": messages})\n",
    "            \n",
    "        return examples\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Process PDFs and prepare training data\"\"\"\n",
    "        all_examples = []\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # First split text into appropriate chunks\n",
    "                chunks = self.split_into_chunks(text)\n",
    "                print(f\"\\nCreated {len(chunks)} chunks from {pdf_path.name}\")\n",
    "                \n",
    "                # Create examples from chunks\n",
    "                examples = self.create_training_examples(chunks)\n",
    "                all_examples.extend(examples)\n",
    "        \n",
    "        # Save training data\n",
    "        training_file_path = self.output_dir / \"training_data.jsonl\"\n",
    "        with open(training_file_path, 'w', encoding='utf-8') as f:\n",
    "            for example in all_examples:\n",
    "                f.write(json.dumps(example) + '\\n')\n",
    "        \n",
    "        print(f\"\\nCreated {len(all_examples)} valid training examples\")\n",
    "        print(f\"Training data saved to {training_file_path}\")\n",
    "        return training_file_path\n",
    "\n",
    "    def submit_fine_tuning_job(self, training_file_path):\n",
    "        \"\"\"Submit fine-tuning job to OpenAI\"\"\"\n",
    "        try:\n",
    "            # Upload the training file\n",
    "            with open(training_file_path, 'rb') as f:\n",
    "                training_file = self.client.files.create(\n",
    "                    file=f,\n",
    "                    purpose='fine-tune'\n",
    "                )\n",
    "            print(f\"Training file uploaded with ID: {training_file.id}\")\n",
    "            \n",
    "            # Create fine-tuning job\n",
    "            job = self.client.fine_tuning.jobs.create(\n",
    "                training_file=training_file.id,\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 2,\n",
    "                    \"learning_rate_multiplier\": 0.1\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Fine-tuning job created with ID: {job.id}\")\n",
    "            return job.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting fine-tuning job: {e}\")\n",
    "            return None\n",
    "\n",
    "    def monitor_fine_tuning_job(self, job_id):\n",
    "        \"\"\"Monitor the status of a fine-tuning job\"\"\"\n",
    "        print(\"\\nMonitoring fine-tuning job...\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                job = self.client.fine_tuning.jobs.retrieve(job_id)\n",
    "                print(f\"\\nStatus: {job.status}\")\n",
    "                \n",
    "                # Safely print additional info if available\n",
    "                if hasattr(job, 'trained_tokens') and job.trained_tokens is not None:\n",
    "                    print(f\"Trained tokens: {job.trained_tokens:,}\")\n",
    "                if hasattr(job, 'training_accuracy') and job.training_accuracy is not None:\n",
    "                    print(f\"Training accuracy: {job.training_accuracy:.4f}\")\n",
    "                \n",
    "                if job.status == 'succeeded':\n",
    "                    print(f\"\\nFine-tuning completed successfully!\")\n",
    "                    print(f\"Fine-tuned model ID: {job.fine_tuned_model}\")\n",
    "                    return job\n",
    "                elif job.status == 'failed':\n",
    "                    print(f\"\\nFine-tuning failed: {getattr(job, 'error', 'Unknown error')}\")\n",
    "                    return job\n",
    "                elif job.status == 'cancelled':\n",
    "                    print(\"\\nFine-tuning job was cancelled\")\n",
    "                    return job\n",
    "                \n",
    "                time.sleep(60)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking job status: {e}\")\n",
    "                time.sleep(60)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        prep = FineTunePrep()\n",
    "        \n",
    "        print(\"Step 1: Preparing training data...\")\n",
    "        training_file_path = prep.prepare_training_data()\n",
    "        \n",
    "        print(\"\\nStep 2: Submitting fine-tuning job...\")\n",
    "        job_id = prep.submit_fine_tuning_job(training_file_path)\n",
    "        \n",
    "        if job_id:\n",
    "            final_job = prep.monitor_fine_tuning_job(job_id)\n",
    "            \n",
    "            if getattr(final_job, 'status', None) == 'succeeded':\n",
    "                print(\"\\nFine-tuning process completed successfully!\")\n",
    "                print(f\"You can now use your fine-tuned model with ID: {final_job.fine_tuned_model}\")\n",
    "                \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3602ca-6c2e-4c37-bc93-4be91e88d430",
   "metadata": {},
   "source": [
    "# 5: RAG vs Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb37745d-117b-4912-a95e-1b9370c8fc2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3859642/655315240.py:24: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN Research Assistant Comparison\n",
      "Compare Fine-tuned model vs RAG approach\n",
      "Type 'quit' to exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  why is the universe expanding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: why is the universe expanding\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The expansion of the universe is a fundamental observation in cosmology, first noted by Edwin Hubble in the 1920s. The primary reason for this expansion is attributed to the Big Bang, the event that marked the beginning of the universe approximately 13.8 billion years ago. \n",
      "\n",
      "Initially, the universe was in an extremely hot and dense state, and as it expanded, it cooled down, allowing for the formation of subatomic particles and later atoms. This expansion is described by the solutions to Einstein's equations of general relativity, which govern the dynamics of spacetime.\n",
      "\n",
      "In addition to the initial expansion, observations show that the rate of expansion is currently accelerating. This acceleration is attributed to a mysterious form of energy known as dark energy, which makes up about 68% of the universe. The exact nature of dark energy is still one of the biggest questions in cosmology.\n",
      "\n",
      "In summary, the universe is expanding due to the initial conditions set by the Big Bang and is currently accelerating in its expansion due to dark energy.\n",
      "Response time: 2.73 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The universe is expanding due to a phenomenon known as dark energy, which is driving the accelerated expansion. This was first realized about 20 years ago and has been confirmed by numerous observations (CERN Courier Volume 57 Issue 4, May 2017). The expansion rate of the universe, defined by the Hubble constant, is a fundamental quantity in cosmology and is usually determined using techniques involving Cepheid variables and supernovae (CERN Courier Volume 57 Issue 3, April 2017).\n",
      "\n",
      "One idea is that the cosmological constant, initially introduced by Einstein, is linked to vacuum energy. However, the vacuum energy alone cannot fully explain the observed expansion rate, as theoretical calculations produce a value about 120 orders of magnitude higher than what is observed. This discrepancy suggests that the vacuum energy cannot be the final answer (CERNCourier2019SepOct-digitaledition.pdf).\n",
      "\n",
      "The source and physics of dark energy remain unknown, and it is an ongoing area of research. Physicists and astronomers are exploring various theoretical models and experimental programs to understand this mysterious component of the universe. The current understanding is based on the lambda-cold-dark-matter (ΛCDM) model, but the exact nature of dark energy is still one of the most important puzzles in modern fundamental physics (CERN Courier Volume 57 Issue 4, May 2017).\n",
      "\n",
      "In summary, the universe is expanding due to dark energy, but the exact nature and source of this energy are still not fully understood. Further observations and research are needed to uncover the physics behind this phenomenon.\n",
      "Response time: 5.71 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  what is the Future Circular Collider (FCC) feasibility study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: what is the Future Circular Collider (FCC) feasibility study\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The Future Circular Collider (FCC) feasibility study is a comprehensive assessment of the technical, financial, and organizational aspects of a proposed next-generation particle accelerator complex at CERN. The FCC aims to explore the energy frontier of particle physics beyond the capabilities of the Large Hadron Collider (LHC) and to address fundamental questions about the universe, such as the nature of dark matter and the properties of the Higgs boson.\n",
      "\n",
      "The feasibility study, which began in 2014, is part of a broader effort to develop a long-term vision for the future of particle physics. It includes detailed studies of the accelerator technologies, detector concepts, and experimental programs that would be required for the FCC, as well as an evaluation of the potential scientific impact of the project.\n",
      "\n",
      "The FCC feasibility study is being conducted by a large international collaboration of scientists and engineers, and it is expected to be completed in the coming years. The results of the study will be used to inform the decision-making process for the future of particle physics at CERN and around the world.\n",
      "Response time: 2.50 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The Future Circular Collider (FCC) feasibility study is an initiative aimed at exploring the potential for a new particle collider that would succeed the Large Hadron Collider (LHC) at CERN. The study is focused on the development of a circular collider that could serve as a Higgs factory and later evolve into a hadron collider with significantly higher energy capabilities than the LHC.\n",
      "\n",
      "1. **Purpose and Vision**: The FCC feasibility study is part of a broader vision outlined in the European strategy for particle physics. The goal is to prepare for a Higgs factory, which would be followed by a future hadron collider with sensitivity to energy scales an order of magnitude higher than those achievable at the LHC (CERNCourier2021JanFeb).\n",
      "\n",
      "2. **Workshops and Collaborations**: The FCC study has involved numerous workshops and collaborations. For instance, a workshop held online from 9 to 13 November brought together around 500 scientists, engineers, and stakeholders to discuss the roadmap for realizing this vision (CERNCourier2021JanFeb). Another significant meeting took place in Brussels from 24 to 28 June, where more than 400 researchers gathered to discuss innovations in superconductivity, high-field magnets, and other technologies relevant to the FCC (CERNCourier2019SepOct).\n",
      "\n",
      "3. **Horizon 2020 Projects**: The FCC feasibility study is supported by European Union projects such as the Horizon 2020 FCC Innovation Study (FCCIS) and the EuroCirCol project. These projects aim to produce conceptual design studies and support the preparation of feasibility studies for a 100 km-circumference collider (CERNCourier2021JanFeb, CERNCourier2019SepOct).\n",
      "\n",
      "4. **Economic Impact**: A report commissioned by the FCC group indicated that participation in CERN procurement had a positive impact on the long-term operating revenues and profitability of LHC suppliers. The benefits of future colliders, such as the FCC, are expected to be at least as high (CERN Courier Volume 58 Issue 2, March 2018).\n",
      "\n",
      "In summary, the FCC feasibility study is a comprehensive effort to assess the technical, economic, and scientific viability of a next-generation circular collider that could significantly advance particle physics research beyond the capabilities of the current LHC.\n",
      "Response time: 11.32 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  What does the timeline for a 10TeV muon collider look like?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: What does the timeline for a 10TeV muon collider look like?\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The timeline for a 10 TeV muon collider is not definitively established, as it depends on various factors including technological developments, funding, and international collaboration. However, the European Strategy for Particle Physics Update 2020 has identified a muon collider as a potential future project. \n",
      "\n",
      "The timeline for such a project could be divided into several phases:\n",
      "\n",
      "1. **Feasibility Studies (Ongoing)**: Continued research into the feasibility of muon colliders, including studies on cooling, acceleration, and detector technologies.\n",
      "\n",
      "2. **Conceptual Design Report (5-10 years)**: Development of a detailed conceptual design report (CDR) that outlines the technical and financial aspects of the project.\n",
      "\n",
      "3. **R&D Phase (10-15 years)**: A dedicated R&D phase to address the technical challenges identified in the CDR, including the development of muon cooling and acceleration technologies.\n",
      "\n",
      "4. **Construction (10-15 years)**: Following a successful R&D phase and securing funding, the construction of the collider could take place, potentially in the 2040s or 2050s.\n",
      "\n",
      "5. **Operation (Ongoing)**: Once constructed, the collider would enter an operational phase, with data collection and analysis continuing for many years.\n",
      "\n",
      "This is a very rough estimate and the actual timeline could vary significantly based on the progress of the R&D and the global scientific landscape.\n",
      "Response time: 3.19 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The timeline for a 10 TeV muon collider involves several stages and is subject to various technical and funding considerations. According to the articles, the panel exploring muon beams and colliders has set a target date of 2026 to demonstrate that further investment in a 10 TeV collider is justified. This includes focusing on a 3 TeV intermediate-scale facility targeted for the 2040s (CERNCourier2022JanFeb-digitaledition.pdf).\n",
      "\n",
      "The timeline for the muon collider is described as a technically limited plan, which assumes that sufficient funding is available and that there are no external constraints impacting deployment. This timeline is considered more aspirational compared to the FCC-ee timeline, which attempts to incorporate actual deployment constraints (CERNCourier2024MarApr-digitaledition.pdf).\n",
      "\n",
      "Overall, the timeline suggests that physics operations for a high-energy muon collider could begin at least a decade after construction approval, with the current schedule placing it about five years earlier than the FCC-ee. However, this remains an aspiration rather than a fixed schedule (CERNCourier2024MarApr-digitaledition.pdf).\n",
      "Response time: 3.06 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(self, fine_tuned_model_id, db_dir=\"cern_vectordb\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def query_fine_tuned_model(self, question):\n",
    "        \"\"\"Query the fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying fine-tuned model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.rag_chain.invoke(question)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from both approaches\"\"\"\n",
    "        print(\"\\nQuerying both models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        ft_result = self.query_fine_tuned_model(question)\n",
    "        rag_result = self.query_rag(question)\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Question:\", question)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nFine-tuned Model Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\nRAG System Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'fine_tuned': ft_result,\n",
    "            'rag': rag_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Replace with your fine-tuned model ID\n",
    "    FINE_TUNED_MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\"\n",
    "    \n",
    "    try:\n",
    "        comparison = ModelComparison(FINE_TUNED_MODEL_ID)\n",
    "        \n",
    "        print(\"CERN Research Assistant Comparison\")\n",
    "        print(\"Compare Fine-tuned model vs RAG approach\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7310b5d-bf94-4ab8-946b-3cbee47f82f8",
   "metadata": {},
   "source": [
    "# 6: Fine Tunging on nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8163c-ed3a-4657-9df0-0e962b26fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3090\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5a2106eb2840fab41f24605ff4f145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff230932bec4a529d4396dc89d5a9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig  # Added BitsAndBytesConfig import\n",
    ")\n",
    "from datasets import Dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "from accelerate import Accelerator\n",
    "import bitsandbytes as bnb\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class LlamaFineTuner:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        cache_dir=None,\n",
    "        pdf_dir=\"cern_pdfs\",\n",
    "        output_dir=\"llama_finetuned\",\n",
    "        device=\"cuda\",\n",
    "        hf_token=None\n",
    "    ):\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.device = device\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        try:\n",
    "            # Load tokenizer with error handling\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=self.cache_dir\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "                \n",
    "            print(\"Loading model...\")\n",
    "            # Configure quantization\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config\n",
    "            )\n",
    "            \n",
    "            # Resize token embeddings if needed\n",
    "            if len(self.tokenizer) != self.model.config.vocab_size:\n",
    "                self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "            \n",
    "            # Prepare model for LoRA fine-tuning\n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "            \n",
    "            # Configure LoRA\n",
    "            lora_config = LoraConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM\n",
    "            )\n",
    "            \n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "            \n",
    "            # Initialize Accelerator\n",
    "            self.accelerator = Accelerator(\n",
    "                gradient_accumulation_steps=4,\n",
    "                mixed_precision=\"fp16\",\n",
    "                log_with=\"tensorboard\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error initializing model and tokenizer: {str(e)}\")\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_training_data(self, max_length=512):\n",
    "        \"\"\"Prepare training data from PDFs\"\"\"\n",
    "        if not self.pdf_dir.exists():\n",
    "            raise ValueError(f\"PDF directory not found: {self.pdf_dir}\")\n",
    "            \n",
    "        training_data = []\n",
    "        \n",
    "        # Process each PDF\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            raise ValueError(f\"No PDF files found in {self.pdf_dir}\")\n",
    "            \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Create instruction format\n",
    "                entry = {\n",
    "                    \"instruction\": \"Analyze and explain the following CERN research:\",\n",
    "                    \"input\": text,\n",
    "                    \"output\": \"This CERN research discusses: \" + text[:500]\n",
    "                }\n",
    "                training_data.append(entry)\n",
    "        \n",
    "        if not training_data:\n",
    "            raise ValueError(\"No valid training data could be extracted from PDFs\")\n",
    "        \n",
    "        # Convert to dataset\n",
    "        dataset = Dataset.from_list(training_data)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            prompts = []\n",
    "            for inst, inp in zip(examples[\"instruction\"], examples[\"input\"]):\n",
    "                prompt = f\"[INST] {inst}\\n{inp} [/INST]\"\n",
    "                prompts.append(prompt)\n",
    "            \n",
    "            tokenized = self.tokenizer(\n",
    "                prompts,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return tokenized\n",
    "        \n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "\n",
    "    def train(self, num_epochs=3, batch_size=4):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        try:\n",
    "            print(\"Preparing training data...\")\n",
    "            train_dataset = self.prepare_training_data()\n",
    "            \n",
    "            # Configure training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=str(self.output_dir),\n",
    "                num_train_epochs=num_epochs,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                gradient_accumulation_steps=4,\n",
    "                warmup_steps=100,\n",
    "                logging_steps=10,\n",
    "                save_steps=100,\n",
    "                learning_rate=2e-4,\n",
    "                fp16=True,\n",
    "                optim=\"adamw_torch\",\n",
    "                report_to=\"tensorboard\",\n",
    "                remove_unused_columns=False,\n",
    "            )\n",
    "            \n",
    "            # Initialize trainer\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                data_collator=DataCollatorForLanguageModeling(\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    mlm=False\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"Starting training...\")\n",
    "            trainer.train()\n",
    "            \n",
    "            print(\"Saving model...\")\n",
    "            trainer.save_model(str(self.output_dir / \"final_model\"))\n",
    "            self.tokenizer.save_pretrained(str(self.output_dir / \"final_model\"))\n",
    "            \n",
    "            print(f\"Training complete! Model saved to {self.output_dir / 'final_model'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during training: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Check for GPU\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"This script requires a CUDA-capable GPU\")\n",
    "        \n",
    "        # Print GPU info\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"Using GPU: {gpu_name}\")\n",
    "        \n",
    "        # Configure PyTorch for maximum performance\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        \n",
    "        # Set maximum memory usage (95% of available memory)\n",
    "        torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "        \n",
    "        # Create cache directory\n",
    "        cache_dir = Path.home() / \".cache\" / \"huggingface\"\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize and train with cache directory\n",
    "        finetuner = LlamaFineTuner(cache_dir=str(cache_dir))\n",
    "        finetuner.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11f248-26f8-4529-92e2-f6aa8830c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "class RAGQuerySystem:\n",
    "    def __init__(self, embeddings_path: str, openai_model: str = \"gpt-4o\"):\n",
    "        # Load environment variables\n",
    "        dotenv.load_dotenv()\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = openai_model\n",
    "        \n",
    "        # Load embeddings and documents\n",
    "        data = np.load(embeddings_path, allow_pickle=True)\n",
    "        self.embeddings = data['encodings']\n",
    "        self.chunks = data['chunks']\n",
    "        self.metadata = data['metadata']\n",
    "        \n",
    "        # Initialize the same embedding model used for document encoding\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        # Get query embedding using the same model as document encoding\n",
    "        query_embedding = self.embedding_model.encode(question)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = np.dot(self.embeddings, query_embedding) / (\n",
    "            norm(self.embeddings, axis=1) * norm(query_embedding)\n",
    "        )\n",
    "        \n",
    "        # Get top 3 similar chunks\n",
    "        top_indices = np.argsort(similarities)[-3:][::-1]\n",
    "        \n",
    "        # Prepare context from similar chunks\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Document: {self.metadata[idx]}]\\n{self.chunks[idx]}\"\n",
    "            for idx in top_indices\n",
    "        ])\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Using the following CERN research documents as context, answer the question. \n",
    "        If you cannot answer from the context, say so.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "        # Get response from OpenAI\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful physics research assistant specializing in CERN experiments and findings.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    # Initialize RAG system\n",
    "    rag = RAGQuerySystem('document_encodings.npz')\n",
    "    \n",
    "    # Ask about W boson mass\n",
    "    question = \"Based on the latest data inputs, the Standard Model (SM) constrains the mass of the W boson (mW) to be?\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nQuestion:\", question)\n",
    "        answer = rag.query(question)\n",
    "        print(\"\\nAnswer:\", answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b2ec7-6a24-4088-96c1-7ef8bec787b6",
   "metadata": {},
   "source": [
    "## Anything that you ever wnated to know about Cern.  Review some of the QA from below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class RAGQuerySystem:\n",
    "    def __init__(self, embeddings_path: str, openai_model: str = \"gpt-4o\"):\n",
    "        # Load environment variables\n",
    "        dotenv.load_dotenv()\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.model = openai_model\n",
    "        \n",
    "        # Load embeddings and documents\n",
    "        data = np.load(embeddings_path, allow_pickle=True)\n",
    "        self.embeddings = data['encodings']\n",
    "        self.chunks = data['chunks']\n",
    "        self.metadata = data['metadata']\n",
    "        \n",
    "        # Initialize the same embedding model used for document encoding\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def find_similar_chunks(self, query_embedding: np.ndarray, top_k: int = 3) -> List[Tuple[int, float]]:\n",
    "        # Calculate cosine similarity\n",
    "        similarities = np.dot(self.embeddings, query_embedding) / (\n",
    "            norm(self.embeddings, axis=1) * norm(query_embedding)\n",
    "        )\n",
    "        \n",
    "        # Get top k similar chunks\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        return [(i, similarities[i]) for i in top_indices]\n",
    "\n",
    "    def query(self, question: str) -> str:\n",
    "        # Get query embedding using the same model as document encoding\n",
    "        query_embedding = self.embedding_model.encode(question)\n",
    "        \n",
    "        # Find relevant chunks\n",
    "        similar_chunks = self.find_similar_chunks(query_embedding)\n",
    "        \n",
    "        # Prepare context from similar chunks\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Document: {self.metadata[idx]}]\\n{self.chunks[idx]}\"\n",
    "            for idx, _ in similar_chunks\n",
    "        ])\n",
    "\n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Using the following CERN research documents as context, answer the question. \n",
    "        If you cannot answer from the context, say so.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "\n",
    "        # Get response from OpenAI\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful physics research assistant specializing in CERN experiments and findings.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    rag = RAGQuerySystem('document_encodings.npz')\n",
    "    \n",
    "    print(\"CERN Research Query System (type 'quit' to exit)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nWhat would you like to know about CERN research? \").strip()\n",
    "        \n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            answer = rag.query(question)\n",
    "            print(\"\\nAnswer:\", answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cabde4-4ed3-448a-b8b3-1fe90721d28b",
   "metadata": {},
   "source": [
    "# RAG System Analysis: Improving Answer Quality with Document Grounding\n",
    "## A Comparison of Direct LLM vs. RAG-Enhanced Responses\n",
    "\n",
    "## Introduction\n",
    "This notebook analyzes the effectiveness of using Retrieval-Augmented Generation (RAG) for querying CERN research data compared to direct LLM queries. Our implementation processes CERN Courier documents to ground responses in authoritative sources.\n",
    "\n",
    "## System Architecture\n",
    "### Document Processing Pipeline\n",
    "```python\n",
    "# Key statistics from our implementation\n",
    "total_pdfs_processed = 96\n",
    "total_chunks_processed = 22103\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "chunk_size = 1000\n",
    "```\n",
    "\n",
    "Our system processes PDFs through several stages:\n",
    "1. Document Collection: Downloads CERN Courier PDFs\n",
    "2. Text Extraction: Converts PDFs to processable text\n",
    "3. Chunking: Splits text into manageable segments\n",
    "4. Embedding Generation: Creates vector representations\n",
    "5. Indexing: Organizes embeddings for efficient retrieval\n",
    "\n",
    "## Comparative Analysis\n",
    "\n",
    "### Case Study: W Boson Mass Query\n",
    "\n",
    "#### Direct OpenAI Query\n",
    "```python\n",
    "question = \"Based on the latest data inputs, what does the Standard Model (SM) constrain the mass of the W boson (mW) to be?\"\n",
    "\n",
    "# Direct OpenAI response (truncated):\n",
    "\"\"\"\n",
    "As of my last update in 2023, within the framework of the Standard Model of particle physics, \n",
    "precise calculations constrain the mass of the W boson (mW) based on various experimental \n",
    "inputs and theoretical considerations...\n",
    "\n",
    "Before 2022, the Particle Data Group (PDG) reported a world average...\n",
    "CDF collaboration announced a new measurement...\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### RAG System Query\n",
    "```python\n",
    "# RAG system response:\n",
    "\"\"\"\n",
    "80357 ± 6 MeV\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Key Improvements Analysis\n",
    "\n",
    "#### 1. Answer Precision\n",
    "- **Direct LLM**: \n",
    "  - Provides general context\n",
    "  - Includes historical data\n",
    "  - No specific current value\n",
    "  \n",
    "- **RAG System**:\n",
    "  - Exact measurement with uncertainty\n",
    "  - Current CERN-sourced value\n",
    "  - No extraneous information\n",
    "\n",
    "#### 2. Response Time\n",
    "```python\n",
    "# Average response times\n",
    "direct_llm_time = \"3.2 seconds\"\n",
    "rag_system_time = \"5.1 seconds\"  # Includes retrieval overhead\n",
    "```\n",
    "\n",
    "#### 3. Source Reliability\n",
    "```python\n",
    "# RAG system source tracking\n",
    "source_metadata = {\n",
    "    \"document_type\": \"CERN Courier\",\n",
    "    \"total_sources\": 96,\n",
    "    \"date_range\": \"Last 11 years\",\n",
    "    \"verification\": \"Direct CERN publications\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Technical Implementation Details\n",
    "\n",
    "### Embedding Generation\n",
    "```python\n",
    "class DocumentEncoder:\n",
    "    def __init__(self, batch_size=256):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def encode_documents(self, pdf_directory):\n",
    "        # Process and encode documents\n",
    "        # Returns: embeddings array of shape (n_chunks, 384)\n",
    "```\n",
    "\n",
    "### Query Processing\n",
    "```python\n",
    "class RAGQuerySystem:\n",
    "    def query(self, question: str) -> str:\n",
    "        # 1. Encode question\n",
    "        # 2. Find similar chunks\n",
    "        # 3. Prepare context\n",
    "        # 4. Generate answer\n",
    "        return answer\n",
    "```\n",
    "\n",
    "## Results Summary\n",
    "\n",
    "### Performance Metrics\n",
    "1. **Accuracy**:\n",
    "   - RAG responses grounded in CERN documentation\n",
    "   - Specific numerical values vs. general ranges\n",
    "   - Reduced hallucination risk\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Fast response times (~5s)\n",
    "   - Focused, relevant answers\n",
    "   - Direct access to technical details\n",
    "\n",
    "3. **Source Attribution**:\n",
    "   - All answers traceable to CERN documents\n",
    "   - Metadata preserved for verification\n",
    "   - Recent publications ensure currency\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "The RAG system demonstrates significant advantages over direct LLM queries:\n",
    "1. Higher precision in technical answers\n",
    "2. Direct grounding in authoritative sources\n",
    "3. Clear provenance for all information\n",
    "4. Reduced tendency for hallucination or generalization\n",
    "\n",
    "### Future Improvements\n",
    "1. Implement caching for faster responses\n",
    "2. Add source citation in responses\n",
    "3. Expand document collection\n",
    "4. Optimize chunk size for better context\n",
    "\n",
    "## Usage Example\n",
    "```python\n",
    "# Initialize RAG system\n",
    "rag = RAGQuerySystem('document_encodings.npz')\n",
    "\n",
    "# Example query\n",
    "question = \"How many people work for CERN?\"\n",
    "answer = rag.query(question)\n",
    "print(f\"Answer: {answer}\")  # Output: \"CERN employs around 2600 staff members.\"\n",
    "```\n",
    "\n",
    "This implementation shows how combining retrieval with generation can significantly improve the quality and reliability of answers in specialized technical domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59127029-07f4-41d3-a70f-872c154416ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
