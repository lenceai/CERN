{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "## CERN is celebrating its 70th anniversary \n",
    "During a crucial period for high-energy physics, coinciding with the initiation of the third update to the European strategy for particle physics. In this special edition of CERN Courier magazine, early-career researchers share their visions for the future of the field while reflecting on CERN's scientific and societal contributions. The magazine features expert insights into the achievements of the Large Hadron Collider (LHC) and explores the advancements of the hybrid pixel detector technology, emphasizing its applications beyond particle physics.\n",
    "\n",
    "## The CERN Courier website \n",
    "is a rich repository of articles covering a wide array of topics in particle physics, high-energy physics, and associated technological advancements. It provides in-depth reporting on the latest experimental results from CERN and other international laboratories, offering insights into ongoing research and discoveries in the field.\n",
    "\n",
    "## Last 11 years of the CERN Courier Magazine in PDF\n",
    "In this dataset I am downloading the Last 11 years of the CERN Courier Magazine.  I will then take this database and then encode it to be used as a Context Window to ask Questions to OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "# 1: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "555b0f1c-a3f7-47cc-9c3e-546e7d19754f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 existing PDF files\n",
      "Starting CERN PDF crawler (pages 0 to 7)\n",
      "\n",
      "Processing page 0...\n",
      "Found 15 new article links on page 0\n",
      "\n",
      "Found PDF: CERNCourier2024MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2896932/files/CERNCourier2024MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 14.7M/14.7M [00:01<00:00, 8.40MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2024MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2893513/files/CERNCourier2024MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 13.5M/13.5M [00:01<00:00, 12.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2024JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2886335/files/CERNCourier2024JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2024JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 13.0M/13.0M [00:01<00:00, 8.06MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023NovDec-digitaledition NEW.pdf\n",
      "URL: https://cds.cern.ch/record/2879381/files/CERNCourier2023NovDec-digitaledition%20NEW.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023NovDec-digitaledition NEW.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████| 12.3M/12.3M [00:01<00:00, 10.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2869155/files/CERNCourier2023SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 19.6M/19.6M [00:01<00:00, 12.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2863407/files/CERNCourier2023JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 16.3M/16.3M [00:01<00:00, 11.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2857134/files/CERNCourier2023MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 18.3M/18.3M [00:01<00:00, 12.7MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2857133/files/CERNCourier2023MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 15.9M/15.9M [00:01<00:00, 12.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2023JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2845914/files/CERNCourier2023JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2023JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 11.7M/11.7M [00:01<00:00, 8.07MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2840144/files/CERNCourier2022NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 15.4M/15.4M [00:01<00:00, 11.9MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2826497/files/CERNCourier2022SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 13.6M/13.6M [00:01<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2807618/files/CERNCourier2022MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 12.9M/12.9M [00:01<00:00, 7.46MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 1...\n",
      "Found 15 new article links on page 1\n",
      "\n",
      "Found PDF: CERNCourier2022MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2804425/files/CERNCourier2022MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 6.64MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2022JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2799462/files/CERNCourier2022JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2022JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 9.46M/9.46M [00:01<00:00, 8.60MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2789409/files/CERNCourier2021NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 11.9M/11.9M [00:01<00:00, 8.35MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2782568/files/CERNCourier2021SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 15.1M/15.1M [00:01<00:00, 10.8MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2773907/files/CERNCourier2021JulAug-digitaledition.pdf\n",
      "Error downloading CERNCourier2021JulAug-digitaledition.pdf: 404 Client Error: Not Found for url: https://cds.cern.ch/record/2773907/files/CERNCourier2021JulAug-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier2021MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2765233/files/CERNCourier2021MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 9.97M/9.97M [00:01<00:00, 6.30MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2753402/files/CERNCourier2021MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 13.8M/13.8M [00:01<00:00, 9.70MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2021JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2750037/files/CERNCourier2021JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2021JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 21.6M/21.6M [00:01<00:00, 13.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2743359/files/CERNCourier2020NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 10.6M/10.6M [00:01<00:00, 9.70MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2743358/files/CERNCourier2020SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 13.1M/13.1M [00:01<00:00, 7.75MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2722711/files/CERNCourier2020JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 13.5M/13.5M [00:01<00:00, 7.12MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020MayJun-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2717129/files/CERNCourier2020MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020MayJun-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 12.2M/12.2M [00:01<00:00, 6.52MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2712176/files/CERNCourier2020MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 13.7M/13.7M [00:01<00:00, 7.32MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2020JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2706508/files/CERNCourier2020JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2020JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 8.74M/8.74M [00:01<00:00, 5.60MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019NovDec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2701615/files/CERNCourier2019NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019NovDec-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 11.3M/11.3M [00:01<00:00, 7.09MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 2...\n",
      "Found 15 new article links on page 2\n",
      "\n",
      "Found PDF: CERNCourier2019SepOct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2689203/files/CERNCourier2019SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019SepOct-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 11.6M/11.6M [00:01<00:00, 6.60MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CCJulAug19-digital.pdf\n",
      "URL: https://cds.cern.ch/record/2681906/files/CCJulAug19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CCJulAug19-digital.pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 6.13MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CCMayJun19-digital.pdf\n",
      "URL: https://cds.cern.ch/record/2673718/files/CCMayJun19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CCMayJun19-digital.pdf: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 15.4M/15.4M [00:01<00:00, 8.80MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019MarApr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2666160/files/CERNCourier2019MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019MarApr-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 12.1M/12.1M [00:01<00:00, 6.93MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2019JanFeb-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2654576/files/CERNCourier2019JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2019JanFeb-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 11.2M/11.2M [00:01<00:00, 7.88MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Dec-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2649360/files/CERNCourier2018Dec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Dec-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 10.2M/10.2M [00:01<00:00, 6.55MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Nov-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2645275/files/CERNCourier2018Nov-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Nov-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 8.90M/8.90M [00:01<00:00, 7.11MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Oct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2640475/files/CERNCourier2018Oct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Oct-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 8.13M/8.13M [00:01<00:00, 5.72MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018Sep-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2636286/files/CERNCourier2018Sep-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018Sep-digitaledition.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 15.6M/15.6M [00:01<00:00, 8.91MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERNCourier2018JulAug-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/2628313/files/CERNCourier2018JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERNCourier2018JulAug-digitaledition.pdf: 100%|████████████████████████████████████████████████████████████████████████████████████████| 6.69M/6.69M [00:01<00:00, 6.19MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier June 2018 (Volume 53 Issue 5).pdf\n",
      "URL: https://home.cern/sites/default/files/2018-06/CERN%20Courier%20June%202018%20%28Volume%2053%20Issue%205%29.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier June 2018 (Volume 53 Issue 5).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████| 8.69M/8.69M [00:01<00:00, 5.03MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier May 2018 (Volume 53 Issue 4).pdf\n",
      "URL: https://cds.cern.ch/record/2318574/files/CERN%20Courier%20May%202018%20(Volume%2053%20Issue%204).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier May 2018 (Volume 53 Issue 4).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████| 9.32M/9.32M [00:01<00:00, 6.00MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 3 (April 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2309976/files/CERN%20Courier%20Volume%2058%20Issue%203%20(April%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 3 (April 2018).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████| 13.3M/13.3M [00:01<00:00, 8.46MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 2 (March 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2304934/files/CERN%20Courier%20Volume%2058%20Issue%202%20(March%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 2 (March 2018).pdf: 100%|█████████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 8.37MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf\n",
      "URL: https://cds.cern.ch/record/2300591/files/CERN%20Courier%20Volume%2058%20Issue%201%20(Jan-Feb%202018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf: 100%|███████████████████████████████████████████████████████████████████████████████| 10.5M/10.5M [00:01<00:00, 8.39MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 3...\n",
      "Found 15 new article links on page 3\n",
      "\n",
      "Found PDF: CERN Courier December 2017 (Volume 57 Issue 10).pdf\n",
      "URL: https://cds.cern.ch/record/2292627/files/CERN%20Courier%20December%202017%20(Volume%2057%20Issue%2010).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier December 2017 (Volume 57 Issue 10).pdf: 100%|█████████████████████████████████████████████████████████████████████████████| 8.04M/8.04M [00:01<00:00, 5.57MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 9 (November 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2289267/files/CERN%20Courier%20Volume%2057%20Issue%209%20(November%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 9 (November 2017).pdf: 100%|██████████████████████████████████████████████████████████████████████████████| 8.51M/8.51M [00:01<00:00, 7.67MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 8 October 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2285637/files/CERN%20Courier%20Volume%2057%20Issue%208%20October%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 8 October 2017.pdf: 100%|█████████████████████████████████████████████████████████████████████████████████| 9.17M/9.17M [00:01<00:00, 8.30MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 7 (September 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2281303/files/CERN%20Courier%20Volume%2057%20Issue%207%20(September%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 7 (September 2017).pdf: 100%|█████████████████████████████████████████████████████████████████████████████| 10.1M/10.1M [00:01<00:00, 7.82MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 6 (July-August 2017).pdf\n",
      "URL: https://cds.cern.ch/record/2273705/files/CERN%20Courier%20Volume%2057%20Issue%206%20(July-August%202017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 6 (July-August 2017).pdf: 100%|███████████████████████████████████████████████████████████████████████████| 10.6M/10.6M [00:01<00:00, 8.17MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 4 .pdf\n",
      "URL: https://cds.cern.ch/record/2259560/files/CERN%20Courier%20Volume%2057%20Issue%204%20.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 4 .pdf: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 12.5M/12.5M [00:01<00:00, 7.87MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 3 April 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2256135/files/CERN%20Courier%20Volume%2057%20Issue%203%20April%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 3 April 2017.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████| 6.81M/6.81M [00:01<00:00, 7.08MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Volume 57 Issue 2 March 2017.pdf\n",
      "URL: https://cds.cern.ch/record/2252407/files/CERN%20Courier%20Volume%2057%20Issue%202%20March%202017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Volume 57 Issue 2 March 2017.pdf: 100%|███████████████████████████████████████████████████████████████████████████████████| 9.75M/9.75M [00:01<00:00, 7.60MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf\n",
      "URL: https://cds.cern.ch/record/2241972/files/CERN%20Courier%20Jan-Feb%202017%20(Volume%2057%20issue%201).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf: 100%|███████████████████████████████████████████████████████████████████████████████| 9.89M/9.89M [00:01<00:00, 8.96MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier November 2016 (Volume 56 Issue 9).pdf\n",
      "URL: https://cds.cern.ch/record/2224294/files/CERN%20Courier%20November%202016%20(Volume%2056%20Issue%209).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier November 2016 (Volume 56 Issue 9).pdf: 100%|██████████████████████████████████████████████████████████████████████████████| 9.23M/9.23M [00:01<00:00, 6.37MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier October 2016 (Volume 56 Issue 8).pdf\n",
      "URL: https://cds.cern.ch/record/2219443/files/CERN%20Courier%20October%202016%20(Volume%2056%20Issue%208).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier October 2016 (Volume 56 Issue 8).pdf: 100%|███████████████████████████████████████████████████████████████████████████████| 16.3M/16.3M [00:01<00:00, 10.0MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier September 2016 (Volume 56 Issue 7).pdf\n",
      "URL: https://cds.cern.ch/record/2211464/files/CERN%20Courier%20September%202016%20(Volume%2056%20Issue%207).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier September 2016 (Volume 56 Issue 7).pdf: 100%|█████████████████████████████████████████████████████████████████████████████| 15.3M/15.3M [00:01<00:00, 9.60MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier July-August 2016 (Volume 56 Issue 6).pdf\n",
      "URL: http://cds.cern.ch/record/2198166/files/CERN%20Courier%20July-August%202016%20(Volume%2056%20Issue%206).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier July-August 2016 (Volume 56 Issue 6).pdf: 100%|████████████████████████████████████████████████████████████████████████████| 21.9M/21.9M [00:35<00:00, 654kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 4...\n",
      "Found 15 new article links on page 4\n",
      "\n",
      "Found PDF: CERN Courier June 2016 (Volume 56 Issue 5).pdf\n",
      "URL: https://cds.cern.ch/record/2155287/files/CERN%20Courier%20June%202016%20(Volume%2056%20Issue%205).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier June 2016 (Volume 56 Issue 5).pdf: 100%|██████████████████████████████████████████████████████████████████████████████████| 9.50M/9.50M [00:01<00:00, 6.04MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier May 2016 (Volume 56 Issue 4).pdf\n",
      "URL: http://cds.cern.ch/record/2146835/files/CERN%20Courier%20May%202016%20(Volume%2056%20Issue%204).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier May 2016 (Volume 56 Issue 4).pdf: 100%|████████████████████████████████████████████████████████████████████████████████████| 10.2M/10.2M [00:13<00:00, 801kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found PDF: CERN Courier Mar 2016 (Volume 56 Issue 2).pdf\n",
      "URL: https://cds.cern.ch/record/2131754/files/CERN%20Courier%20Mar%202016%20(Volume%2056%20Issue%202).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CERN Courier Mar 2016 (Volume 56 Issue 2).pdf: 100%|███████████████████████████████████████████████████████████████████████████████████| 9.46M/9.46M [00:01<00:00, 6.54MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing page 5...\n",
      "Found 15 new article links on page 5\n",
      "\n",
      "Found PDF: CERNCourier2013Oct-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1603700/files/CERNCourier2013Oct-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Oct-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1735007/files/CERNCourier2013Oct-digitaledition.pdf\n",
      "\n",
      "Processing page 6...\n",
      "Found 5 new article links on page 6\n",
      "\n",
      "Found PDF: CERN Courier June 2013.pdf\n",
      "URL: http://cds.cern.ch/record/1550751/files/CERN%20Courier%20June%202013.pdf\n",
      "Error downloading CERN Courier June 2013.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734960/files/CERN%20Courier%20June%202013.pdf\n",
      "\n",
      "Found PDF: CERN Courier digital edition May 2013.pdf\n",
      "URL: http://cds.cern.ch/record/1544352/files/CERN%20Courier%20digital%20edition%20May%202013.pdf\n",
      "Error downloading CERN Courier digital edition May 2013.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734947/files/CERN%20Courier%20digital%20edition%20May%202013.pdf\n",
      "\n",
      "Found PDF: CERNCourier2013Apr-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1537017/files/CERNCourier2013Apr-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Apr-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734933/files/CERNCourier2013Apr-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier2013Mar-digitaledition.pdf\n",
      "URL: https://cds.cern.ch/record/1517538/files/CERNCourier2013Mar-digitaledition.pdf\n",
      "Error downloading CERNCourier2013Mar-digitaledition.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734919/files/CERNCourier2013Mar-digitaledition.pdf\n",
      "\n",
      "Found PDF: CERNCourier-2013-53-1.pdf\n",
      "URL: http://cds.cern.ch/record/1514615/files/CERNCourier-2013-53-1.pdf\n",
      "Error downloading CERNCourier-2013-53-1.pdf: 404 Client Error: Not Found for url: http://cds.cern.ch/record/1734891/files/CERNCourier-2013-53-1.pdf\n",
      "\n",
      "Processing page 7...\n",
      "Found 0 new article links on page 7\n",
      "\n",
      "Download Summary:\n",
      "--------------------\n",
      "Total PDFs found: 64\n",
      "Successfully downloaded: 57\n",
      "Skipped (already downloaded): 0\n",
      "Failed downloads: 7\n",
      "\n",
      "Failed downloads:\n",
      "- CERNCourier2021JulAug-digitaledition.pdf\n",
      "- CERNCourier2013Oct-digitaledition.pdf\n",
      "- CERN Courier June 2013.pdf\n",
      "- CERN Courier digital edition May 2013.pdf\n",
      "- CERNCourier2013Apr-digitaledition.pdf\n",
      "- CERNCourier2013Mar-digitaledition.pdf\n",
      "- CERNCourier-2013-53-1.pdf\n",
      "CPU times: user 12.5 s, sys: 2.3 s, total: 14.8 s\n",
      "Wall time: 6min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.parse import urljoin, unquote\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "class CERNPDFCrawler:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://home.cern/resources\"\n",
    "        self.session = requests.Session()\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.download_folder = \"cern_pdfs\"\n",
    "        self.processed_article_urls = set()\n",
    "        self.downloaded_files = set()\n",
    "        \n",
    "        if not os.path.exists(self.download_folder):\n",
    "            os.makedirs(self.download_folder)\n",
    "        self.load_existing_files()\n",
    "\n",
    "    def load_existing_files(self):\n",
    "        for filename in os.listdir(self.download_folder):\n",
    "            if filename.lower().endswith('.pdf'):\n",
    "                self.downloaded_files.add(filename)\n",
    "        print(f\"Found {len(self.downloaded_files)} existing PDF files\")\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"Error fetching {url}: {e}\")\n",
    "                    return None\n",
    "                time.sleep(2 ** attempt)\n",
    "        return None\n",
    "\n",
    "    def extract_pdf_urls_from_text(self, text):\n",
    "        \"\"\"Extract PDF URLs from text content including 'File path:' patterns\"\"\"\n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Look for \"File path:\" pattern\n",
    "        file_path_matches = re.finditer(r'File path:\\s*(https?://[^\\s<>\"]+\\.pdf)', text, re.IGNORECASE)\n",
    "        for match in file_path_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        # Look for direct PDF links\n",
    "        pdf_link_matches = re.finditer(r'href=\"(https?://[^\\s<>\"]+\\.pdf)\"', text, re.IGNORECASE)\n",
    "        for match in pdf_link_matches:\n",
    "            pdf_urls.add(match.group(1))\n",
    "            \n",
    "        return pdf_urls\n",
    "\n",
    "    def find_courier_links(self, page_url):\n",
    "        content = self.get_page_content(page_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        courier_links = []\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/resources/courier/' in href or '/record/' in href:\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                if full_url not in self.processed_article_urls:\n",
    "                    courier_links.append(full_url)\n",
    "                    self.processed_article_urls.add(full_url)\n",
    "        \n",
    "        return courier_links\n",
    "\n",
    "    def find_pdf_links(self, article_url):\n",
    "        \"\"\"Find all PDF download links on an article page\"\"\"\n",
    "        content = self.get_page_content(article_url)\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        pdf_urls = set()\n",
    "        \n",
    "        # Extract URLs from text content\n",
    "        pdf_urls.update(self.extract_pdf_urls_from_text(content))\n",
    "        \n",
    "        # Parse with BeautifulSoup for structured extraction\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Look for links containing PDF\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.lower().endswith('.pdf'):\n",
    "                full_url = urljoin(\"https://home.cern\", href)\n",
    "                pdf_urls.add(full_url)\n",
    "        \n",
    "        return list(pdf_urls)\n",
    "\n",
    "    def sanitize_filename(self, url):\n",
    "        \"\"\"Create a safe filename from URL\"\"\"\n",
    "        filename = unquote(url.split('/')[-1])\n",
    "        # Remove or replace unsafe characters\n",
    "        filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "        return filename\n",
    "\n",
    "    def download_pdf(self, pdf_url, filename):\n",
    "        if filename in self.downloaded_files:\n",
    "            print(f\"Skipping {filename} - already downloaded\")\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            response = self.session.get(pdf_url, headers=self.headers, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            file_path = os.path.join(self.download_folder, filename)\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(file_path, 'wb') as file, tqdm(\n",
    "                desc=filename,\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as pbar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = file.write(data)\n",
    "                    pbar.update(size)\n",
    "            \n",
    "            self.downloaded_files.add(filename)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {filename}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def crawl_and_download(self, start_page=0, end_page=7):\n",
    "        print(f\"Starting CERN PDF crawler (pages {start_page} to {end_page})\")\n",
    "        \n",
    "        found_pdfs = 0\n",
    "        downloaded_pdfs = 0\n",
    "        skipped_pdfs = 0\n",
    "        failed_downloads = []\n",
    "        \n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = f\"{self.base_url}?type=52&page={page_num}\"\n",
    "            print(f\"\\nProcessing page {page_num}...\")\n",
    "            \n",
    "            courier_links = self.find_courier_links(page_url)\n",
    "            print(f\"Found {len(courier_links)} new article links on page {page_num}\")\n",
    "            \n",
    "            for article_url in courier_links:\n",
    "                pdf_urls = self.find_pdf_links(article_url)\n",
    "                \n",
    "                for pdf_url in pdf_urls:\n",
    "                    found_pdfs += 1\n",
    "                    filename = self.sanitize_filename(pdf_url)\n",
    "                    \n",
    "                    print(f\"\\nFound PDF: {filename}\")\n",
    "                    print(f\"URL: {pdf_url}\")\n",
    "                    \n",
    "                    if filename in self.downloaded_files:\n",
    "                        print(f\"Skipping - already downloaded\")\n",
    "                        skipped_pdfs += 1\n",
    "                        continue\n",
    "                        \n",
    "                    if self.download_pdf(pdf_url, filename):\n",
    "                        downloaded_pdfs += 1\n",
    "                    else:\n",
    "                        failed_downloads.append(filename)\n",
    "                \n",
    "                time.sleep(1)\n",
    "        \n",
    "        print(\"\\nDownload Summary:\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Total PDFs found: {found_pdfs}\")\n",
    "        print(f\"Successfully downloaded: {downloaded_pdfs}\")\n",
    "        print(f\"Skipped (already downloaded): {skipped_pdfs}\")\n",
    "        print(f\"Failed downloads: {len(failed_downloads)}\")\n",
    "        if failed_downloads:\n",
    "            print(\"\\nFailed downloads:\")\n",
    "            for fail in failed_downloads:\n",
    "                print(f\"- {fail}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawler = CERNPDFCrawler()\n",
    "    crawler.crawl_and_download(0, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78865c",
   "metadata": {},
   "source": [
    "# 2: Encode the CERN PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d93cde4-96fd-47f5-b862-93900a21ae9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [05:27<00:00,  5.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating vector database with 17732 chunks...\n",
      "Vector database created and saved to cern_vectordb\n",
      "CPU times: user 6min 1s, sys: 4.03 s, total: 6min 5s\n",
      "Wall time: 8min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:85: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.db_dir = Path(db_dir)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                \n",
    "                # Extract text from each page\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        \"\"\"Process all PDFs in the directory and return chunks with metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        # Process each PDF file\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Split text into chunks\n",
    "                chunks = self.text_splitter.create_documents(\n",
    "                    texts=[text],\n",
    "                    metadatas=[{\"source\": pdf_path.name}]\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "    def create_vector_db(self):\n",
    "        \"\"\"Create and populate the vector database\"\"\"\n",
    "        # Get text chunks\n",
    "        chunks = self.process_pdfs()\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"No text chunks were created. Check the PDF processing.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nCreating vector database with {len(chunks)} chunks...\")\n",
    "        \n",
    "        # Create and persist the vector store\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=str(self.db_dir)\n",
    "        )\n",
    "        \n",
    "        # Persist the database\n",
    "        vectordb.persist()\n",
    "        print(f\"Vector database created and saved to {self.db_dir}\")\n",
    "        \n",
    "        return vectordb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        processor = PDFProcessor()\n",
    "        vectordb = processor.create_vector_db()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd11e0-aa06-440b-af5d-d3835d32be28",
   "metadata": {},
   "source": [
    "# 3: Chat with Cern Magazine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6facc867-bba9-44a6-9c78-2f199b187dfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:20: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN Research Assistant Ready!\n",
      "Ask questions about CERN research (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  Neutrinos out of the woods\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The question \"Neutrinos out of the woods\" seems to metaphorically ask about the current state of neutrino research and whether significant progress or clarifications have been made in understanding neutrinos, moving beyond previous uncertainties or \"woods.\" Based on the provided CERN Courier articles, we can piece together a narrative that shows significant advancements in neutrino physics, though challenges and mysteries remain.\n",
      "\n",
      "1. **Understanding Neutrino Puzzles and Opportunities**:\n",
      "   The November 2016 article highlights that neutrinos are at the center of many puzzles in particle physics but also present numerous opportunities for discovery. It mentions that a minimal extension of the Standard Model, which includes three right-handed neutrinos, could explain current experiments and observations, including dark matter. This suggests that while neutrinos are complex and challenging to study, they are integral to our understanding of the universe and potential new physics beyond the Standard Model.\n",
      "\n",
      "2. **Experimental Advances**:\n",
      "   The 2022 article discusses the FASER project and its co-leader David Casper's anticipation of recording over 10,000 neutrino interactions in the next run of the LHC. This level of detection represents a significant experimental advance, indicating that researchers are moving out of the \"woods\" in terms of neutrino detection capabilities. The ability to observe and record such a high number of interactions could lead to new insights and breakthroughs in neutrino physics.\n",
      "\n",
      "3. **Astrophysical Neutrino Sources**:\n",
      "   The July-August 2016 article sheds light on the historical context of neutrino research, particularly in astrophysics. It discusses how neutrinos were initially neglected in calculations of radiation and power in the universe but later became a significant focus due to their role in solar burning and supernovae. This historical perspective shows the journey of neutrino research from relative obscurity to a central role in understanding astrophysical processes.\n",
      "\n",
      "4. **Sterile Neutrinos and Anomalies**:\n",
      "   The 2019 article discusses the search for sterile neutrinos, a hypothetical type of neutrino that does not interact via the weak force, unlike the three known types of neutrinos. It mentions experiments like MINOS and MiniBooNE, which have observed anomalies that could suggest the existence of sterile neutrinos. These ongoing investigations into sterile neutrinos and other anomalies represent areas where the \"woods\" are still thick, with researchers actively working to understand these phenomena.\n",
      "\n",
      "In summary, the articles indicate that while significant progress has been made in neutrino research, moving it out of the \"woods\" in many respects, there remain areas of uncertainty and active investigation. Advances in experimental detection, the integration of neutrinos into our understanding of astrophysical processes, and the exploration of new neutrino properties like those of sterile neutrinos, all contribute to a vibrant and evolving field of study.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  Grappling with dark energy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: The articles from CERN Courier provide a comprehensive overview of the current state of research and understanding regarding dark energy, its implications for our understanding of gravity, and the ongoing efforts to unravel its mysteries. Dark energy, alongside dark matter, represents one of the most profound enigmas in modern physics, accounting for about 95% of the energy in the universe and driving the accelerated expansion of the universe (CERN Courier Volume 57 Issue 4).\n",
      "\n",
      "The observation that the universe is experiencing accelerated expansion is a key piece of evidence pointing towards the existence of dark energy. This phenomenon suggests that the universe contains an enormous amount of additional energy, which is not accounted for by visible matter alone (CERN Courier Volume 57 Issue 4). The existence of dark energy, along with dark matter, challenges our current understanding of gravity and the overall dynamics of the universe. Despite the overwhelming evidence for dark matter through its gravitational effects on luminous matter, the true nature and composition of dark energy remain elusive (CERN Courier Volume 57 Issue 2 March 2017).\n",
      "\n",
      "Theoretical perspectives, such as those derived from string theory, suggest that gravity may be emergent, indicating that our current understanding of gravity, as described by general relativity, might not be the final answer (CERN Courier Volume 57 Issue 4). This aligns with the broader experimental and theoretical challenges faced by physicists in integrating gravity with the other fundamental forces of nature, a problem that has persisted for nearly a century (CERN Courier Jan-Feb 2017 Volume 57 issue 1).\n",
      "\n",
      "To address these challenges, experiments and projects like DARWIN are being developed. DARWIN, a dark-matter detector using liquid xenon, aims to shed light on the fundamental questions surrounding dark matter, which may also provide insights into the nature of dark energy. Scheduled to begin its first physics runs in 2024 at the Gran Sasso National Laboratory (LNGS) in Italy, DARWIN represents a significant effort by the international scientific community to probe these mysteries (CERN Courier Volume 57 Issue 2 March 2017).\n",
      "\n",
      "Furthermore, the articles highlight the importance of continuing to test general relativity (GR) and its foundational principles, such as the equivalence principle, with increasing precision. These efforts are crucial for potentially extending GR and integrating it with the quantum description of the other forces, an endeavor that might be necessary to fully understand phenomena like dark energy and dark matter (CERN Courier Jan-Feb 2017 Volume 57 issue 1).\n",
      "\n",
      "In summary, grappling with dark energy involves a multifaceted approach that includes theoretical challenges, such as understanding its role in the accelerated expansion of the universe, and experimental efforts, like the DARWIN project, aimed at uncovering the nature of dark matter and, by extension, dark energy. These efforts are part of the broader scientific quest to understand the fundamental forces of nature and the composition of the universe.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 523 ms, total: 2.25 s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "class CERNResearchAssistant:\n",
    "    def __init__(self, db_dir=\"cern_vectordb\"):\n",
    "        # Check for API key\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        # Initialize the vector store\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        # Initialize the language model\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Create the retriever\n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Setup the prompt template\n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Setup the RAG chain\n",
    "        self.chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Ask a question about CERN research\"\"\"\n",
    "        try:\n",
    "            response = self.chain.invoke(question)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {e}\"\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize the assistant\n",
    "        assistant = CERNResearchAssistant()\n",
    "        \n",
    "        print(\"CERN Research Assistant Ready!\")\n",
    "        print(\"Ask questions about CERN research (type 'quit' to exit)\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            response = assistant.query(question)\n",
    "            print(\"\\nAssistant:\", response)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c1f129-fdef-4725-a154-1bffd26c5b2b",
   "metadata": {},
   "source": [
    "# 4: Fine Tuning with OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6a4e76-cbb5-43da-92c4-e78531886a02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   2%|██▎                                                                                                                                 | 1/57 [00:05<04:56,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2024MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   4%|████▋                                                                                                                               | 2/57 [00:10<04:35,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2022NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   5%|██████▉                                                                                                                             | 3/57 [00:13<03:49,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier Volume 57 Issue 8 October 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   7%|█████████▎                                                                                                                          | 4/57 [00:17<03:45,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2023MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   9%|███████████▌                                                                                                                        | 5/57 [00:24<04:30,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2018Sep-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  11%|█████████████▉                                                                                                                      | 6/57 [00:28<04:06,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2019NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  12%|████████████████▏                                                                                                                   | 7/57 [00:31<03:32,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 57 Issue 3 April 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  14%|██████████████████▌                                                                                                                 | 8/57 [00:40<04:31,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier September 2016 (Volume 56 Issue 7).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  16%|████████████████████▊                                                                                                               | 9/57 [00:46<04:33,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERN Courier Volume 57 Issue 6 (July-August 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  18%|██████████████████████▉                                                                                                            | 10/57 [00:50<04:07,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2021MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  19%|█████████████████████████▎                                                                                                         | 11/57 [00:57<04:28,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  21%|███████████████████████████▌                                                                                                       | 12/57 [01:01<03:55,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2022JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  23%|█████████████████████████████▉                                                                                                     | 13/57 [01:05<03:39,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 57 Issue 9 (November 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  25%|████████████████████████████████▏                                                                                                  | 14/57 [01:10<03:29,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 14 chunks from CERN Courier May 2018 (Volume 53 Issue 4).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  26%|██████████████████████████████████▍                                                                                                | 15/57 [01:15<03:28,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERN Courier Volume 57 Issue 2 March 2017.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  28%|████████████████████████████████████▊                                                                                              | 16/57 [01:21<03:34,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier June 2018 (Volume 53 Issue 5).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  30%|███████████████████████████████████████                                                                                            | 17/57 [01:26<03:21,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2020NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  32%|█████████████████████████████████████████▎                                                                                         | 18/57 [01:38<04:47,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERN Courier July-August 2016 (Volume 56 Issue 6).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  33%|███████████████████████████████████████████▋                                                                                       | 19/57 [01:42<03:55,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2020JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  35%|█████████████████████████████████████████████▉                                                                                     | 20/57 [01:47<03:43,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2022SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  37%|████████████████████████████████████████████████▎                                                                                  | 21/57 [01:52<03:23,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CCJulAug19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  39%|██████████████████████████████████████████████████▌                                                                                | 22/57 [01:58<03:20,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier Volume 58 Issue 1 (Jan-Feb 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  40%|████████████████████████████████████████████████████▊                                                                              | 23/57 [02:06<03:37,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  42%|███████████████████████████████████████████████████████▏                                                                           | 24/57 [02:18<04:23,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2021JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  44%|█████████████████████████████████████████████████████████▍                                                                         | 25/57 [02:23<03:54,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 26 chunks from CERNCourier2020MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  46%|███████████████████████████████████████████████████████████▊                                                                       | 26/57 [02:30<03:35,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 18 chunks from CERN Courier Volume 57 Issue 4 .pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  47%|██████████████████████████████████████████████████████████████                                                                     | 27/57 [02:41<04:06,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier October 2016 (Volume 56 Issue 8).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  49%|████████████████████████████████████████████████████████████████▎                                                                  | 28/57 [02:47<03:44,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  51%|██████████████████████████████████████████████████████████████████▋                                                                | 29/57 [02:51<03:06,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier December 2017 (Volume 57 Issue 10).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  53%|████████████████████████████████████████████████████████████████████▉                                                              | 30/57 [02:57<02:49,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2022MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  54%|███████████████████████████████████████████████████████████████████████▏                                                           | 31/57 [03:05<02:55,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier June 2016 (Volume 56 Issue 5).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  56%|█████████████████████████████████████████████████████████████████████████▌                                                         | 32/57 [03:10<02:35,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  58%|███████████████████████████████████████████████████████████████████████████▊                                                       | 33/57 [03:16<02:29,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Volume 58 Issue 2 (March 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  60%|██████████████████████████████████████████████████████████████████████████████▏                                                    | 34/57 [03:25<02:41,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier Volume 58 Issue 3 (April 2018).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  61%|████████████████████████████████████████████████████████████████████████████████▍                                                  | 35/57 [03:31<02:29,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERN Courier Mar 2016 (Volume 56 Issue 2).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  63%|██████████████████████████████████████████████████████████████████████████████████▋                                                | 36/57 [03:41<02:44,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 chunks from CERNCourier2020JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  65%|█████████████████████████████████████████████████████████████████████████████████████                                              | 37/57 [03:46<02:16,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2023NovDec-digitaledition NEW.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|███████████████████████████████████████████████████████████████████████████████████████▎                                           | 38/57 [03:52<02:07,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  68%|█████████████████████████████████████████████████████████████████████████████████████████▋                                         | 39/57 [03:58<01:53,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2019JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  70%|███████████████████████████████████████████████████████████████████████████████████████████▉                                       | 40/57 [04:01<01:32,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 19 chunks from CERNCourier2023JanFeb-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  72%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 41/57 [04:05<01:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2024MarApr-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  74%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 42/57 [04:09<01:09,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERN Courier Volume 57 Issue 7 (September 2017).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                                | 43/57 [04:13<01:03,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021NovDec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████                              | 44/57 [04:19<01:01,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CERNCourier2020MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  79%|███████████████████████████████████████████████████████████████████████████████████████████████████████▍                           | 45/57 [04:21<00:50,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 16 chunks from CERN Courier November 2016 (Volume 56 Issue 9).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 46/57 [04:26<00:46,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2023JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 47/57 [04:30<00:41,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERNCourier2018JulAug-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                    | 48/57 [04:34<00:36,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 23 chunks from CERNCourier2020SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 49/57 [04:39<00:36,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 22 chunks from CCMayJun19-digital.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                | 50/57 [04:44<00:32,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17 chunks from CERNCourier2018Dec-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏             | 51/57 [04:49<00:27,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERNCourier2018Oct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 52/57 [04:56<00:27,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 24 chunks from CERNCourier2022MayJun-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 53/57 [05:03<00:22,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 14 chunks from CERN Courier May 2016 (Volume 56 Issue 4).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 54/57 [05:07<00:16,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERNCourier2021SepOct-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 55/57 [05:11<00:10,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 21 chunks from CERN Courier Jan-Feb 2017 (Volume 57 issue 1).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋  | 56/57 [05:16<00:04,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 15 chunks from CERNCourier2018Nov-digitaledition.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [05:21<00:00,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 20 chunks from CERNCourier2024JanFeb-digitaledition.pdf\n",
      "\n",
      "Created 1104 valid training examples\n",
      "Training data saved to finetune_data/training_data.jsonl\n",
      "\n",
      "Step 2: Submitting fine-tuning job...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file uploaded with ID: file-57Mt4LPzbyj7jMS49CcMc8\n",
      "Fine-tuning job created with ID: ftjob-8tjSyNrQ1YLXahS4y7m2V82M\n",
      "\n",
      "Monitoring fine-tuning job...\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: validating_files\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: running\n",
      "\n",
      "Status: succeeded\n",
      "Trained tokens: 6,454,262\n",
      "\n",
      "Fine-tuning completed successfully!\n",
      "Fine-tuned model ID: ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\n",
      "\n",
      "Fine-tuning process completed successfully!\n",
      "You can now use your fine-tuned model with ID: ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\n",
      "CPU times: user 5min 20s, sys: 1.74 s, total: 5min 22s\n",
      "Wall time: 53min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class FineTunePrep:\n",
    "    def __init__(self, pdf_dir=\"cern_pdfs\", output_dir=\"finetune_data\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        # Constants for token limits\n",
    "        self.MAX_TOKENS_PER_EXAMPLE = 3000  # Leave room for system and user messages\n",
    "        self.MIN_TOKENS_PER_EXAMPLE = 500   # Ensure meaningful content\n",
    "        \n",
    "    def count_tokens(self, text):\n",
    "        \"\"\"Count tokens in a text string\"\"\"\n",
    "        return len(self.tokenizer.encode(text))\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def split_into_chunks(self, text):\n",
    "        \"\"\"Split text into chunks of appropriate token length\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        # Split into sentences (roughly)\n",
    "        sentences = [s.strip() + \".\" for s in text.replace(\"\\n\", \" \").split(\".\") if s.strip()]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.count_tokens(sentence)\n",
    "            \n",
    "            # If single sentence is too long, split it into smaller parts\n",
    "            if sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                words = sentence.split()\n",
    "                temp_chunk = \"\"\n",
    "                temp_tokens = 0\n",
    "                \n",
    "                for word in words:\n",
    "                    word_tokens = self.count_tokens(word + \" \")\n",
    "                    if temp_tokens + word_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                        if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = word + \" \"\n",
    "                        temp_tokens = word_tokens\n",
    "                    else:\n",
    "                        temp_chunk += word + \" \"\n",
    "                        temp_tokens += word_tokens\n",
    "                \n",
    "                if temp_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(temp_chunk.strip())\n",
    "                continue\n",
    "            \n",
    "            # If adding this sentence would exceed limit, save current chunk and start new one\n",
    "            if current_tokens + sentence_tokens > self.MAX_TOKENS_PER_EXAMPLE:\n",
    "                if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "                current_tokens = sentence_tokens\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "                current_tokens += sentence_tokens\n",
    "        \n",
    "        # Add the last chunk if it's long enough\n",
    "        if current_tokens >= self.MIN_TOKENS_PER_EXAMPLE:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def create_training_examples(self, chunks):\n",
    "        \"\"\"Create training examples from text chunks\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Create the messages for this chunk\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"What are the key findings or developments described in this CERN research?\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"Based on the CERN publications: {chunk}\"\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Verify total tokens\n",
    "            total_tokens = sum(self.count_tokens(msg[\"content\"]) for msg in messages)\n",
    "            if total_tokens <= 4096:  # GPT-4's context window\n",
    "                examples.append({\"messages\": messages})\n",
    "            \n",
    "        return examples\n",
    "\n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Process PDFs and prepare training data\"\"\"\n",
    "        all_examples = []\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # First split text into appropriate chunks\n",
    "                chunks = self.split_into_chunks(text)\n",
    "                print(f\"\\nCreated {len(chunks)} chunks from {pdf_path.name}\")\n",
    "                \n",
    "                # Create examples from chunks\n",
    "                examples = self.create_training_examples(chunks)\n",
    "                all_examples.extend(examples)\n",
    "        \n",
    "        # Save training data\n",
    "        training_file_path = self.output_dir / \"training_data.jsonl\"\n",
    "        with open(training_file_path, 'w', encoding='utf-8') as f:\n",
    "            for example in all_examples:\n",
    "                f.write(json.dumps(example) + '\\n')\n",
    "        \n",
    "        print(f\"\\nCreated {len(all_examples)} valid training examples\")\n",
    "        print(f\"Training data saved to {training_file_path}\")\n",
    "        return training_file_path\n",
    "\n",
    "    def submit_fine_tuning_job(self, training_file_path):\n",
    "        \"\"\"Submit fine-tuning job to OpenAI\"\"\"\n",
    "        try:\n",
    "            # Upload the training file\n",
    "            with open(training_file_path, 'rb') as f:\n",
    "                training_file = self.client.files.create(\n",
    "                    file=f,\n",
    "                    purpose='fine-tune'\n",
    "                )\n",
    "            print(f\"Training file uploaded with ID: {training_file.id}\")\n",
    "            \n",
    "            # Create fine-tuning job\n",
    "            job = self.client.fine_tuning.jobs.create(\n",
    "                training_file=training_file.id,\n",
    "                model=\"gpt-4o-mini-2024-07-18\",\n",
    "                hyperparameters={\n",
    "                    \"n_epochs\": 2,\n",
    "                    \"learning_rate_multiplier\": 0.1\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Fine-tuning job created with ID: {job.id}\")\n",
    "            return job.id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error submitting fine-tuning job: {e}\")\n",
    "            return None\n",
    "\n",
    "    def monitor_fine_tuning_job(self, job_id):\n",
    "        \"\"\"Monitor the status of a fine-tuning job\"\"\"\n",
    "        print(\"\\nMonitoring fine-tuning job...\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                job = self.client.fine_tuning.jobs.retrieve(job_id)\n",
    "                print(f\"\\nStatus: {job.status}\")\n",
    "                \n",
    "                # Safely print additional info if available\n",
    "                if hasattr(job, 'trained_tokens') and job.trained_tokens is not None:\n",
    "                    print(f\"Trained tokens: {job.trained_tokens:,}\")\n",
    "                if hasattr(job, 'training_accuracy') and job.training_accuracy is not None:\n",
    "                    print(f\"Training accuracy: {job.training_accuracy:.4f}\")\n",
    "                \n",
    "                if job.status == 'succeeded':\n",
    "                    print(f\"\\nFine-tuning completed successfully!\")\n",
    "                    print(f\"Fine-tuned model ID: {job.fine_tuned_model}\")\n",
    "                    return job\n",
    "                elif job.status == 'failed':\n",
    "                    print(f\"\\nFine-tuning failed: {getattr(job, 'error', 'Unknown error')}\")\n",
    "                    return job\n",
    "                elif job.status == 'cancelled':\n",
    "                    print(\"\\nFine-tuning job was cancelled\")\n",
    "                    return job\n",
    "                \n",
    "                time.sleep(60)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking job status: {e}\")\n",
    "                time.sleep(60)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        prep = FineTunePrep()\n",
    "        \n",
    "        print(\"Step 1: Preparing training data...\")\n",
    "        training_file_path = prep.prepare_training_data()\n",
    "        \n",
    "        print(\"\\nStep 2: Submitting fine-tuning job...\")\n",
    "        job_id = prep.submit_fine_tuning_job(training_file_path)\n",
    "        \n",
    "        if job_id:\n",
    "            final_job = prep.monitor_fine_tuning_job(job_id)\n",
    "            \n",
    "            if getattr(final_job, 'status', None) == 'succeeded':\n",
    "                print(\"\\nFine-tuning process completed successfully!\")\n",
    "                print(f\"You can now use your fine-tuned model with ID: {final_job.fine_tuned_model}\")\n",
    "                \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3602ca-6c2e-4c37-bc93-4be91e88d430",
   "metadata": {},
   "source": [
    "# 5: RAG vs Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb37745d-117b-4912-a95e-1b9370c8fc2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CERN Research Assistant Comparison\n",
      "Compare Fine-tuned model vs RAG approach\n",
      "Type 'quit' to exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  Grappling with dark energy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying both models...\n",
      "\n",
      "==================================================\n",
      "Question: Grappling with dark energy\n",
      "==================================================\n",
      "\n",
      "Fine-tuned Model Response:\n",
      "------------------------------\n",
      "The nature of dark energy is one of the most profound mysteries in modern physics. The term refers to the unknown cause of the observed accelerated expansion of the universe, which was discovered in 1998 by two independent teams of astronomers studying distant supernovae. The discovery was unexpected, as the prevailing view at the time was that the expansion of the universe was slowing down due to the gravitational attraction of matter. Instead, the observations revealed that the expansion is accelerating, suggesting that some form of energy is driving the acceleration. \n",
      "\n",
      "The simplest explanation for dark energy is the cosmological constant, a term that Albert Einstein introduced in 1917 to allow for a static universe. When it was discovered that the universe is expanding, Einstein abandoned the idea, calling it his “greatest blunder”. However, the cosmological constant has made a comeback in the context of dark energy, as it is a simple and mathematically consistent explanation for the observed acceleration. \n",
      "\n",
      "The cosmological constant is a form of energy that is uniformly distributed throughout space and has a constant energy density. It has a negative pressure, which causes the expansion of the universe to accelerate. The cosmological constant is consistent with the observations of distant supernovae, the cosmic microwave background radiation, and the large-scale structure of the universe. However, it also raises a number of questions, such as why the energy density of the cosmological constant is so small compared to the energy density of matter and radiation in the universe. \n",
      "\n",
      "Another possibility for dark energy is a dynamic form of energy, such as a scalar field that evolves over time. This type of dark energy is known as quintessence, and it can have a wide range of properties depending on the form of the scalar field. Quintessence models can be designed to fit the observations of distant supernovae and other cosmological data, but they also raise questions about the nature of the scalar field and its interactions with matter. \n",
      "\n",
      "There are also other possibilities for dark energy, such as modified gravity theories that change the laws of gravity on cosmological scales. These theories can explain the observed acceleration of the universe without the need for dark energy, but they also raise questions about the nature of gravity and its relationship to the other fundamental forces of nature. \n",
      "\n",
      "Despite the many possibilities for dark energy, there is currently no consensus on its nature. The discovery of dark energy has opened up a new field of research in cosmology and particle physics, and many experiments are underway to try to understand its properties. These experiments include observations of distant supernovae, measurements of the cosmic microwave background radiation, and studies of the large-scale structure of the universe. \n",
      "\n",
      "In addition to these observational efforts, there are also theoretical efforts to understand dark energy. These efforts include the development of new models of dark energy, the study of the implications of dark energy for the early universe, and the exploration of the connections between dark energy and other areas of physics, such as particle physics and string theory. \n",
      "\n",
      "The nature of dark energy is one of the most important questions in modern physics, and its resolution could have profound implications for our understanding of the universe. As our understanding of dark energy continues to evolve, it is likely that new ideas and insights will emerge that will help to shed light on this fundamental mystery.\n",
      "Response time: 9.33 seconds\n",
      "\n",
      "RAG System Response:\n",
      "------------------------------\n",
      "The articles from the CERN Courier provide insights into the challenges and mysteries surrounding dark energy, a significant component of our universe. Dark energy is primarily associated with the observation that the universe is experiencing accelerated expansion. This phenomenon suggests that the universe contains an enormous amount of additional energy, which is not accounted for by ordinary matter or dark matter (CERN Courier Volume 57 Issue 4).\n",
      "\n",
      "Despite its profound implications, dark energy remains one of the enigmas of modern physics. It is part of the broader context of gravity-related mysteries, which include dark matter and the quest to understand how gravity relates to the other fundamental forces (CERN Courier Jan-Feb 2017, Volume 57 Issue 1). The articles highlight that while general relativity (GR) has been successful in describing gravitational phenomena, it also presents theoretical challenges, such as the formation of singularities and the non-renormalizability of quantum gravity. These challenges suggest that a suitable extension of GR might be necessary to fully understand dark energy and other gravitational phenomena (CERN Courier Jan-Feb 2017, Volume 57 Issue 1).\n",
      "\n",
      "In summary, dark energy is a critical area of research in modern physics, linked to the accelerated expansion of the universe and the need for a deeper understanding of gravity. The articles emphasize the importance of continuing experimental and theoretical efforts to unravel these mysteries.\n",
      "Response time: 6.13 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(self, fine_tuned_model_id, db_dir=\"cern_vectordb\"):\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "            \n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        template = \"\"\"You are a helpful research assistant with access to CERN Courier articles.\n",
    "        Use the following articles to answer the question. If you can't answer the question based\n",
    "        on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Please provide a detailed answer with specific references to the articles when possible:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def query_fine_tuned_model(self, question):\n",
    "        \"\"\"Query the fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics, trained to provide accurate information from CERN publications.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying fine-tuned model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.rag_chain.invoke(question)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from both approaches\"\"\"\n",
    "        print(\"\\nQuerying both models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        ft_result = self.query_fine_tuned_model(question)\n",
    "        rag_result = self.query_rag(question)\n",
    "        \n",
    "        # Print comparison\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Question:\", question)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nFine-tuned Model Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\nRAG System Response:\")\n",
    "        print(\"-\"*30)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'fine_tuned': ft_result,\n",
    "            'rag': rag_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Replace with your fine-tuned model ID\n",
    "    FINE_TUNED_MODEL_ID = \"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\"\n",
    "    \n",
    "    try:\n",
    "        comparison = ModelComparison(FINE_TUNED_MODEL_ID)\n",
    "        \n",
    "        print(\"CERN Research Assistant Comparison\")\n",
    "        print(\"Compare Fine-tuned model vs RAG approach\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \")\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7310b5d-bf94-4ab8-946b-3cbee47f82f8",
   "metadata": {},
   "source": [
    "# 6: Fine Tunging on nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f8163c-ed3a-4657-9df0-0e962b26fb74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial GPU memory usage:\n",
      "GPU 0: 380.31MB / 24576.00MB (1.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:359: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/home/lence/anaconda3/lib/python3.11/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final GPU memory usage after cleaning:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n",
      "Using GPU: NVIDIA GeForce RTX 3090\n",
      "Available GPU memory: 25.44 GB\n",
      "\n",
      "Initial GPU memory usage:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n",
      "\n",
      "Final GPU memory usage after cleaning:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n",
      "Initializing fine-tuner...\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with optimized quantization...\n",
      "\n",
      "Initial GPU memory usage:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n",
      "\n",
      "Final GPU memory usage after cleaning:\n",
      "GPU 0: 637.25MB / 24576.00MB (2.59%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f03be55e434e1e85dd71c2fc664d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=True, split_batches=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model parameters:\n",
      "trainable params: 1050939392 || all params: 4540600320 || trainable%: 23.15\n",
      "Preparing model for QLoRA training...\n",
      "Applying LoRA...\n",
      "trainable params: 3407872 || all params: 4544008192 || trainable%: 0.07\n",
      "Preparing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 57/57 [05:19<00:00,  5.61s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07469eb1dce44b2ab89d6aa52ee8857d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:54, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import gc\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlDeviceGetCount,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    NVMLError\n",
    ")\n",
    "\n",
    "def clean_gpu_memory():\n",
    "    \"\"\"\n",
    "    Thoroughly clean GPU memory and print memory usage statistics\n",
    "    \"\"\"\n",
    "    def get_gpu_memory_usage():\n",
    "        try:\n",
    "            nvmlInit()\n",
    "            deviceCount = nvmlDeviceGetCount()\n",
    "            memory_usage = []\n",
    "            \n",
    "            for i in range(deviceCount):\n",
    "                handle = nvmlDeviceGetHandleByIndex(i)\n",
    "                info = nvmlDeviceGetMemoryInfo(handle)\n",
    "                memory_usage.append({\n",
    "                    'device': i,\n",
    "                    'used_mb': info.used / 1024**2,\n",
    "                    'total_mb': info.total / 1024**2,\n",
    "                    'used_percent': (info.used / info.total) * 100\n",
    "                })\n",
    "            \n",
    "            return memory_usage\n",
    "        except NVMLError as e:\n",
    "            print(f\"NVML Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Print initial memory usage\n",
    "    memory_usage = get_gpu_memory_usage()\n",
    "    if memory_usage:\n",
    "        print(\"\\nInitial GPU memory usage:\")\n",
    "        for gpu in memory_usage:\n",
    "            print(f\"GPU {gpu['device']}: {gpu['used_mb']:.2f}MB / {gpu['total_mb']:.2f}MB ({gpu['used_percent']:.2f}%)\")\n",
    "\n",
    "    # Empty CUDA cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Delete all variables in CUDA memory\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if obj.is_cuda:\n",
    "                    del obj\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Run garbage collector\n",
    "    gc.collect()\n",
    "    \n",
    "    # Reset peak memory stats\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Print final memory usage\n",
    "    memory_usage = get_gpu_memory_usage()\n",
    "    if memory_usage:\n",
    "        print(\"\\nFinal GPU memory usage after cleaning:\")\n",
    "        for gpu in memory_usage:\n",
    "            print(f\"GPU {gpu['device']}: {gpu['used_mb']:.2f}MB / {gpu['total_mb']:.2f}MB ({gpu['used_percent']:.2f}%)\")\n",
    "\n",
    "# Set environment variable for memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64,garbage_collection_threshold:0.8\"\n",
    "clean_gpu_memory()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "class LlamaFineTuner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        cache_dir=None,\n",
    "        pdf_dir=\"cern_pdfs\",\n",
    "        output_dir=\"llama_finetuned\",\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        self.logging_dir = self.output_dir / \"logs\"\n",
    "        self.logging_dir.mkdir(exist_ok=True)\n",
    "        self.device = device\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        try:\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=self.cache_dir,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            \n",
    "            print(\"Loading model with optimized quantization...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                load_in_8bit_fp32_cpu_offload=True\n",
    "            )\n",
    "            \n",
    "            # Calculate available GPU memory\n",
    "            clean_gpu_memory()  # Clean memory before loading model\n",
    "            gpu_memory = int(torch.cuda.get_device_properties(0).total_memory/1e9) - 2\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\",\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                max_memory={0: f\"{int(gpu_memory*0.7)}GB\", \"cpu\": \"32GB\"},\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            # Enable gradient checkpointing\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "            self.model.enable_input_require_grads()\n",
    "            self.model.config.use_cache = False\n",
    "            \n",
    "            print(\"Initial model parameters:\")\n",
    "            print_trainable_parameters(self.model)\n",
    "            \n",
    "            print(\"Preparing model for QLoRA training...\")\n",
    "            self.model = prepare_model_for_kbit_training(self.model)\n",
    "            \n",
    "            # Configure LoRA\n",
    "            lora_config = LoraConfig(\n",
    "                r=8,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"q_proj\", \"v_proj\"],\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM\n",
    "            )\n",
    "            \n",
    "            print(\"Applying LoRA...\")\n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "            print_trainable_parameters(self.model)\n",
    "            \n",
    "            # Initialize accelerator\n",
    "            self.accelerator = Accelerator(\n",
    "                gradient_accumulation_steps=32,\n",
    "                mixed_precision=\"bf16\",\n",
    "                project_dir=str(self.logging_dir),\n",
    "                split_batches=True,\n",
    "                dispatch_batches=True\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error initializing model and tokenizer: {str(e)}\")\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from a PDF file.\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_training_data(self, max_length=512):\n",
    "        \"\"\"Prepare training data from PDF files.\"\"\"\n",
    "        if not self.pdf_dir.exists():\n",
    "            raise ValueError(f\"PDF directory not found: {self.pdf_dir}\")\n",
    "        \n",
    "        training_data = []\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            raise ValueError(f\"No PDF files found in {self.pdf_dir}\")\n",
    "        \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            text = self.extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                entry = {\n",
    "                    \"instruction\": \"Analyze and explain the following research:\",\n",
    "                    \"input\": text,\n",
    "                    \"output\": \"This research discusses: \" + text[:500]\n",
    "                }\n",
    "                training_data.append(entry)\n",
    "        \n",
    "        if not training_data:\n",
    "            raise ValueError(\"No valid training data could be extracted from PDFs\")\n",
    "        \n",
    "        dataset = Dataset.from_list(training_data)\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            prompts = []\n",
    "            for inst, inp in zip(examples[\"instruction\"], examples[\"input\"]):\n",
    "                prompt = f\"[INST] {inst}\\n{inp} [/INST]\"\n",
    "                prompts.append(prompt)\n",
    "            \n",
    "            return self.tokenizer(\n",
    "                prompts,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        return dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "\n",
    "    def train(self, num_epochs=3, batch_size=1):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        try:\n",
    "            print(\"Preparing training data...\")\n",
    "            train_dataset = self.prepare_training_data()\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=str(self.output_dir),\n",
    "                num_train_epochs=num_epochs,\n",
    "                per_device_train_batch_size=batch_size,\n",
    "                gradient_accumulation_steps=32,\n",
    "                warmup_steps=50,\n",
    "                logging_steps=10,\n",
    "                save_steps=100,\n",
    "                learning_rate=2e-4,\n",
    "                bf16=True,\n",
    "                optim=\"paged_adamw_32bit\",\n",
    "                logging_dir=str(self.logging_dir),\n",
    "                group_by_length=True,\n",
    "                gradient_checkpointing=True,\n",
    "                max_grad_norm=0.3,\n",
    "                save_total_limit=2,\n",
    "                evaluation_strategy=\"no\",\n",
    "                report_to=\"tensorboard\",\n",
    "                remove_unused_columns=False,\n",
    "                lr_scheduler_type=\"cosine\",\n",
    "                weight_decay=0.01\n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                data_collator=DataCollatorForLanguageModeling(\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    mlm=False\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            print(\"Starting training...\")\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                trainer.train()\n",
    "            \n",
    "            print(\"Saving model...\")\n",
    "            trainer.save_model(str(self.output_dir / \"final_model\"))\n",
    "            self.tokenizer.save_pretrained(\n",
    "                str(self.output_dir / \"final_model\"),\n",
    "                safe_serialization=True\n",
    "            )\n",
    "            \n",
    "            print(\"Training complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during training: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"This script requires a CUDA-capable GPU\")\n",
    "        \n",
    "        # Print GPU info\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.2f} GB\")\n",
    "        \n",
    "        # Clean GPU memory before starting\n",
    "        clean_gpu_memory()\n",
    "        \n",
    "        # Set memory optimizations\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.cuda.set_per_process_memory_fraction(0.75)\n",
    "        \n",
    "        # Create cache directory\n",
    "        cache_dir = Path.home() / \".cache\" / \"huggingface\"\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize and train\n",
    "        print(\"Initializing fine-tuner...\")\n",
    "        finetuner = LlamaFineTuner(cache_dir=str(cache_dir))\n",
    "        finetuner.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cce55d-cebc-4ef2-9741-1eae8a3d3ac2",
   "metadata": {},
   "source": [
    "# 7: Chat with the Fine-Tuned nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba6372d-e50d-491b-9876-5bcf6a3e60d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing chat model...\n",
      "Loading tokenizer...\n",
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70983702b27482387c12aa633870e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned adapters...\n",
      "Model loaded successfully!\n",
      "Model is on device: cuda:0\n",
      "\n",
      "Starting chat session with the fine-tuned model\n",
      "Type 'quit' to exit, 'clear' to clear the conversation\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Grappling with dark energy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Debug - Prompt: [INST] Grappling with dark energy [/INST]\n",
      "Debug - Input shape: torch.Size([1, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 16:18:19.077102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-09 16:18:19.088063: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-09 16:18:19.091416: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 16:18:19.100970: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-09 16:18:19.810382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug - Output shape: torch.Size([1, 13])\n",
      "Debug - Raw response: [INST] Grappling with dark energy [/INST]\n",
      "Debug - Cleaned response: \n",
      "\n",
      "Model: No response generated. Please try again.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  what is Grappling with dark energy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Debug - Prompt: [INST] what is Grappling with dark energy [/INST]\n",
      "Debug - Input shape: torch.Size([1, 14])\n",
      "Debug - Output shape: torch.Size([1, 15])\n",
      "Debug - Raw response: [INST] what is Grappling with dark energy [/INST]\n",
      "Debug - Cleaned response: \n",
      "\n",
      "Model: No response generated. Please try again.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  What is driving the accelerated expansion of the Universe?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "Debug - Prompt: [INST] What is driving the accelerated expansion of the Universe? [/INST]\n",
      "Debug - Input shape: torch.Size([1, 16])\n",
      "Debug - Output shape: torch.Size([1, 58])\n",
      "Debug - Raw response: [INST] What is driving the accelerated expansion of the Universe? [/INST] [INST] Can we test the nature of dark energy with future cosmology surveys? [/INST] [INST] Do we finally understand the origin of the large scale structure of the Universe? [/INST]\n",
      "Debug - Cleaned response: [INST] Can we test the nature of dark energy with future cosmology surveys? [/INST] [INST] Do we finally understand the origin of the large scale structure of the Universe? [/INST]\n",
      "\n",
      "Model: [INST] Can we test the nature of dark energy with future cosmology surveys? [/INST] [INST] Do we finally understand the origin of the large scale structure of the Universe? [/INST]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelChat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        finetuned_path=\"llama_finetuned/final_model\",\n",
    "        max_sequence_length=2048\n",
    "    ):\n",
    "        try:\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                base_model_name,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            # Set padding parameters\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            self.max_sequence_length = max_sequence_length\n",
    "            \n",
    "            print(\"Loading base model...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            print(\"Loading fine-tuned adapters...\")\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                finetuned_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            self.model.eval()\n",
    "            print(\"Model loaded successfully!\")\n",
    "            print(f\"Model is on device: {self.model.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error initializing model: {str(e)}\")\n",
    "    \n",
    "    def generate_response(self, instruction, max_new_tokens=256, temperature=0.7):\n",
    "        try:\n",
    "            # Format the input\n",
    "            prompt = f\"[INST] {instruction} [/INST]\"\n",
    "            print(f\"\\nDebug - Prompt: {prompt}\")\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_sequence_length\n",
    "            )\n",
    "            print(f\"Debug - Input shape: {inputs.input_ids.shape}\")\n",
    "            \n",
    "            # Move to GPU\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate with minimal parameters first\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            print(f\"Debug - Output shape: {outputs.shape}\")\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"Debug - Raw response: {response}\")\n",
    "            \n",
    "            # Clean response\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "            print(f\"Debug - Cleaned response: {response}\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Debug - Error in generate_response: {str(e)}\")\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "    \n",
    "    def chat(self):\n",
    "        print(\"\\nStarting chat session with the fine-tuned model\")\n",
    "        print(\"Type 'quit' to exit, 'clear' to clear the conversation\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\nYou: \").strip()\n",
    "                \n",
    "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "                    print(\"\\nGoodbye!\")\n",
    "                    break\n",
    "                \n",
    "                if not user_input:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\nGenerating response...\")\n",
    "                response = self.generate_response(user_input)\n",
    "                \n",
    "                if response:\n",
    "                    print(f\"\\nModel: {response}\")\n",
    "                else:\n",
    "                    print(\"\\nModel: No response generated. Please try again.\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nInterrupted by user. Type 'quit' to exit or continue chatting.\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during chat: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        if not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"This script requires a CUDA-capable GPU\")\n",
    "        \n",
    "        print(\"\\nInitializing chat model...\")\n",
    "        chat_model = ModelChat()\n",
    "        chat_model.chat()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b410781-1b8c-4997-9697-18a7c1241e78",
   "metadata": {},
   "source": [
    "# 8: Now compaire OpenAI RAG vs OpenAI Fine-Tune vs nvidia/Llama3-ChatQA-1.5-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec084a0c-6090-4200-bba7-9b6f9e818da1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model Comparison System...\n",
      "Initializing RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_395023/2066110667.py:37: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a93bd5e484f40129b2597c640004881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA model loaded successfully!\n",
      "\n",
      "CERN Research Assistant - Model Comparison\n",
      "Compare: RAG vs Fine-tuned GPT-4 vs Fine-tuned LLaMA\n",
      "Type 'quit' to exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  What is driving the accelerated expansion of the Universe?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your question across all models...\n",
      "\n",
      "================================================================================\n",
      "Question: What is driving the accelerated expansion of the Universe?\n",
      "================================================================================\n",
      "\n",
      "1. RAG System (GPT-4 + CERN Articles)\n",
      "--------------------------------------------------\n",
      "The accelerated expansion of the Universe is broadly attributed to what is referred to as dark energy, as mentioned in the first document. However, the exact source and physics of dark energy remain unknown and are an ongoing area of research. One idea is that the cosmological constant, initially introduced by Einstein, is linked to the vacuum energy. However, this can't be the final answer because summing the contributions from the presumed quantum states in the universe produces a number for the expansion rate that is significantly higher than observed. The fourth document also mentions that the nature of this so-called dark energy is one of the most important puzzles in modern fundamental physics. It could be due to a cosmological constant, a new dynamical field, a deviation from general relativity on cosmological scales, or something else.\n",
      "Response time: 11.27 seconds\n",
      "\n",
      "2. Fine-tuned GPT-4\n",
      "--------------------------------------------------\n",
      "The accelerated expansion of the Universe is primarily attributed to a mysterious form of energy known as dark energy. Observations, particularly those involving distant supernovae, indicate that the expansion of the Universe is not only continuing but is actually accelerating over time. This discovery, made in the late 1990s, was unexpected and led to the proposal that a form of energy permeates all of space and has a repulsive effect, counteracting the attractive force of gravity on cosmological scales.\n",
      "\n",
      "Dark energy is often modeled as a cosmological constant (Λ), which was originally introduced by Albert Einstein in his equations of General Relativity. This constant represents a uniform energy density filling space homogeneously. While it fits well with current observations, the nature of dark energy remains one of the biggest unsolved problems in cosmology.\n",
      "\n",
      "There are also alternative theories that attempt to explain the accelerated expansion, such as modifications to General Relativity or the existence of dynamic scalar fields (like quintessence) that vary in time and space. However, these theories have not gained as much support as the cosmological constant model.\n",
      "\n",
      "In summary, while dark energy is the leading explanation for the accelerated expansion of the Universe, its exact nature is still unknown, and ongoing research in cosmology and particle physics aims to shed light on this fundamental question.\n",
      "Response time: 3.25 seconds\n",
      "\n",
      "3. Fine-tuned LLaMA (with LoRA)\n",
      "--------------------------------------------------\n",
      "[INST] There are multiple theories, including dark energy. But our observations have not yet ruled out the possibility that this acceleration is due to new physics beyond the Standard Model. If new physics is indeed responsible for the observed accelerated expansion, then it must be happening at extremely low energies and only affect the very largest scales in the universe. This makes it very difficult to detect with current experiments. [/INST]\n",
      "+ [[Category:Accelerated Expansion]]\n",
      "The accelerating expansion of the universe has been inferred from two independent measurements.\n",
      "The first measurement, by Adam Riess, led to the discovery of a mysterious force called dark energy. The second measurement, made 10 years later by a different team of astronomers using the same technique, confirmed the first result. The two teams were completely separate. They didn't know about each other's work until both had published their results in peer-reviewed journals.\n",
      "These two measurements have been independently verified several times over the past two decades.\n",
      "Response time: 7.90 seconds\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fine_tuned_model_id=\"ft:gpt-4o-mini-2024-07-18:personal::AbZrBIYn\",\n",
    "        base_model_name=\"nvidia/Llama3-ChatQA-1.5-8B\",\n",
    "        finetuned_path=\"llama_finetuned/final_model\",\n",
    "        db_dir=\"cern_vectordb\"\n",
    "    ):\n",
    "        load_dotenv()\n",
    "        if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI()\n",
    "        self.fine_tuned_model_id = fine_tuned_model_id\n",
    "        \n",
    "        # Initialize RAG components\n",
    "        print(\"Initializing RAG system...\")\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=OpenAIEmbeddings()\n",
    "        )\n",
    "        \n",
    "        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "        \n",
    "        self.retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}\n",
    "        )\n",
    "        \n",
    "        # Setup RAG prompt template\n",
    "        template = \"\"\"You are a CERN research assistant. Use the following articles to answer the question.\n",
    "        If you cannot answer based on the articles, say so clearly.\n",
    "\n",
    "        Context articles:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer with specific references to the articles:\"\"\"\n",
    "        \n",
    "        self.prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        # Setup RAG chain\n",
    "        self.rag_chain = (\n",
    "            RunnableParallel(\n",
    "                {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            )\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # Initialize LLaMA model\n",
    "        print(\"Loading LLaMA model...\")\n",
    "        try:\n",
    "            # Initialize tokenizer\n",
    "            self.llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                base_model_name,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            \n",
    "            if self.llama_tokenizer.pad_token is None:\n",
    "                self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token\n",
    "            \n",
    "            # Setup quantization\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            \n",
    "            # Load base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=quantization_config,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            # Load LoRA adapters\n",
    "            self.llama_model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                finetuned_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            self.llama_model.eval()\n",
    "            print(\"LLaMA model loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load LLaMA model: {e}\")\n",
    "            self.llama_model = None\n",
    "            self.llama_tokenizer = None\n",
    "\n",
    "    def query_llama(self, question, max_length=512):\n",
    "        \"\"\"Query the LoRA-fine-tuned LLaMA model\"\"\"\n",
    "        if not self.llama_model or not self.llama_tokenizer:\n",
    "            return {\n",
    "                'response': \"LLaMA model not available\",\n",
    "                'time': 0\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Prepare input\n",
    "            prompt = f\"[INST] {question} [/INST]\"\n",
    "            inputs = self.llama_tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=True\n",
    "            ).to(self.llama_model.device)\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.llama_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_length,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95,\n",
    "                    repetition_penalty=1.1,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.llama_tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.llama_tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            response = self.llama_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response.replace(prompt, \"\").strip()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying LLaMA model: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_fine_tuned_gpt(self, question):\n",
    "        \"\"\"Query the OpenAI fine-tuned model\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.fine_tuned_model_id,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert on CERN and particle physics research.\"},\n",
    "                    {\"role\": \"user\", \"content\": question}\n",
    "                ],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response.choices[0].message.content,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying fine-tuned GPT: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def query_rag(self, question):\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.rag_chain.invoke(question)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'time': end_time - start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'response': f\"Error querying RAG system: {e}\",\n",
    "                'time': 0\n",
    "            }\n",
    "\n",
    "    def compare_responses(self, question):\n",
    "        \"\"\"Compare responses from all three models\"\"\"\n",
    "        print(\"\\nProcessing your question across all models...\")\n",
    "        \n",
    "        # Get responses\n",
    "        rag_result = self.query_rag(question)\n",
    "        ft_result = self.query_fine_tuned_gpt(question)\n",
    "        llama_result = self.query_llama(question)\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\n1. RAG System (GPT-4 + CERN Articles)\")\n",
    "        print(\"-\"*50)\n",
    "        print(rag_result['response'])\n",
    "        print(f\"Response time: {rag_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\n2. Fine-tuned GPT-4\")\n",
    "        print(\"-\"*50)\n",
    "        print(ft_result['response'])\n",
    "        print(f\"Response time: {ft_result['time']:.2f} seconds\")\n",
    "        \n",
    "        print(\"\\n3. Fine-tuned LLaMA (with LoRA)\")\n",
    "        print(\"-\"*50)\n",
    "        print(llama_result['response'])\n",
    "        print(f\"Response time: {llama_result['time']:.2f} seconds\")\n",
    "        \n",
    "        return {\n",
    "            'rag': rag_result,\n",
    "            'fine_tuned_gpt': ft_result,\n",
    "            'llama': llama_result\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Initializing Model Comparison System...\")\n",
    "        comparison = ModelComparison()\n",
    "        \n",
    "        print(\"\\nCERN Research Assistant - Model Comparison\")\n",
    "        print(\"Compare: RAG vs Fine-tuned GPT-4 vs Fine-tuned LLaMA\")\n",
    "        print(\"Type 'quit' to exit\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nYour question: \").strip()\n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            if not question:\n",
    "                continue\n",
    "                \n",
    "            comparison.compare_responses(question)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88e812-7685-4307-b646-5eda86f2ffa2",
   "metadata": {},
   "source": [
    "# 9: Build a LLM from scratch on the CERN PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6a4b4-2927-40a3-8307-0d05e7af3acf",
   "metadata": {},
   "source": [
    "## Cell 1: Config and Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa7adc8-21da-48ad-aff8-d072de0e3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "import wandb\n",
    "import gc\n",
    "import os\n",
    "\n",
    "class TransformerConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 32000,\n",
    "        max_sequence_length: int = 512,\n",
    "        d_model: int = 768,\n",
    "        n_heads: int = 12,\n",
    "        n_layers: int = 6,\n",
    "        d_ff: int = 3072,\n",
    "        dropout: float = 0.1,\n",
    "        pad_token_id: int = 0\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        assert self.d_model % self.n_heads == 0\n",
    "        \n",
    "        self.d_k = self.d_model // self.n_heads\n",
    "        self.w_q = nn.Linear(config.d_model, config.d_model)\n",
    "        self.w_k = nn.Linear(config.d_model, config.d_model)\n",
    "        self.w_v = nn.Linear(config.d_model, config.d_model)\n",
    "        self.w_o = nn.Linear(config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def attention(self, q, k, v, mask=None):\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        return torch.matmul(attn, v), attn\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        q = self.w_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        x, attn = self.attention(q, k, v, mask)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6cfb4-4248-4425-9b7a-8a48782cc2c3",
   "metadata": {},
   "source": [
    "## Cell 2: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89caddb9-38a3-47a7-9726-9fbde08d4b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.gelu(self.fc1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.norm1 = nn.LayerNorm(config.d_model)\n",
    "        self.norm2 = nn.LayerNorm(config.d_model)\n",
    "        self.feed_forward = PositionwiseFeedForward(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(config.max_sequence_length, config.d_model)\n",
    "        position = torch.arange(0, config.max_sequence_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, config.d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / config.d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CustomLLM(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.positional_encoding = PositionalEncoding(config)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_layer_norm = nn.LayerNorm(config.d_model)\n",
    "        self.output_projection = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, attention_mask)\n",
    "            \n",
    "        x = self.final_layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349f140-ebaa-4672-aa46-6ee6547a9fa9",
   "metadata": {},
   "source": [
    "## Cell 3: Dataset and Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b6f0665-2ed3-4573-8e70-68eb5e588030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CERNDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdf_dir: str,\n",
    "        tokenizer_name: str = \"gpt2\",\n",
    "        max_length: int = 512,\n",
    "        min_text_length: int = 100\n",
    "    ):\n",
    "        self.pdf_dir = Path(pdf_dir)\n",
    "        if not self.pdf_dir.exists():\n",
    "            raise ValueError(f\"PDF directory {pdf_dir} does not exist\")\n",
    "            \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "                logging.info(\"Set pad_token to eos_token\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load tokenizer: {str(e)}\")\n",
    "            \n",
    "        self.max_length = max_length\n",
    "        self.min_text_length = min_text_length\n",
    "        \n",
    "        # Process PDFs\n",
    "        self.texts = []\n",
    "        self._process_pdfs()\n",
    "        \n",
    "        if not self.texts:\n",
    "            raise ValueError(f\"No valid texts found in {pdf_dir}\")\n",
    "            \n",
    "        # Process texts in batches\n",
    "        self._tokenize_texts()\n",
    "        \n",
    "    def _tokenize_texts(self):\n",
    "        self.input_ids = []\n",
    "        self.attention_masks = []\n",
    "        \n",
    "        batch_size = 32\n",
    "        for i in tqdm(range(0, len(self.texts), batch_size), desc=\"Tokenizing texts\"):\n",
    "            batch_texts = self.texts[i:i + batch_size]\n",
    "            try:\n",
    "                encodings = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=self.max_length,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                self.input_ids.extend(encodings['input_ids'])\n",
    "                self.attention_masks.extend(encodings['attention_mask'])\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error tokenizing batch {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not self.input_ids:\n",
    "            raise ValueError(\"No texts were successfully tokenized\")\n",
    "            \n",
    "        try:\n",
    "            self.input_ids = torch.stack([t for t in self.input_ids if isinstance(t, torch.Tensor)])\n",
    "            self.attention_masks = torch.stack([t for t in self.attention_masks if isinstance(t, torch.Tensor)])\n",
    "            logging.info(f\"Successfully processed {len(self.input_ids)} sequences\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to stack tensors: {str(e)}\")\n",
    "        \n",
    "    def _process_pdfs(self):\n",
    "        pdf_files = list(self.pdf_dir.glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            raise ValueError(f\"No PDF files found in {self.pdf_dir}\")\n",
    "            \n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            try:\n",
    "                with open(pdf_path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    text = \"\"\n",
    "                    for page in reader.pages:\n",
    "                        text += page.extract_text() + \"\\n\"\n",
    "                    \n",
    "                    words = text.split()\n",
    "                    chunk_size = self.max_length * 4\n",
    "                    \n",
    "                    for i in range(0, len(words), chunk_size):\n",
    "                        chunk = \" \".join(words[i:i + chunk_size])\n",
    "                        if len(chunk.strip()) >= self.min_text_length:\n",
    "                            self.texts.append(chunk)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error processing {pdf_path}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': torch.roll(self.input_ids[idx], -1)\n",
    "        }\n",
    "        item['labels'][-1] = self.tokenizer.pad_token_id\n",
    "        return item\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: CustomLLM,\n",
    "        train_dataset: CERNDataset,\n",
    "        val_dataset: Optional[CERNDataset] = None,\n",
    "        batch_size: int = 8,\n",
    "        learning_rate: float = 3e-4,\n",
    "        num_epochs: int = 10,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        wandb_project: Optional[str] = None,\n",
    "        checkpoint_dir: str = \"checkpoints\"\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.num_epochs = num_epochs\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            self.train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            self.val_loader = None\n",
    "            if val_dataset:\n",
    "                self.val_loader = DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4,\n",
    "                    pin_memory=True\n",
    "                )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to create data loaders: {str(e)}\")\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.95),\n",
    "            weight_decay=0.1\n",
    "        )\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=len(self.train_loader) * num_epochs\n",
    "        )\n",
    "        \n",
    "        if wandb_project:\n",
    "            try:\n",
    "                wandb.init(project=wandb_project)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to initialize wandb: {str(e)}\")\n",
    "                \n",
    "    def save_checkpoint(self, epoch: int, loss: float, is_best: bool = False):\n",
    "        try:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                'loss': loss\n",
    "            }\n",
    "            \n",
    "            filename = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "            torch.save(checkpoint, filename)\n",
    "            \n",
    "            if is_best:\n",
    "                best_filename = self.checkpoint_dir / 'best_model.pt'\n",
    "                torch.save(checkpoint, best_filename)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save checkpoint: {str(e)}\")\n",
    "            \n",
    "    def train(self):\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(self.num_epochs):\n",
    "                self.model.train()\n",
    "                total_loss = 0\n",
    "                train_pbar = tqdm(self.train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "                \n",
    "                for batch in train_pbar:\n",
    "                    try:\n",
    "                        input_ids = batch['input_ids'].to(self.device)\n",
    "                        attention_mask = batch['attention_mask'].to(self.device)\n",
    "                        labels = batch['labels'].to(self.device)\n",
    "                        \n",
    "                        self.optimizer.zero_grad()\n",
    "                        outputs = self.model(input_ids, attention_mask)\n",
    "                        \n",
    "                        loss = F.cross_entropy(\n",
    "                            outputs.view(-1, outputs.size(-1)),\n",
    "                            labels.view(-1),\n",
    "                            ignore_index=self.model.config.pad_token_id\n",
    "                        )\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                        self.optimizer.step()\n",
    "                        self.scheduler.step()\n",
    "                        \n",
    "                        total_loss += loss.item()\n",
    "                        train_pbar.set_postfix({'loss': loss.item()})\n",
    "                        \n",
    "                        if wandb.run:\n",
    "                            wandb.log({\n",
    "                                'train_batch_loss': loss.item(),\n",
    "                                'learning_rate': self.scheduler.get_last_lr()[0]\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error in training batch: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                avg_train_loss = total_loss / len(self.train_loader)\n",
    "                logging.info(f\"Epoch {epoch+1} average training loss: {avg_train_loss:.4f}\")\n",
    "                \n",
    "                if self.val_loader:\n",
    "                    val_loss = self.evaluate()\n",
    "                    logging.info(f\"Epoch {epoch+1} validation loss: {val_loss:.4f}\")\n",
    "                    \n",
    "                    is_best = val_loss < best_val_loss\n",
    "                    if is_best:\n",
    "                        best_val_loss = val_loss\n",
    "                    \n",
    "                    self.save_checkpoint(epoch + 1, val_loss, is_best)\n",
    "                \n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Training failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        n_batches = len(self.val_loader)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Evaluating\"):\n",
    "                try:\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    labels = batch['labels'].to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(input_ids, attention_mask)\n",
    "                    loss = F.cross_entropy(\n",
    "                        outputs.view(-1, outputs.size(-1)),\n",
    "                        labels.view(-1),\n",
    "                        ignore_index=self.model.config.pad_token_id\n",
    "                    )\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error in validation batch: {str(e)}\")\n",
    "                    n_batches -= 1\n",
    "                    continue\n",
    "        \n",
    "        if n_batches == 0:\n",
    "            raise RuntimeError(\"No valid batches in validation\")\n",
    "            \n",
    "        return total_loss / n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d60a0-95c4-47b9-95d3-145141ada2e4",
   "metadata": {},
   "source": [
    "## Cell 4: Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95229673-d4b5-4f92-a663-e00ec4b20fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 08:44:30,910 - INFO - Creating datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 0 files to train/\n",
      "Moved 0 files to val/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lence/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2024-12-11 08:44:31,189 - INFO - Set pad_token to eos_token\n",
      "Processing PDFs: 100%|████████████████████████████████████████████████████████████████████████████████| 45/45 [04:22<00:00,  5.83s/it]\n",
      "Tokenizing texts: 100%|███████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 32.45it/s]\n",
      "2024-12-11 08:48:54,170 - INFO - Successfully processed 841 sequences\n",
      "2024-12-11 08:48:54,401 - INFO - Set pad_token to eos_token\n",
      "Processing PDFs: 100%|████████████████████████████████████████████████████████████████████████████████| 12/12 [01:07<00:00,  5.60s/it]\n",
      "Tokenizing texts: 100%|█████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 36.63it/s]\n",
      "2024-12-11 08:50:01,870 - INFO - Successfully processed 226 sequences\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mikdataml\u001b[0m (\u001b[33mikdataml-lence\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/IK/00_Projects/CERN/wandb/run-20241211_085003-4iwcvwx1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ikdataml-lence/cern-llm/runs/4iwcvwx1' target=\"_blank\">ruby-energy-1</a></strong> to <a href='https://wandb.ai/ikdataml-lence/cern-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ikdataml-lence/cern-llm' target=\"_blank\">https://wandb.ai/ikdataml-lence/cern-llm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ikdataml-lence/cern-llm/runs/4iwcvwx1' target=\"_blank\">https://wandb.ai/ikdataml-lence/cern-llm/runs/4iwcvwx1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 1: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.95]\n",
      "2024-12-11 08:50:43,644 - INFO - Epoch 1 average training loss: 7.8084\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  7.95it/s]\n",
      "2024-12-11 08:50:47,292 - INFO - Epoch 1 validation loss: 7.5778\n",
      "Training Epoch 2:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 2:  99%|█████████████████████████████████████████████████████████████████▍| 105/106 [00:39<00:00,  2.66it/s, loss=7.55]=(true | false)\n",
      "Training Epoch 2: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.55]\n",
      "2024-12-11 08:51:30,654 - INFO - Epoch 2 average training loss: 7.5018\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  7.94it/s]\n",
      "2024-12-11 08:51:34,308 - INFO - Epoch 2 validation loss: 7.5913\n",
      "Training Epoch 3:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 3: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.48]\n",
      "2024-12-11 08:52:16,106 - INFO - Epoch 3 average training loss: 7.4776\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  7.99it/s]\n",
      "2024-12-11 08:52:19,737 - INFO - Epoch 3 validation loss: 7.6187\n",
      "Training Epoch 4:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 4: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.31]\n",
      "2024-12-11 08:53:01,514 - INFO - Epoch 4 average training loss: 7.4627\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.00it/s]\n",
      "2024-12-11 08:53:05,143 - INFO - Epoch 4 validation loss: 7.6245\n",
      "Training Epoch 5:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 5: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.58]\n",
      "2024-12-11 08:53:46,960 - INFO - Epoch 5 average training loss: 7.4570\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  7.97it/s]\n",
      "2024-12-11 08:53:50,601 - INFO - Epoch 5 validation loss: 7.6305\n",
      "Training Epoch 6:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 6: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.31]\n",
      "2024-12-11 08:54:32,403 - INFO - Epoch 6 average training loss: 7.4460\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  7.99it/s]\n",
      "2024-12-11 08:54:36,035 - INFO - Epoch 6 validation loss: 7.6287\n",
      "Training Epoch 7:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 7: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.23]\n",
      "2024-12-11 08:55:17,844 - INFO - Epoch 7 average training loss: 7.4374\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.00it/s]\n",
      "2024-12-11 08:55:21,470 - INFO - Epoch 7 validation loss: 7.6332\n",
      "Training Epoch 8:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 8:  99%|█████████████████████████████████████████████████████████████████▍| 105/106 [00:39<00:00,  2.66it/s, loss=7.96]TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 8: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.96]\n",
      "2024-12-11 08:56:03,309 - INFO - Epoch 8 average training loss: 7.4365\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  7.98it/s]\n",
      "2024-12-11 08:56:06,946 - INFO - Epoch 8 validation loss: 7.6312\n",
      "Training Epoch 9:   0%|                                                                                       | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 9:  99%|█████████████████████████████████████████████████████████████████▍| 105/106 [00:39<00:00,  2.65it/s, loss=7.38]TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 9: 100%|██████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.38]\n",
      "2024-12-11 08:56:48,797 - INFO - Epoch 9 average training loss: 7.4262\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.00it/s]\n",
      "2024-12-11 08:56:52,426 - INFO - Epoch 9 validation loss: 7.6296\n",
      "Training Epoch 10:   0%|                                                                                      | 0/106 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training Epoch 10: 100%|█████████████████████████████████████████████████████████████████| 106/106 [00:39<00:00,  2.66it/s, loss=7.83]\n",
      "2024-12-11 08:57:34,190 - INFO - Epoch 10 average training loss: 7.4266\n",
      "Evaluating:   0%|                                                                                              | 0/29 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:03<00:00,  8.01it/s]\n",
      "2024-12-11 08:57:37,811 - INFO - Epoch 10 validation loss: 7.6305\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>████████▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_batch_loss</td><td>▇▅▂▅▄▇▄▄▅▇▅▆▅▆▇▃▄▄▃▇▂▃▆▅▇▂▄▅▄▂▂▅▅▆▄▃▁█▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0</td></tr><tr><td>train_batch_loss</td><td>7.83195</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-energy-1</strong> at: <a href='https://wandb.ai/ikdataml-lence/cern-llm/runs/4iwcvwx1' target=\"_blank\">https://wandb.ai/ikdataml-lence/cern-llm/runs/4iwcvwx1</a><br/> View project at: <a href='https://wandb.ai/ikdataml-lence/cern-llm' target=\"_blank\">https://wandb.ai/ikdataml-lence/cern-llm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241211_085003-4iwcvwx1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def split_pdfs(source_dir='cern_pdfs', train_ratio=0.8):\n",
    "    source_path = Path(source_dir)\n",
    "    train_path = source_path / 'train'\n",
    "    val_path = source_path / 'val'\n",
    "    \n",
    "    # Create directories\n",
    "    train_path.mkdir(exist_ok=True)\n",
    "    val_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all PDFs\n",
    "    pdf_files = list(source_path.glob('*.pdf'))\n",
    "    random.shuffle(pdf_files)\n",
    "    \n",
    "    # Calculate split\n",
    "    split_idx = int(len(pdf_files) * train_ratio)\n",
    "    train_files = pdf_files[:split_idx]\n",
    "    val_files = pdf_files[split_idx:]\n",
    "    \n",
    "    # Move files\n",
    "    for pdf in train_files:\n",
    "        shutil.move(str(pdf), str(train_path / pdf.name))\n",
    "    \n",
    "    for pdf in val_files:\n",
    "        shutil.move(str(pdf), str(val_path / pdf.name))\n",
    "    \n",
    "    print(f\"Moved {len(train_files)} files to train/\")\n",
    "    print(f\"Moved {len(val_files)} files to val/\")\n",
    "\n",
    "split_pdfs()\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('training.log'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        config = TransformerConfig(\n",
    "            vocab_size=50257,  # GPT-2 vocabulary size\n",
    "            max_sequence_length=512,\n",
    "            d_model=768,\n",
    "            n_heads=12,\n",
    "            n_layers=6,\n",
    "            d_ff=3072,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        os.makedirs(\"cern_pdfs/train\", exist_ok=True)\n",
    "        os.makedirs(\"cern_pdfs/val\", exist_ok=True)\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        \n",
    "        logging.info(\"Creating datasets...\")\n",
    "        train_dataset = CERNDataset(\n",
    "            \"cern_pdfs/train\",\n",
    "            tokenizer_name=\"gpt2\",\n",
    "            max_length=config.max_sequence_length\n",
    "        )\n",
    "        val_dataset = CERNDataset(\n",
    "            \"cern_pdfs/val\", \n",
    "            tokenizer_name=\"gpt2\",\n",
    "            max_length=config.max_sequence_length\n",
    "        )\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = CustomLLM(config)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            batch_size=8,\n",
    "            learning_rate=3e-4,\n",
    "            num_epochs=10,\n",
    "            device=device,\n",
    "            wandb_project=\"cern-llm\",\n",
    "            checkpoint_dir=\"checkpoints\"\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Training failed: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        if wandb.run:\n",
    "            wandb.finish()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db776e58-9c88-4e75-b996-278f94518548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
